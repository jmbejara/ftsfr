{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a5fc3f",
   "metadata": {},
   "source": [
    "#  Summary of CDS Return Replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13355c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import requests\n",
    "import io\n",
    "import datetime\n",
    "\n",
    "from he_kelly_manela import pull_he_kelly_manela\n",
    "from wrds_markit import pull_markit_cds, pull_fed_yield_curve\n",
    "from settings import config\n",
    "\n",
    "DATA_DIR = Path(config(\"DATA_DIR\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f08c803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from settings import config\n",
    "\n",
    "DATA_DIR = config(\"DATA_DIR\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3a6aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from calc_cds_returns import *\n",
    "# from pull_fed_yield_curve import *\n",
    "# from pull_markit_cds import *\n",
    "\n",
    "# reset START-DATE -- Currently, it is 1985\n",
    "START_DATE = pull_markit_cds.START_DATE\n",
    "END_DATE = pull_markit_cds.END_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e0ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change default pandas display options\n",
    "pd.options.display.max_columns = 25\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Change default figure size\n",
    "plt.rcParams['figure.figsize'] = 6, 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf14006",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "This notebook replicates quarterly CDS portfolio returns as constructed in He, Kelly, and Manela (2017). However, *He et al.* do not compute CDS returns directly — instead, they use data shared by Diogo Palhares, based on his 2013 paper *\"Cash-Flow Maturity and Risk Premia in CDS Markets\"*, where the methodology for return construction is fully described.\n",
    "\n",
    "> \"For CDS, we construct 20 portfolios sorted by spreads using individual name 5-year contracts... Our definition of CDS returns follows Palhares (2013).\"\n",
    "\n",
    "```{note}\n",
    "from Fed reserve only? there is another data source?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e6b839",
   "metadata": {},
   "source": [
    "### Quarterly CDS Return\n",
    "The CDS return series consists of **20 portfolios**, formed by sorting **5-year single-name CDS contracts** into quintiles based on credit spreads. Each quarter, returns are computed for portfolios within each quintile, producing 20 total portfolios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd0fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make a function to do all the following steps \n",
    "\n",
    "he_kelly = pull_he_kelly_manela.load_he_kelly_manela_all(data_dir=DATA_DIR / \"he_kelly_manela\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e8806",
   "metadata": {},
   "outputs": [],
   "source": [
    "he_kelly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e1b587",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_lst = [\"yyyymm\"]\n",
    "for i in range(1,10):\n",
    "    col_lst.append(f\"CDS_0{i}\")\n",
    "for i in range(10,21):\n",
    "    col_lst.append(f\"CDS_{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92473c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "he_kelly_df = he_kelly[col_lst].dropna(axis=0).set_index(\"yyyymm\")\n",
    "\n",
    "plt.plot(he_kelly_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704a730f",
   "metadata": {},
   "source": [
    "## 2. Data Retrival"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f2513",
   "metadata": {},
   "source": [
    "### Step 1: Retrive data from Markit (`pull_markit_cds.py`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b512314",
   "metadata": {},
   "source": [
    "To replicate the quarterly CDS return series following the methodology of Palhares (2013), we must account for both the mark-to-market value of the CDS and the coupon payments received over the quarter, along with any losses due to default. Therefore, when retrieving data from Markit, we include only the fields necessary to construct these components.\n",
    "\n",
    "We specifically extract:\n",
    "\n",
    "- `date`: to timestamp each observation and organize data quarterly.\n",
    "- `ticker` and `RedCode`: identifiers for the reference entity, allowing entity tracking over time.\n",
    "- `parspread`: the **quoted CDS spread**, used to compute premium (coupon) payments.\n",
    "- `convspreard`: the conversion spread, which can be used to validate spread consistency.\n",
    "- `tenor`: used to filter contracts by maturity — specifically, we later restrict to **5-year CDSs**.\n",
    "- `country` and `currency`: we retain only **USD-denominated contracts** for consistency with He, Kelly, and Manela.\n",
    "- `creditdv01` and `riskypv01`: key for valuing the premium leg of the CDS contract.\n",
    "- `irdv01`: the interest rate DV01, indicating sensitivity to changes in the risk-free curve.\n",
    "- `rec01`: sensitivity to changes in the assumed recovery rate.\n",
    "- `dp`: the implied default probability, which informs risk assessment and pricing.\n",
    "- `jtd`: jump-to-default — the loss from an immediate credit event.\n",
    "- `dtz`: jump-to-zero — the loss assuming zero recovery in a default scenario.\n",
    "- We also include only quotes where `docclause` is `\"XR\"` and `CompositeDepth5Y` ≥ 3 to ensure high-quality, consistent CDS quotes.\n",
    "\n",
    "```{note}\n",
    "Do we need the `country` field? Since we're filtering for USD-denominated CDS, using the `currency` field alone may be sufficient.\n",
    "\n",
    "However, if we plan to do a disaggregated dataset, it might be useful to retain `country` and other identifiers at this stage.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f887529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cds_df = pull_markit_cds.load_cds_data(data_dir=DATA_DIR / \"wrds_markit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db734f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pull_markit_cds\n",
    "# cds_df = pd.read_parquet(\"../../_data/wrds_markit/markit_cds.parquet\")\n",
    "# TODO: here the load_cds_data reads from _data/markit_cds.parquet instead of wrds_markit/markit_cds.parquet, so I am loading directly right now\n",
    "# cds_df = pull_markit_cds.load_cds_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b99078",
   "metadata": {},
   "outputs": [],
   "source": [
    "cds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b81ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cds_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65806cd",
   "metadata": {},
   "source": [
    "### Step 2: Retrive data from Federal Reserve (`pull_fed_yield_curve.py`)\n",
    "In this step, we download and process the [U.S. Treasury zero-coupon yield curve data]((https://www.federalreserve.gov/econres/feds/the-us-treasury-yield-curve-1961-to-the-present.htm)) published by the Federal Reserve Board. This dataset is based on the Gürkaynak, Sack, and Wright (2007) methodology, which fits a smooth yield curve to observed Treasury yields.  Zero-coupon yields for maturities from 1 year to 30 years, labeled as `SVENY01, SVENY02, ..., SVENY30`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92ac0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d3167",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_df = pull_fed_yield_curve.load_fed_yield_curve(data_dir=DATA_DIR / \"fed_yield_curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca04c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1493e9",
   "metadata": {},
   "source": [
    "### Step 3: Retrieve Swap rates data\n",
    "We also download daily U.S. Treasury yields for the 3-month (DGS3MO) and 6-month (DGS6MO) tenors from the Federal Reserve Economic Data (FRED).\n",
    "\n",
    "These short-term interest rates serve as risk-free rate proxies and are used in the construction of discount curves or in approximating the short end of the yield curve when computing CDS returns and risky durations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e86e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Move Swap rates func to a new pull file \n",
    "# Kausthub did not use this function, but I think at least worth trying bc He's paper mentioned this data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add0fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calc_cds_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4880ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "swap_rates_df = calc_cds_returns.pull_swap_rates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730b894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "swap_rates_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296925b6",
   "metadata": {},
   "source": [
    "Then, we just take raw interest rate data (zero-coupon yield curves + short-term Treasury rates), clean it, align it by date, and format everything into a single, consistent DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23449424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rates(raw_rates = None, start_date = START_DATE, end_date = END_DATE):\n",
    "    \"\"\"\n",
    "    Processes raw interest rate data by filtering within a specified date range\n",
    "    and converting column names to numerical maturity values.\n",
    "\n",
    "    Parameters:\n",
    "    - raw_rates (DataFrame): Raw interest rate data with column names like 'SVENY01', 'SVENY02', etc.\n",
    "    - start_date (str or datetime): Start date for filtering.\n",
    "    - end_date (str or datetime): End date for filtering.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Processed interest rate data with maturity values as column names and rates in decimal form.\n",
    "    \"\"\"\n",
    "    raw_rates = raw_rates.copy().dropna()\n",
    "    short_tenor_rates = pull_swap_rates(pd.to_datetime(start_date))\n",
    "    short_tenor_rates_renamed = short_tenor_rates.rename(columns={\n",
    "    'DGS3MO': 0.25,\n",
    "    'DGS6MO': 0.5\n",
    "})\n",
    "    raw_rates.columns = raw_rates.columns.str.extract(r\"(\\d+)$\")[0].astype(int)  # Extract numeric part from column names\n",
    "    rates = raw_rates[\n",
    "        (raw_rates.index >= pd.to_datetime(start_date)) &\n",
    "        (raw_rates.index <= pd.to_datetime(end_date))\n",
    "    ] # / 100  # Convert percentages to decimal format\n",
    "\n",
    "    merged_rates = pd.merge(rates, short_tenor_rates_renamed, left_index=True, right_index=True, how='inner').sort_values('Date')\n",
    "    cols = merged_rates.columns.tolist()\n",
    "    ordered_cols = [0.25, 0.5] + [col for col in cols if col not in [0.25, 0.5]]\n",
    "    merged_rates = merged_rates[ordered_cols]\n",
    "    return merged_rates\n",
    "\n",
    "\n",
    "# TODO: put it into another file\n",
    "# TODO: make the urls updated -- the end date is set to March 2025 here\n",
    "def pull_swap_rates(start_year = START_DATE):\n",
    "    urls = {\n",
    "        \"DGS6MO\": \"https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23ebf3fb&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=803&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=DGS6MO&scale=left&cosd=1981-09-01&coed=2025-03-12&line_color=%230073e6&link_values=false&line_style=solid&mark_type=none&mw=3&lw=3&ost=-99999&oet=99999&mma=0&fml=a&fq=Daily&fam=avg&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=lin&vintage_date=2025-03-14&revision_date=2025-03-14&nd=1981-09-01\",\n",
    "        \"DGS3MO\": \"https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23ebf3fb&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=803&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=DGS3MO&scale=left&cosd=1981-09-01&coed=2025-03-12&line_color=%230073e6&link_values=false&line_style=solid&mark_type=none&mw=3&lw=3&ost=-99999&oet=99999&mma=0&fml=a&fq=Daily&fam=avg&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=lin&vintage_date=2025-03-14&revision_date=2025-03-14&nd=1981-09-01\"\n",
    "    }\n",
    "\n",
    "    dataframes = {}\n",
    "    for key, url in urls.items():\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  \n",
    "        \n",
    "        df = pd.read_csv(io.StringIO(response.text), parse_dates=[\"observation_date\"])\n",
    "        df.columns = [\"observation_date\", key]  \n",
    "        dataframes[key] = df\n",
    "\n",
    "    # Merge dataframes on DATE\n",
    "    df_merged = dataframes[\"DGS3MO\"].merge(dataframes[\"DGS6MO\"], on=\"observation_date\", how=\"outer\")\n",
    "    df_merged = df_merged.rename(columns = {\"observation_date\": \"Date\"})\n",
    "    df_merged = df_merged.set_index(\"Date\")\n",
    "    df_merged = df_merged[start_year:]\n",
    "    df_merged = df_merged.dropna(axis=0)\n",
    "    df_merged = df_merged / 100\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6033e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_df = process_rates(fed_df,start_date = START_DATE, end_date = END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4faf07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc449b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_df.to_csv(\"rates_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cb07bb",
   "metadata": {},
   "source": [
    "## 3. Replication\n",
    "To replicate the quarterly CDS return, we follow the formulas below as suggested in *He-Kelly-Manela (2017)*, page 45, referencing Palhares:\n",
    "\n",
    "$$\n",
    "\\text{CDSRet}_t = \\frac{\\text{CDS}_{t-1}}{250} + \\Delta \\text{CDS}_t \\cdot \\text{RD}_{t-1}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\frac{\\text{CDS}_{t-1}}{250}$: **Carry return** — daily accrual of the previous day’s spread (annualized and spread over 250 days),\n",
    "- $\\Delta \\text{CDS}_t = \\text{CDS}_t - \\text{CDS}_{t-1}$: daily change in CDS spread,\n",
    "- $\\text{RD}_{t-1}$: **risky duration** at time $t-1$, approximating the present value of future spread payments.\n",
    "\n",
    "    $$\n",
    "    \\text{RD}_t = \\frac{1}{4} \\sum_{j=1}^{4M} e^{-j\\lambda/4} \\cdot e^{-jr_t^{(j/4)}/4}\n",
    "    $$\n",
    "\n",
    "    where:\n",
    "    - $M$: maturity in years (e.g., 5),\n",
    "    - $\\lambda$: default intensity, estimated as:\n",
    "    $$\n",
    "    \\lambda = 4 \\cdot \\log \\left(1 + \\frac{\\text{CDS}}{4L} \\right)\n",
    "    $$\n",
    "    with $L$ = loss given default, typically 60% (i.e., $L = 0.6$).\n",
    "\n",
    "    - $r_t^{(j/4)}$: risk-free rate for quarter $j/4$ at time $t$,\n",
    "    - The discounting is done using the term structure of interest rates (e.g., Treasury + swaps as per Gurkaynak et al.).\n",
    "\n",
    "### Step 4. `get_portfolio_dict`\n",
    "**Here I wrote my own understanding to the function, if there is anything wrong, plz advice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7d1d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calc_cds_returns\n",
    "\n",
    "def get_portfolio_dict(start_date = START_DATE, end_date = END_DATE, cds_spreads = None):\n",
    "    \"\"\"\n",
    "    Creates a dictionary of credit portfolios based on the CDS spread data.\n",
    "\n",
    "    Parameters:\n",
    "    - start_date (str or datetime): Start date for filtering.\n",
    "    - end_date (str or datetime): End date for filtering.\n",
    "    - cds_spreads (pl.DataFrame): CDS spread data.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionary where keys are tenor-quantile pairs and values are Polars DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(start_date, str):\n",
    "        start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    if isinstance(end_date, str):\n",
    "        end_date = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    if isinstance(cds_spreads, pd.DataFrame):\n",
    "        cds_spreads = pl.from_pandas(cds_spreads)\n",
    "    # Filter DataFrame\n",
    "    filtered_cds_spread = cds_spreads.filter(\n",
    "        (pl.col(\"date\") >= start_date) & (pl.col(\"date\") <= end_date)\n",
    "    )\n",
    "    filtered_cds_spread_us_only = filtered_cds_spread.filter(pl.col(\"country\") == \"United States\")\n",
    "    # Data Cleaning and Preparation\n",
    "    cds_spread_noNA = filtered_cds_spread_us_only.drop_nulls(subset=[\"parspread\"])\n",
    "    cds_spread_noNA = cds_spread_noNA.drop(['convspreard', 'year', 'redcode'])\n",
    "\n",
    "    # Remove duplicates\n",
    "    cds_spread_unique = cds_spread_noNA.unique()\n",
    "    \n",
    "    # TODO: here filter by parspread value was not mentioned in paper?\n",
    "    cds_spread_unique = cds_spread_unique.filter(pl.col(\"parspread\") <= 0.5)\n",
    "\n",
    "    # Convert date column to year-month format\n",
    "    cds_spread_unique = cds_spread_unique.with_columns(\n",
    "        pl.col(\"date\").dt.strftime(\"%Y-%m\").alias(\"year_month\")\n",
    "    )\n",
    "\n",
    "    # Compute Credit Quantiles\n",
    "    spread_5y = cds_spread_unique.filter(pl.col(\"tenor\") == \"5Y\")\n",
    "\n",
    "    # Get first available spread for each ticker in each month\n",
    "    first_spread_5y = (\n",
    "        spread_5y.sort(\"date\")\n",
    "        .group_by([\"ticker\", \"year_month\"])\n",
    "        .first()\n",
    "        .select([\"ticker\", \"year_month\", \"parspread\"])\n",
    "    )\n",
    "    # TODO: quantile?\n",
    "    # Compute separate credit quantiles per month\n",
    "    credit_quantiles = (\n",
    "        first_spread_5y.group_by(\"year_month\").agg([\n",
    "            pl.col(\"parspread\").quantile(0.2).alias(\"q1\"),\n",
    "            pl.col(\"parspread\").quantile(0.4).alias(\"q2\"),\n",
    "            pl.col(\"parspread\").quantile(0.6).alias(\"q3\"),\n",
    "            pl.col(\"parspread\").quantile(0.8).alias(\"q4\")\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # Assign credit quantile labels\n",
    "    first_spread_5y = first_spread_5y.join(credit_quantiles, on=\"year_month\")\n",
    "\n",
    "    first_spread_5y = first_spread_5y.with_columns(\n",
    "        pl.when(pl.col(\"parspread\") <= pl.col(\"q1\"))\n",
    "        .then(1)\n",
    "        .when(pl.col(\"parspread\") <= pl.col(\"q2\"))\n",
    "        .then(2)\n",
    "        .when(pl.col(\"parspread\") <= pl.col(\"q3\"))\n",
    "        .then(3)\n",
    "        .when(pl.col(\"parspread\") <= pl.col(\"q4\"))\n",
    "        .then(4)\n",
    "        .otherwise(5)\n",
    "        .alias(\"credit_quantile\")\n",
    "    ).select([\"ticker\", \"year_month\", \"credit_quantile\"])\n",
    "\n",
    "    # Assign computed credit quantiles to all tenors\n",
    "    cds_spreads_final = cds_spread_unique.join(\n",
    "        first_spread_5y, on=[\"ticker\", \"year_month\"], how=\"left\"\n",
    "    )\n",
    "    cds_spreads_final = cds_spreads_final.sort(\"date\")\n",
    "\n",
    "    # Compute Representative Parspread\n",
    "    relevant_tenors = [\"3Y\", \"5Y\", \"7Y\", \"10Y\"]\n",
    "    relevant_quantiles = [1, 2, 3, 4, 5]\n",
    "\n",
    "    filtered_df = cds_spreads_final.filter(\n",
    "        (pl.col(\"tenor\").is_in(relevant_tenors)) & \n",
    "        (pl.col(\"credit_quantile\").is_in(relevant_quantiles))\n",
    "    )\n",
    "\n",
    "    rep_parspread_df = (\n",
    "        filtered_df\n",
    "        .group_by([\"date\", \"tenor\", \"credit_quantile\"])\n",
    "        .agg(pl.col(\"parspread\").mean().alias(\"rep_parspread\"))\n",
    "    )\n",
    "\n",
    "    # Convert 'date' column to month level (truncate to the first day of the month)\n",
    "    rep_parspread_df = rep_parspread_df.with_columns(\n",
    "        pl.col(\"date\").dt.truncate(\"1mo\").alias(\"month\")\n",
    "    )\n",
    "\n",
    "    portfolio_dict = {}\n",
    "\n",
    "    for tenor in relevant_tenors:\n",
    "        for quantile in relevant_quantiles:\n",
    "            key = f\"{tenor}_Q{quantile}\"  # Example key: \"5Y_Q3\"\n",
    "            \n",
    "            # Filter dataframe for this specific tenor-quantile pair\n",
    "            portfolio_df = rep_parspread_df.filter(\n",
    "                (pl.col(\"tenor\") == tenor) & (pl.col(\"credit_quantile\") == quantile)\n",
    "            )\n",
    "\n",
    "            portfolio_df = portfolio_df.sort(\"date\")\n",
    "            \n",
    "            # Store in dictionary\n",
    "            portfolio_dict[key] = portfolio_df\n",
    "    return portfolio_dict\n",
    "portfolio_dict = get_portfolio_dict(start_date = START_DATE, end_date = END_DATE, cds_spreads = cds_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa3b4c2",
   "metadata": {},
   "source": [
    "### **Step 5: Interpolating Quarterly Risk-Free Rates**\n",
    "\n",
    "**in the paper, the term is extrapolation, but by definition, it should be interpolation???**\n",
    "\n",
    "One term we need to compute in the CDS return formula is an exponential discount factor involving the risk-free rate, specifically of the form $e^{-\\frac{j \\cdot r_t^{(j/4)}}{4}}$, where $r_t^{(j/4)}$ is the rate for quarter $j/4$ at time $t$.\n",
    "\n",
    "However, our available yield data includes only a limited set of maturities: 3-month, 6-month, and annual terms from 1Y to 30Y. To obtain the necessary rates at intermediate quarterly maturities (e.g., 0.75Y, 1.25Y, 1.5Y, etc.), we follow the approach suggested in thye paper and apply **interpolation** across the term structure. This interpolation step allows us to estimate risk-free rates for arbitrary quarterly horizons, which are essential for accurately computing discount factors and the risky duration component in the CDS return calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07e0913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrapolate_rates(rates = None):\n",
    "    \"\"\"\n",
    "    Applies cubic spline extrapolation to fill in interest rate values at quarterly intervals.\n",
    "\n",
    "    Parameters:\n",
    "    - rates (DataFrame): A DataFrame where columns represent maturity years,\n",
    "                         and values are interest rates.\n",
    "\n",
    "    Returns:\n",
    "    - df_quarterly: A DataFrame with interpolated rates at quarterly maturities.\n",
    "    \"\"\"\n",
    "    years = np.array(rates.columns)\n",
    "    \n",
    "    # Define the new maturities at quarterly intervals (0.25, 0.5, ..., 30)\n",
    "    quarterly_maturities = np.arange(0.25, 30.25, 0.25)\n",
    "    # 3m, 6m, 9m, 1Y, 1.25Y, 1.5Y, 1.75Y, 2Y \n",
    "\n",
    "    interpolated_data = []\n",
    "\n",
    "    for _, row in rates.iterrows():\n",
    "        values = row.values  # Get values for the current row\n",
    "        cs = CubicSpline(years, values, extrapolate=True)  # Create spline function\n",
    "        interpolated_values = cs(\n",
    "            quarterly_maturities\n",
    "        )  # Interpolate for quarterly intervals\n",
    "        interpolated_data.append(interpolated_values)  # Append results\n",
    "\n",
    "    # Create a new DataFrame with interpolated values for all rows\n",
    "    df_quarterly = pd.DataFrame(interpolated_data, columns=quarterly_maturities)\n",
    "    df_quarterly.index = rates.index\n",
    "    return df_quarterly\n",
    "\n",
    "def calc_discount(raw_rates = None, start_date = START_DATE, end_date = END_DATE):\n",
    "    \"\"\"\n",
    "    Calculates the discount factor for given interest rate data using quarterly rates.\n",
    "\n",
    "    Parameters:\n",
    "    - raw_rates (DataFrame): The raw interest rate data.\n",
    "    - start_date (str or datetime): The start date for filtering.\n",
    "    - end_date (str or datetime): The end date for filtering.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Discount factors for various maturities.\n",
    "    \"\"\"\n",
    "    # Call the function to get rates\n",
    "    rates_data = process_rates(raw_rates, start_date, end_date)\n",
    "    if rates_data is None:\n",
    "        print(\"No data available for the given date range.\")\n",
    "        return None\n",
    "\n",
    "    quarterly_rates = extrapolate_rates(rates_data)\n",
    "\n",
    "    quarterly_discount = pd.DataFrame(\n",
    "        columns=quarterly_rates.columns, index=quarterly_rates.index\n",
    "    )\n",
    "    for col in quarterly_rates.columns:\n",
    "        quarterly_discount[col] = quarterly_rates[col].apply(\n",
    "            lambda x: np.exp(-(col * x) / 4)\n",
    "        )\n",
    "\n",
    "    return quarterly_discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da75058",
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_discount = calc_discount(fed_df, start_date = START_DATE, end_date = END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33065eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_discount.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db0b92b",
   "metadata": {},
   "source": [
    "\n",
    "### Step6. Calculate daily return \n",
    "`calc_cds_return_for_portfolios()` calculates lambda, risk duration, and finally the risk-free return of each quantilextenor combo.\n",
    "\n",
    "### Step7. Aggreate the monthly return\n",
    "`calculate_monthly_returns` aggregates the monthly reutrn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514b1882",
   "metadata": {},
   "source": [
    "## 4. Graph"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
