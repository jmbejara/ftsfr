[Assimakopoulos2000]
description = "The paper introduces the Theta model, a novel univariate forecasting method that modifies the local curvature of time-series data through a Theta coefficient. This method decomposes the original series into Theta-lines, which are extrapolated and combined to improve forecasting accuracy. The model was tested on the M3 competition data and showed strong performance, particularly for monthly and microeconomic series. The approach offers a new way to enhance long-term data approximation or short-term feature augmentation."
empirical_facts = [ "The Theta model was applied to 3003 series from the M3 competition.", "The method performed particularly well for monthly series.", "The model also showed strong results for microeconomic data.",]
lessons_learned = [ "Decomposing time series into Theta-lines can improve forecasting accuracy.", "The combination of Theta $=0$ and Theta $=2$ lines is effective for producing forecasts.", "The method is versatile, enhancing both long-term and short-term forecasting depending on the Theta coefficient.",]
data_sources = [ "M3 competition data",]
cleaning_steps = "- Not applicable"

[Bagnall2018]
description = "The paper introduces the UEA Multivariate Time Series Classification (MTSC) archive, a comprehensive collection of 30 datasets designed to facilitate rigorous evaluation of multivariate time series classification algorithms. This archive addresses the limitations of previous collections by offering a diverse range of datasets with varying dimensions, lengths, and class labels, formatted for consistency and ease of use. The archive aims to advance research in multivariate time series classification by providing a standardized benchmark for algorithm comparison."
empirical_facts = [ "The UEA MTSC archive consists of 30 datasets with varying dimensions, lengths, and class labels.", "The datasets cover a wide range of applications, including Human Activity Recognition, Motion Classification, ECG, EEG/MEG, and Audio Spectra Classification.", "The archive includes datasets from various sources such as UCI Machine Learning archive, Brain Computer Interface competitions, and Kaggle competitions.",]
lessons_learned = [ "Standardized datasets can significantly enhance the evaluation and comparison of multivariate time series classification algorithms.", "Diverse datasets are crucial for developing robust algorithms that can generalize across different domains.", "Collaborative efforts between institutions can lead to the creation of valuable resources for the research community.",]
data_sources = [ "UCI Machine Learning archive", "Brain Computer Interface competitions", "Kaggle competitions", "University of East Anglia", "University of California, Riverside",]
cleaning_steps = [ "All datasets were formatted to be of equal length.", "Datasets with missing data were excluded.", "Train/test splits were provided and retained if originally included by dataset donators.", "Data was truncated to the length of the shortest series retained, with random intervals taken for longer series.",]

[Barth2021]
description = "The paper examines the Treasury cash-futures basis trade, an arbitrage strategy that exploits price differences between cash and futures markets for Treasuries. It documents the rise of this trade among hedge funds post-2016 and its significant role in hedge fund Treasury positions and repo borrowing. The authors present a model linking the basis trade to financial instability, particularly during the COVID-19 pandemic, and highlight the Federal Reserve's interventions in preventing a liquidity spiral. The study underscores the systemic risks posed by non-bank actors in the Treasury market."
empirical_facts = [ "Hedge fund Treasury exposure increased from $1.06 trillion to $2.02 trillion between 2017 and 2019, largely due to the basis trade.", "At its peak, the basis trade accounted for more than 60% of total hedge fund Treasury exposure and over 70% of hedge fund repo borrowing.", "The cash-futures disconnect widened significantly between 2017 and 2019, driven by demand for off-balance-sheet duration exposure.", "In March 2020, hedge funds reduced short futures positions from $659 billion to $554 billion, accompanied by sales of cash Treasuries.", "The Federal Reserve's interventions in March 2020 likely prevented a liquidity spiral in the Treasury market.",]
lessons_learned = [ "Non-bank actors like hedge funds play a critical role in the Treasury market structure, posing potential systemic risks.", "The basis trade can exacerbate financial instability, particularly during periods of market stress.", "Limits to arbitrage, such as repo market illiquidity and margin requirements, significantly impact the cash-futures disconnect.", "Timely central bank interventions can stabilize markets and prevent further deterioration during crises.", "The Treasury market's reliance on market-based finance increases its vulnerability to shocks.",]
data_sources = [ "Regulatory datasets on hedge fund exposures", "Repo transaction data", "U.S. Securities and Exchange Commission's (SEC) Form PF",]
cleaning_steps = "- Not applicable"

[Bauer2021]
description = "The paper introduces Libra, a comprehensive benchmark for evaluating time series forecasting methods. Libra aims to provide a level playing field by automatically evaluating and ranking forecasting methods across diverse scenarios. The benchmark includes four use cases, each with 100 heterogeneous time series from various domains, offering greater diversity than existing competitions. The authors also share the source code and data set for reproducibility and further research."
empirical_facts = [ "Libra includes four distinct use cases, each with 100 heterogeneous time series.", "The data set is assembled from publicly available sources and is more diverse than existing forecasting competitions.", "Libra evaluates and ranks forecasting methods automatically across diverse scenarios.",]
lessons_learned = [ "No single forecasting method is universally best for all time series due to the 'No-Free-Lunch Theorem.'", "A diverse and comprehensive benchmark like Libra can aid in objectively comparing forecasting methods.", "Existing forecasting competitions often lack diversity and focus on specific use cases.",]
data_sources = [ "- Publicly available time series data",]
cleaning_steps = [ "- Not applicable",]

[Borri2011]
description = "The paper 'Sovereign Risk Premia' by Nicola Borri and Adrien Verdelhan explores how risk aversion among investors affects sovereign bond prices and default decisions in emerging markets. The authors find that higher correlations between sovereign bond returns and U.S. market returns lead to higher sovereign excess returns, indicating compensation for risk. They develop a model demonstrating that sovereign defaults and bond prices are influenced by both borrowers' economic conditions and lenders' risk aversion. The study provides a risk-based explanation for sovereign bond returns, highlighting the impact of global economic conditions on emerging market debt."
empirical_facts = [ "Sovereign bond excess returns range from 4% to 15%, with a spread of about 5% per year between low and high default probability countries.", "Higher covariances between sovereign returns and U.S. equity or corporate bond returns result in higher average sovereign returns.", "Market prices of risk are positive and significant, with pricing errors statistically indistinguishable from zero.", "The market price of bond risk is higher than, but not statistically different from, the mean of U.S. BBB corporate bond excess returns.",]
lessons_learned = [ "Risk aversion among investors significantly impacts sovereign bond prices and default decisions.", "Sovereign bond returns are influenced by global economic conditions, particularly the correlation with U.S. market returns.", "Emerging market debt exposes countries to U.S. business cycle risks, affecting borrowing costs and default probabilities.", "The model suggests that borrowing and default decisions are shaped by both borrowers' and lenders' economic conditions.",]
data_sources = [ "JP Morgan's Emerging Market Bond Index (EMBI)", "Standard and Poor's credit ratings",]
cleaning_steps = "- Not applicable"

[Bound2001]
description = "The paper by Bound, Brown, and Mathiowetz examines the impact of measurement error in survey data on economic research, challenging the classical assumption that such errors are uncorrelated with true values. It highlights the limitations of standard correction methods like instrumental variables when errors are non-classical. The authors emphasize the importance of validation studies in assessing and bounding measurement errors and suggest that these studies should be better integrated into survey design. The paper also reviews the extent of measurement error across various economic variables, providing insights into its effects on parameter estimates."
empirical_facts = [ "Measurement errors in survey data often negatively correlate with true values, contrary to the classical assumption.", "Validation studies show that errors in survey data can significantly bias parameter estimates, particularly in non-linear models.", "Errors in survey data are not always random and can lead to incorrect conclusions about economic phenomena.",]
lessons_learned = [ "Non-classical measurement errors should be considered seriously in economic analyses using survey data.", "Validation studies are crucial for understanding and correcting measurement errors but are often underutilized.", "Explicit modeling of measurement errors can provide better insights into biases and improve the reliability of parameter estimates.",]
data_sources = "- Not applicable"
cleaning_steps = "- Not applicable"

[Brown1992]
description = "The paper 'Survivorship Bias in Performance Studies' by Brown, Goetzmann, Ibbotson, and Ross examines the impact of survivorship bias on the perceived predictability of mutual fund performance. The authors argue that the apparent predictability of mutual fund returns is largely a result of survivorship bias, where only successful funds are included in analyses, thus skewing results. They provide numerical examples and empirical evidence to demonstrate that this bias can significantly contribute to the illusion of return predictability. The study highlights the importance of considering survivorship bias when evaluating the performance of mutual funds."
empirical_facts = [ "Past mutual fund performance is often used as a predictor for future performance, but this may be misleading due to survivorship bias.", "Studies show mixed results on the persistence of mutual fund performance, with some indicating predictability and others not.", "The paper uses data from 728 mutual funds over the period 1976-1988, with 258 funds surviving the entire period.", "Contingency tables and regression analyses are used to assess performance persistence, revealing statistically significant persistence in some periods.",]
lessons_learned = [ "Survivorship bias can significantly distort the perceived predictability of mutual fund returns.", "Careful interpretation of performance data is necessary to avoid misleading conclusions about fund manager skill.", "The methodologies used in performance studies may not adequately account for the effects of survivorship bias.", "Persistence in mutual fund performance can relate to both positive and negative outcomes, not just 'hot hands'.",]
data_sources = [ "Ibbotson Associates", "Morningstar, Inc.",]
cleaning_steps = [ "Risk-adjustment using Jensen's alpha relative to the S&P 500 Index.", "Classification of funds into winners and losers based on median performance.", "Exclusion of funds that did not survive the entire study period.",]

[Brown1995]
description = "The paper by Brown, Goetzmann, and Ross explores the implications of conditioning on the survival of securities in empirical finance studies, particularly how this affects the observed relationship between returns and risk. The authors demonstrate that such conditioning can bias the observed returns, leading to spurious relationships and impacting studies on equity premiums, long-term autocorrelations, and event studies. They highlight that the bias is more pronounced in volatile markets, such as emerging markets, and provide preliminary numerical examples to illustrate these effects. The paper suggests that survival bias should be considered in empirical finance to avoid misleading conclusions."
empirical_facts = [ "Survival conditioning induces a spurious relationship between observed return and total risk for surviving securities.", "The bias in expected returns due to survival conditioning increases with the volatility of returns.", "Long-term autocorrelation studies are biased towards rejecting a random walk due to survival conditioning.", "Event studies show a strong association between earnings announcement magnitude and postannouncement performance, partly due to survival bias.", "Stock split studies show upward trends in preannouncement returns due to the exclusion of securities with lower prices.",]
lessons_learned = [ "Survival bias can significantly distort empirical findings in finance, particularly in studies of equity returns and market behavior.", "Researchers should account for survival bias when analyzing long-term financial data to avoid erroneous conclusions.", "The impact of survival bias is more pronounced in volatile and emerging markets, necessitating careful consideration in these contexts.", "Event studies and analyses of corporate announcements should consider survival bias to accurately interpret postannouncement performance.", "Understanding the implications of survival bias can lead to more accurate assessments of market dynamics and investor returns.",]
data_sources = "- Not applicable"
cleaning_steps = "- Not applicable"

[Buehlmaier2018]
description = "The paper introduces novel measures of financial constraints using textual analysis of firms' annual reports and examines their impact on stock returns. It finds that constrained firms earn higher returns that cannot be explained by traditional factor models, with the strongest results observed for debt constraints. The study reveals that large, liquid stocks are most affected by financial constraints, and a trading strategy based on these constraints yields significant risk-adjusted returns. The research highlights the importance of debt market constraints in financial constraints risk, distinguishing it from equity market constraints."
empirical_facts = [ "Constrained firms earn higher returns, especially those constrained in debt markets.", "A trading strategy based on financial constraints yields an annualized risk-adjusted excess return of 6.5% for debt constraints.", "The largest and most liquid stocks are most affected by financial constraints risk, not small-cap stocks.", "Debt constraints yield an annualized alpha of 7.2% for the top market capitalization percentile.", "The equity market is less concerned about a firm's ability to raise money compared to debt markets.",]
lessons_learned = [ "Textual analysis of annual reports can effectively measure financial constraints.", "Financial constraints are priced in stock returns, with significant implications for asset pricing.", "Debt market constraints are more critical than equity market constraints in determining financial constraints risk.", "Large and liquid stocks can be significantly impacted by financial constraints, contrary to the expectation that small stocks are more affected.", "Textual analysis provides a more direct measure of financial constraints compared to traditional accounting data-based measures.",]
data_sources = [ "Compustat North America Fundamentals Quarterly database", "Center for Research in Security Prices (CRSP)", "EDGAR database from the SEC",]
cleaning_steps = [ "Omitted regulated firms and financial firms based on SIC classifications.", "Deleted firms with coding errors, such as reporting smaller total debt than short-term debt.", "Excluded firms undergoing significant mergers or with zero/negative total assets, book equity, or sales.", "Merged CRSP data with Compustat data to avoid look-ahead bias.", "Extracted and focused on the MD&A section of 10-K filings from the EDGAR database.",]

[Carhart1997]
description = "The paper by Mark M. Carhart examines the persistence of mutual fund performance, finding that common factors in stock returns and investment expenses largely explain this persistence. The study challenges the notion of skilled mutual fund managers, attributing the 'hot hands' effect to the momentum effect rather than managerial skill. The research highlights that only the underperformance of the worst-return funds remains unexplained. The study uses a survivor-bias-free dataset to provide insights into mutual fund performance predictability."
empirical_facts = [ "Common factors in stock returns and investment expenses explain most of the persistence in mutual fund performance.", "The 'hot hands' effect is primarily driven by the one-year momentum effect, not by managerial skill.", "Mutual funds that appear to follow momentum strategies earn lower abnormal returns after expenses.", "Expenses and turnover negatively impact mutual fund performance, with trading reducing performance by approximately 0.95% of the trade's market value.", "Load funds underperform no-load funds by approximately 80 basis points per year, holding expense ratios constant.",]
lessons_learned = [ "Persistence in mutual fund performance is not indicative of superior stock-picking skill.", "Transaction costs can negate the gains from momentum strategies in mutual funds.", "Survivor bias must be accounted for to accurately assess mutual fund performance.", "Expense ratios and transaction costs are significant determinants of mutual fund performance.", "Model misspecification can affect the interpretation of mutual fund performance data.",]
data_sources = [ "Micropal/Investment Company Data, Inc. (ICDI)", "FundScope Magazine", "United Babson Reports", "Wiesenberger Investment Companies", "The Wall Street Journal",]
cleaning_steps = [ "The dataset includes all known equity funds from 1962 to 1993, free of survivor bias.", "Data on surviving funds and those that disappeared since 1989 were collected from multiple sources.", "Monthly total returns were obtained from multiple sources to minimize missing data.", "Reinvestment NAVs for capital gains and income distributions were included to mitigate biases.",]

[Challu2022]
description = "The paper introduces NHITS, a novel model for long-horizon time series forecasting that addresses challenges of volatility and computational complexity. By utilizing hierarchical interpolation and multi-rate data sampling, NHITS assembles predictions sequentially, focusing on different frequencies and scales. The model demonstrates a 20% improvement in accuracy over state-of-the-art Transformer architectures and reduces computation time by 50 times. Extensive experiments on large-scale datasets validate the model's efficiency and effectiveness."
empirical_facts = [ "NHITS provides an average accuracy improvement of almost 20% over the latest Transformer architectures.", "The model reduces computation time by an order of magnitude (50 times) compared to existing methods.", "NHITS achieves state-of-the-art results on six large-scale benchmark datasets.", "The model incorporates multi-rate data sampling and hierarchical interpolation to improve forecasting.", "NHITS is based on a redefined fully-connected N-BEATS architecture.",]
lessons_learned = [ "Hierarchical interpolation can efficiently approximate long horizons in the presence of smoothness.", "Multi-rate data sampling significantly reduces memory footprint and computational requirements.", "Specializing model outputs in different signal frequencies can improve accuracy and computational efficiency.", "Combining multi-rate input processing with hierarchical interpolation offers a promising solution for long-horizon forecasting challenges.",]
data_sources = [ "Electricity transformer temperature dataset", "Exchange rate dataset", "Electricity consumption dataset", "San Francisco bay area highway traffic dataset", "Weather dataset", "Influenza-like illness dataset",]
cleaning_steps = "- Not applicable"

[Chen2022]
description = "The paper by Chen and Zimmermann provides an open-source dataset and code that reproduces nearly all cross-sectional stock return predictors from previous studies. They find that 98% of the 161 characteristics deemed significant in original studies show t-stats above 1.96 in their reproductions. Their work emphasizes the importance of open collaboration in finance, showing that most predictability results in the literature can be reproduced. The authors also highlight the robustness of their findings through various tests and provide a comprehensive dataset for public use."
empirical_facts = [ "98% of the 161 significant characteristics in original studies have t-stats above 1.96 in reproductions.", "A regression of reproduced t-stats on original t-stats finds a slope of 0.88 and an R² of 82%.", "Mean returns are monotonic in predictive signals at the characteristic level.", "Only three out of 205 clear and likely predictor reproductions failed to achieve original predictability evidence.", "The dataset includes 319 firm-level characteristics, with only three failing to reproduce original statistical significance.",]
lessons_learned = [ "Open collaboration and transparency can significantly enhance the credibility of academic finance.", "Reproducibility is achievable for nearly all cross-sectional asset pricing predictors with careful methodology.", "Minor deviations in test design can lead to differences in replication results, emphasizing the need for methodological consistency.", "The robustness of findings can be supported by various tests, including out-of-sample performance and monotonicity checks.", "Misclassification of anomalies can lead to perceived replication failures, highlighting the importance of accurate classification.",]
data_sources = [ "- WRDS", "- Original papers from which characteristics were drawn",]
cleaning_steps = [ "- Hand-collection of key tables, empirical tests, and t-stats from original papers", "- Application of modular code with exception handling for data processing", "- Use of semantic versioning to track code updates and ensure consistency",]

[Chen2022a]
description = "The paper investigates whether peer-reviewed economic research offers incremental predictive power for stock returns compared to naive data mining. By analyzing 29,000 accounting ratios and comparing them to 173 published predictors, the study finds that both methods yield similar post-sample return predictability, with about 50% of predictability persisting. The research suggests that peer review may mislabel mispricing as risk, as risk-based predictors underperform post-sample. Data mining not only replicates the predictability of published research but also uncovers similar themes, often decades earlier."
empirical_facts = [ "Both peer-reviewed and data-mined predictors show about 50% of their original sample return predictability post-sample.", "Only 20% of published predictors are attributed to risk, with 61% attributed to mispricing.", "Data mining uncovers themes like investment, issuance, and accruals, similar to academic research, often decades earlier.", "Risk-based predictors tend to underperform their data-mined benchmarks post-sample.", "Data mining can generate substantial out-of-sample returns by searching for large t-statistics in accounting variables.",]
lessons_learned = [ "Peer-reviewed research offers only modest improvement in predicting stock returns over naive data mining.", "The academic discovery process may resemble data mining, as both yield similar post-sample decay patterns.", "Risk-based research does not necessarily lead to better post-sample performance compared to data mining.", "Data mining can serve as a foundation for future economic ideas, despite its atheoretical nature.", "The scientific process in finance may mislabel mispricing as risk or identify unstable risk factors.",]
data_sources = [ "Chen and Zimmermann (2022) dataset of replicated asset pricing studies", "Yan and Zheng (2017) dataset for data mining benchmarks", "Compustat annual accounting variables", "CRSP market equity data",]
cleaning_steps = [ "Selected accounting variables with non-missing values for at least 20 years and at least 1,000 firms per year.", "Generated accounting ratios using two functional forms and restricted denominators to avoid division by zero.", "Lagged signals by six months relative to fiscal year ends and formed long-short decile strategies.",]

[Chen2022b]
description = "The paper by Andrew Y. Chen provides bounds for the false discovery rate (FDR) in cross-sectional return predictability studies, challenging previous claims that most findings in financial economics are likely false. By using simple summary statistics, Chen demonstrates that the FDR is at most 25% in eight out of nine studies, and a more refined analysis shows it could be as low as 9%. This work reconciles conflicting estimates in the literature by highlighting misinterpretations in previous studies and shows that most claimed findings are likely true."
empirical_facts = [ "The false discovery rate (FDR) is at most 25% in eight out of nine studies analyzed.", "A refined bound indicates the FDR could be as low as 9%, suggesting at least 91% of discoveries are true.", "Accounting ratios are a common data source for cross-sectional predictors, comprising 43% of predictors.", "Naively-selected accounting ratios show at least 20% lead to significant results, implying an FDR of at most 25%.", "Exponential extrapolation of t-statistics provides a conservative estimate of the FDR.",]
lessons_learned = [ "Misinterpretations of statistical terms can lead to overestimation of false discovery rates.", "Summary statistics from previous studies can provide robust bounds on the FDR.", "Naive data mining can serve as a benchmark for evaluating expert-driven research findings.", "Publication bias must be considered when estimating the probability of significant findings.", "Conservative extrapolation techniques can provide upper bounds for the FDR in the presence of publication bias.",]
data_sources = [ "Previous studies on cross-sectional return predictability", "Accounting ratios data from studies such as Yan and Zheng (2017), Chordia, Goyal, and Saretto (2020), and Chen, Lopez-Lira, and Zimmermann (2025)",]
cleaning_steps = "- Not applicable"

[Constantinides2013]
description = "The paper constructs a panel of S&P 500 Index call and put option portfolios, adjusted daily to maintain specific maturity, moneyness, and market beta, to test multi-factor pricing models. It finds that crisis-related factors such as price jumps and volatility jumps, alongside the market, explain the cross-sectional variation in returns. The study introduces a methodology that reduces the variance and skewness of portfolio returns, making them suitable for linear factor pricing models. The findings suggest that while some abnormal returns remain unexplained, incorporating additional factors can further reduce pricing errors."
empirical_facts = [ "The constructed panel consists of 54 option portfolios, each adjusted daily for targeted beta, maturity, and moneyness.", "The study covers the period from April 1986 to January 2012, including major financial crises.", "Crisis-related factors like Jump and Volatility Jump significantly reduce the pricing errors of option returns.", "The Jump and Volatility Jump factors are highly correlated (-74%) and capture major financial crises.", "Volatility Jump performs better than Volatility itself in explaining option returns.",]
lessons_learned = [ "Crisis-related factors are crucial in explaining the cross-sectional variation in index option returns.", "Daily adjustment of option portfolios for beta, maturity, and moneyness reduces variance and skewness, making them suitable for linear factor models.", "Incorporating additional factors like Liquidity can further reduce pricing errors, though some abnormal returns remain unexplained.", "Short-maturity out-of-the-money puts are particularly sensitive to market conditions.", "The methodology allows for testing pricing models with factor premia estimated from both equities and options.",]
data_sources = [ "Berkeley Options Database (April 1986 - December 1995)", "OptionMetrics database (January 1996 - January 2012)",]
cleaning_steps = [ "Construction of a cross-section of portfolios of options with different moneyness and maturity.", "Daily adjustment of portfolios to maintain targeted beta, maturity, and moneyness.", "Filtering option prices to ensure data quality and consistency across databases.",]

[Das2023]
description = "The paper introduces TimesFM, a foundation model for time-series forecasting that achieves near state-of-the-art zero-shot accuracy on previously unseen datasets. TimesFM is built using a decoder-style attention model with input patching, trained on a large corpus of real-world and synthetic time-series data. The model demonstrates strong performance across various domains, forecasting horizons, and temporal granularities, offering a practical alternative to large language models for time-series forecasting. The study highlights the potential of pretraining models specifically on time-series data for efficient and accurate forecasting."
empirical_facts = [ "TimesFM achieves close to state-of-the-art zero-shot accuracy on diverse time-series datasets without additional training.", "The model is significantly smaller than large language models, with 200M parameters and a pretraining data size of O(100B) timepoints.", "TimesFM outperforms models like GPT-3 and LLaMA-2 in zero-shot forecasting at a fraction of their computational cost.",]
lessons_learned = [ "Pretraining on a large and diverse corpus of time-series data can yield effective zero-shot forecasting models.", "Decoder-only architectures with input patching are well-suited for time-series forecasting tasks.", "Foundation models trained specifically on time-series data can outperform general-purpose large language models in forecasting tasks.",]
data_sources = [ "Real-world time-series data from web search queries and Wikipedia page visits", "Synthetic time-series data",]
cleaning_steps = "- Not applicable"

[Das2023a]
description = "The paper introduces the Time-series Dense Encoder (TiDE), a Multi-layer Perceptron (MLP) based model for long-term time-series forecasting. TiDE combines the simplicity and speed of linear models with the ability to handle covariates and non-linear dependencies. The model is shown to outperform or match state-of-the-art models on popular benchmarks while being significantly faster. Theoretical analysis demonstrates that a linear analogue of TiDE achieves near-optimal error rates under certain conditions."
empirical_facts = [ "TiDE achieves over 10% lower Mean Squared Error on the largest dataset compared to prior models.", "The model is 5-10 times faster than the best Transformer-based models in terms of inference.", "TiDE is more than 10 times faster in training compared to the best Transformer-based models.",]
lessons_learned = [ "Simple linear models can outperform complex Transformer-based models in long-term time-series forecasting.", "Incorporating non-linearities through MLPs can effectively handle covariates and non-linear dependencies.", "Efficient model architectures can significantly reduce computational costs while maintaining or improving performance.",]
data_sources = "- Not applicable"
cleaning_steps = "- Not applicable"

[Dau2019]
description = "The paper discusses the expansion of the UCR time series archive from 85 to 128 datasets, providing a critical resource for the time series data mining community. It highlights the archive's role in benchmarking algorithms and addresses criticisms regarding its design and assumptions. The authors argue that some improvements over baseline algorithms might be due to simple modifications rather than complex innovations. They also provide guidance on evaluating new algorithms using the archive."
empirical_facts = [ "The UCR time series archive expanded from 85 to 128 datasets.", "The archive has been cited approximately 850 times as of fall 2018.", "Baseline accuracies in the archive are based on 1-nearest neighbor classification using Euclidean distance and dynamic time warping.", "Criticisms of the archive include unrealistic assumptions, poor data provenance, and small dataset sizes.",]
lessons_learned = [ "Simple modifications to algorithms can sometimes achieve improvements over baseline methods.", "The choice of warping window width in dynamic time warping can significantly affect classification accuracy.", "Data normalization can mitigate issues related to offset and scale variance in time series data.", "The archive's single train/test split has been criticized but remains a standard for benchmarking.",]
data_sources = [ "UCR time series archive datasets",]
cleaning_steps = [ "Z-normalization of time series data to remove offset and scaling",]

[Dempster2025]
description = "The paper introduces MONSTER, a repository of large datasets for time series classification, aiming to address the limitations of existing benchmarks like UCR and UEA, which predominantly feature small datasets. The authors argue that current benchmarks favor models optimized for small datasets, thus limiting the exploration of methods suitable for larger datasets. By providing larger datasets, MONSTER seeks to encourage the development of scalable models and enhance the relevance of time series classification research to real-world problems. The paper highlights the potential for significant progress in the field by engaging with the challenges of learning from larger datasets."
empirical_facts = [ "The median training set size for UCR and UEA archives is just 217 examples.", "Current benchmarks favor low-variance (high-bias) methods optimized for small datasets.", "Deep learning methods have had limited impact in time series classification due to insufficient training data.", "The 'no free lunch' theorem suggests that no single method performs best across all datasets.",]
lessons_learned = [ "Benchmarks should reflect broader tasks and generalize to real-world problems.", "Current benchmarks may not adequately represent the challenges of large-scale time series classification.", "There is a need to diversify research to include methods suitable for larger datasets.", "The 'bitter lesson' suggests leveraging computation is crucial for long-term progress.",]
data_sources = [ "- Not applicable",]
cleaning_steps = [ "- Not applicable",]

[Deng2009]
description = "The paper introduces ImageNet, a large-scale hierarchical image database built on the WordNet structure, aiming to provide a comprehensive and diverse collection of images for each synset. At the time of publication, ImageNet consists of 5247 synsets and 3.2 million images, showcasing its scale, accuracy, and diversity compared to existing datasets. The authors describe the use of Amazon Mechanical Turk for data collection and demonstrate the utility of ImageNet in applications like object recognition and image classification. ImageNet's hierarchical structure and high-quality annotations offer significant opportunities for advancing computer vision research."
empirical_facts = [ "ImageNet contains 5247 synsets and 3.2 million images.", "ImageNet aims to provide 500-1000 images per synset, with an average of over 600 images collected per synset.", "ImageNet achieves an average labeling precision of 99.7% across different tree depths.", "ImageNet offers a denser and more diverse hierarchy compared to other datasets, with more categories and higher image quality.",]
lessons_learned = [ "Large-scale, high-quality image datasets like ImageNet are crucial for advancing computer vision research.", "Hierarchical organization of images can enhance the utility and applicability of image databases.", "Crowdsourcing platforms like Amazon Mechanical Turk can be effectively used for large-scale data collection and annotation.", "Ensuring diversity in image datasets is important for developing robust image recognition algorithms.",]
data_sources = [ "Images were collected from the Internet using search engines and organized using the WordNet structure.",]
cleaning_steps = [ "Intra-synset duplicate removal was performed to ensure each synset has over 10,000 unique candidate images.", "Human annotation and verification were used to achieve high labeling precision.",]

[Drechsler2017]
description = "The paper introduces the 'deposits channel' as a new mechanism through which monetary policy affects the economy. It demonstrates that when the Federal Reserve raises the Fed funds rate, banks increase the spreads on deposits, leading to outflows from the banking system, particularly in concentrated markets. This channel impacts both the supply of liquid assets to households and the stability of bank funding, ultimately influencing bank lending. The authors provide empirical evidence supporting the deposits channel, showing that it can account for the entire transmission of monetary policy through bank balance sheets."
empirical_facts = [ "Deposit spreads increase with the Fed funds rate, leading to significant deposit outflows.", "The effect of monetary policy on deposit spreads and outflows is stronger in concentrated markets.", "A 100 basis points increase in the Fed funds rate leads to a 14 bps increase in spreads on savings deposits and a 66 bps larger deposit outflow in high-concentration areas.", "A typical 400 bps Fed hiking cycle is expected to generate a $1.2 trillion outflow of deposits.", "Banks in concentrated markets reduce lending more significantly when the Fed funds rate rises.",]
lessons_learned = [ "Market power in deposit markets allows banks to widen spreads without losing deposits to cash when the Fed funds rate rises.", "The deposits channel provides a new perspective on how monetary policy affects the supply of liquid assets and bank lending.", "The deposits channel can explain the strong relationship between the Fed funds rate and the liquidity premium.", "Banks' ability to raise deposit spreads is a key measure of their exposure to monetary policy.", "The deposits channel has significant implications for understanding the transmission of monetary policy through bank balance sheets.",]
data_sources = [ "Aggregate, bank-level, and branch-level data for the United States", "Call Reports data", "Small business lending data",]
cleaning_steps = [ "Exploited geographic variation in local deposit market concentration to address identification challenges.", "Controlled for bank-specific lending opportunities by comparing branches of the same bank in different concentration areas.", "Used event study methodology to analyze the timing of changes in deposit spreads.",]

[Du2018]
description = "The paper investigates deviations from the covered interest rate parity (CIP) condition, revealing significant and persistent arbitrage opportunities in major currency markets post-global financial crisis. These deviations are not accounted for by credit risk or transaction costs but are linked to banking regulations and international funding imbalances. The study highlights the impact of post-crisis regulatory reforms on CIP arbitrage and explores the correlation between CIP deviations, fixed income spreads, and nominal interest rates."
empirical_facts = [ "CIP deviations have been persistent among G10 currencies since the 2008 financial crisis, leading to significant arbitrage opportunities.", "The average annualized absolute value of the cross-currency basis is 24 basis points at the three-month horizon and 27 basis points at the five-year horizon from 2010 to 2016.", "CIP deviations are not explained by credit risk differences, as evidenced by the analysis of fully collateralized repo contracts and KfW bonds.", "The cross-currency basis is positively correlated with nominal interest rates and other liquidity spreads.",]
lessons_learned = [ "Post-crisis regulatory reforms have a causal impact on CIP deviations and arbitrage opportunities.", "Constraints on financial intermediaries and international funding imbalances contribute to persistent CIP deviations.", "CIP deviations increase towards quarter-ends due to tighter balance sheet constraints faced by banks.", "The cross-currency basis is influenced by the shadow costs of banks' balance sheets and is correlated with nominal interest rates.",]
data_sources = [ "Bank of International Settlements reports on FX forward and swap markets.", "Libor interest rates and credit default spreads of banks.", "General collateral repurchase agreements (repo) and Kreditanstalt für Wiederaufbau (KfW) bonds.",]
cleaning_steps = "- Not applicable"

[Du2023]
description = "The paper examines a significant regime change in the Treasury market following the Global Financial Crisis (GFC), where dealers shifted from net short to net long positions in Treasury bonds. It introduces 'net-long' and 'net-short' curves that incorporate balance sheet and financing costs, demonstrating that actual yields transitioned from the net short to the net long curve post-GFC. The study links this regime shift to negative swap spreads and co-movement among swap spreads, dealer positions, and covered-interest-parity (CIP) violations. The authors propose that Treasury supply is a plausible driver of this regime change."
empirical_facts = [ "Pre-GFC, primary dealers maintained a net short position in Treasury bonds, resulting in positive swap spreads.", "Post-GFC, primary dealers switched to a net long position, leading to negative swap spreads.", "CIP deviations were almost non-existent before the GFC but became significant and persistent afterward.", "The regime shift caused co-movement among swap spreads, dealer positions, and CIP violations.",]
lessons_learned = [ "Monetary and regulatory policies have regime-dependent effects on the Treasury market.", "The supply of Treasury bonds can significantly influence market regimes and swap spreads.", "Understanding dealer positions is crucial for interpreting changes in swap spreads and CIP deviations.",]
data_sources = "- Not applicable"
cleaning_steps = "- Not applicable"

[Duffie1999]
description = "The paper by Darrell Duffie provides a comprehensive review of the pricing mechanisms for credit swaps, a type of derivative security that acts as default insurance on loans or bonds. It delves into the structure and valuation of credit swaps, explaining the contingent payments triggered by credit events like defaults. The paper also explores the calculation of credit-swap spreads and the role of asset swaps, providing insights into the complexities of pricing these financial instruments. The analysis is aimed at both understanding the theoretical underpinnings and addressing practical challenges in the market."
empirical_facts = [ "Credit swaps are a form of derivative security that provides default insurance on loans or bonds.", "A credit swap involves a contingent payment triggered by a credit event, such as the default of an underlying bond.", "The pricing of credit swaps involves determining the at-market annuity premium rate, known as the credit-swap spread.", "Credit swaps can be combined with interest rate swaps, affecting the quoted credit swap spread.", "The market value of a credit swap changes after origination due to shifts in market interest rates and credit quality.",]
lessons_learned = [ "Understanding the basic structure of credit swaps is crucial for pricing and risk management.", "The credit-swap spread is a key component in determining the market value of a credit swap.", "Combining credit swaps with interest rate swaps introduces additional complexity in pricing.", "Market conditions such as interest rates and credit quality significantly impact the valuation of credit swaps.", "Documentation and enforceability risks are important considerations in the valuation of credit swaps.",]
data_sources = "- Not applicable"
cleaning_steps = "- Not applicable"

[Elton1996]
description = "The paper investigates the impact of survivorship bias on mutual fund performance by tracking all funds existing at the end of 1976, including those that disappeared due to mergers. The authors provide a precise estimate of survivorship bias by calculating returns accounting for merger terms and comparing samples with and without survivorship bias. They find that ignoring fund attrition overstates mutual fund performance and that survivorship bias varies depending on the evaluation model and time horizon. This study is unique in its comprehensive tracking of fund performance, termed 'follow the money,' which ensures a sample free of survivorship bias."
empirical_facts = [ "Survivorship bias can significantly overstate mutual fund performance by excluding poorly performing funds that disappear.", "The study tracks 361 funds from 1976, finding that 42 of 207 funds with over $15 million in assets merged by 1993.", "Survivorship bias estimates vary, with previous studies suggesting increases in returns by 10 to 150 basis points annually.",]
lessons_learned = [ "Ignoring fund attrition can lead to an overestimation of mutual fund returns, impacting investment decisions.", "Different mutual fund objectives may experience varying rates of attrition, affecting performance analysis.", "Comprehensive tracking of fund performance, including post-merger returns, is crucial for accurate performance evaluation.",]
data_sources = [ "Wiesenberger's Investment Companies (1977 edition)", "Data from Micropal, Ivessco, Salomon Brothers, Lehman Brothers, and Interactive Data Corporation",]
cleaning_steps = [ "Tracked each fund with over $15 million in assets from 1976 to 1993 to ensure comprehensive data.", "Calculated returns for merged funds using actual merger terms to maintain accuracy.", "Divided the sample by fund size to manage tracking and analysis effectively.",]

[Fama1993]
description = "The paper by Fama and French identifies five common risk factors that influence the returns on stocks and bonds. These include three stock-market factors: an overall market factor, firm size, and book-to-market equity, and two bond-market factors related to maturity and default risks. The study finds that these factors explain the shared variation in stock and bond returns, with the bond-market factors capturing the common variation in bond returns, except for low-grade corporates. Overall, the five factors provide a comprehensive explanation for average returns on stocks and bonds."
empirical_facts = [ "The study identifies three stock-market factors: an overall market factor, firm size, and book-to-market equity.", "Two bond-market factors are identified, related to maturity and default risks.", "The five factors collectively explain the average returns on stocks and bonds.", "Bond-market factors capture common variation in bond returns, except for low-grade corporates.",]
lessons_learned = [ "Traditional asset-pricing models may not fully capture the cross-section of average returns on stocks.", "Empirically determined variables like size and book-to-market equity have significant explanatory power for average returns.", "Understanding common risk factors can improve the explanation of stock and bond return variations.",]
data_sources = "- Not applicable"
cleaning_steps = "- Not applicable"

[Fleckenstein2014]
description = "The paper investigates the persistent mispricing between Treasury bonds and Treasury Inflation-Protected Securities (TIPS), finding that Treasury bonds are consistently overvalued relative to TIPS. This mispricing represents a significant arbitrage opportunity, with price differences often exceeding $20 per $100 notional. The study supports the slow-moving-capital hypothesis, showing that mispricing narrows as more capital flows into the market. It also highlights the implications for public finance, suggesting that the U.S. Treasury could save money by issuing nominal bonds instead of TIPS."
empirical_facts = [ "The mispricing between Treasury bonds and TIPS can exceed $20 per $100 notional.", "The total TIPS-Treasury mispricing has exceeded $56 billion, representing nearly 8% of the total amount of TIPS outstanding.", "The average size of the mispricing is 54.5 basis points, but it can exceed 200 basis points for some pairs.", "The mispricing is not due to inflation swap market mispricing, as no mispricing is found when applying the same strategy to corporate bonds.", "TIPS-Treasury mispricing is affected by funding liquidity factors such as the availability of Treasury collateral in the repo market.",]
lessons_learned = [ "Treasury bonds are perceived to have special attributes like liquidity and safety, driving their higher prices relative to TIPS.", "The slow-moving-capital hypothesis explains the persistence of arbitrage mispricing, as capital flows slowly into the market.", "Changes in capital available to arbitrageurs can forecast future changes in mispricing.", "The Treasury-TIPS price differentials cannot reliably indicate market inflation expectations due to persistent mispricing.", "Issuing nominal bonds instead of TIPS could save the U.S. Treasury billions of dollars.",]
data_sources = [ "Daily prices for 29 maturity-matched pairs of TIPS issues and Treasury bonds from 2004 to 2009.",]
cleaning_steps = "- Not applicable"

[Fleckenstein2020]
description = "The paper investigates the persistent basis in the interest rate futures market, attributing it to the cost of renting intermediary balance sheet space. The authors find that this basis is linked to balance sheet costs from debt overhang problems and capital regulation, extending similar bases in other large financial markets. Their analysis suggests that these costs play a significant role in the mispricing of derivatives relative to underlying securities, challenging traditional asset pricing models."
empirical_facts = [ "The average funding basis in the interest rate futures market is 58.70 basis points, reaching up to 200 basis points during financial crises.", "The funding basis is significantly related to the turn-of-the-year premium in Libor rates, indicating balance sheet usage costs.", "Debt overhang costs and capital regulation are major factors influencing the funding basis throughout the 1991-2018 period.",]
lessons_learned = [ "Intermediary balance sheet costs are crucial in explaining the mispricing of derivatives.", "Debt overhang and capital regulation significantly impact balance sheet rental costs.", "The cost of renting balance sheet space can resolve several asset pricing puzzles.",]
data_sources = [ "Bank for International Settlements (BIS) reports", "Eurodollar (Libor) futures prices", "Treasury note and Treasury note futures markets data",]
cleaning_steps = [ "Identification of the turn-of-the-year effect on Libor rates using eurodollar futures prices.", "Calculation of the funding basis as the difference between implied repo rates and actual repo rates.",]

[FleckensteinFrancisLongstaffHannoLustig2010]
description = "The paper investigates the persistent mispricing between Treasury bonds and Treasury Inflation-Protected Securities (TIPS), highlighting a significant arbitrage opportunity. It finds that Treasury bonds are consistently overvalued compared to TIPS, with mispricing exceeding $56 billion. The study links this mispricing to supply factors and other fixed-income arbitrages, challenging classical asset pricing theories. The authors question the rationale behind the Treasury's issuance of TIPS, as it appears to forgo fiscal benefits and incur higher costs."
empirical_facts = [ "The mispricing between Treasury bonds and TIPS has exceeded $56 billion.", "Treasury bonds are consistently overpriced relative to TIPS by more than $20 per $100 notional.", "The average size of the TIPS-Treasury arbitrage is 54.5 basis points, sometimes exceeding 200 basis points.", "The mispricing is strongly influenced by supply factors such as Treasury debt issuance and repo failures.", "There is a strong correlation between TIPS-Treasury mispricing and other fixed-income arbitrages.",]
lessons_learned = [ "Classical asset pricing theories struggle to explain the persistent arbitrage opportunities between TIPS and Treasury bonds.", "The issuance of TIPS by the Treasury may not be fiscally optimal, as it appears to leave money on the table.", "Supply factors and market frictions play a significant role in the mispricing of TIPS and Treasury bonds.", "The TIPS-Treasury price differentials should not be used to infer market inflation expectations due to inherent biases.", "Arbitrage opportunities can provide insights into market dynamics that are not captured by traditional asset-pricing models.",]
data_sources = [ "Daily prices for 29 matched-maturity pairs of TIPS issues and Treasury bonds from July 2004 to November 2009.",]
cleaning_steps = [ "- Not applicable",]

[Fung2000]
description = "The paper by Fung and Hsieh explores the biases in measuring the performance of hedge funds and commodity funds, emphasizing the challenges posed by their private and often offshore nature. The authors propose using funds-of-hedge funds to estimate aggregate hedge fund performance, which can help mitigate biases such as selection, instant history, and survivorship biases. They highlight the importance of understanding these biases to provide a more accurate picture of hedge fund performance and suggest practical methods to adjust for these biases using available databases."
empirical_facts = [ "Hedge funds typically exhibit low correlation with standard asset indices.", "There are multiple hedge fund styles, each with distinct return characteristics.", "Survivorship bias in hedge funds can lead to an upward-biased measure of performance.", "Commodity funds have a higher dropout rate (19% per year) compared to mutual funds (5% per year).", "The survivorship bias in commodity funds averages 3.4% per year.",]
lessons_learned = [ "Understanding biases in hedge fund performance measurement is crucial for accurate evaluation.", "Funds-of-hedge funds can serve as a practical tool to estimate aggregate hedge fund performance.", "Adjusting for selection, instant history, and survivorship biases is necessary to obtain a true picture of hedge fund returns.", "The lack of comprehensive data on defunct hedge funds poses challenges in performance measurement.", "Equally-weighted portfolios, while not perfect, are often used due to the lack of complete data on assets under management.",]
data_sources = [ "TASS Investment Research", "MAR/Hedge", "Hedge Fund Research (HFR)",]
cleaning_steps = [ "Adjusting for selection bias by ensuring the database contains a representative sample of hedge funds.", "Correcting for instant history bias by accounting for backfilled performance data.", "Estimating and adjusting for survivorship bias by comparing observable and surviving portfolios.",]

[Godahewa2021]
description = "The paper introduces the Monash Time Series Forecasting Archive, a comprehensive collection of 20 publicly available time series datasets from various domains. It aims to address the lack of benchmarking archives for evaluating global forecasting models by providing datasets with varied characteristics, such as frequency and missing values. The archive also introduces a new data storage format, .tsf, and provides baseline forecasting model evaluations across eight error metrics. The archive and its resources are available for researchers to benchmark and improve their forecasting algorithms."
empirical_facts = [ "The archive contains 20 primary datasets, resulting in 50 dataset variations based on frequency and missing values.", "Datasets cover multiple domains, including tourism, banking, web, energy, sales, economics, transportation, health, and nature.", "The archive includes both real-world and competition datasets, with 8 originating from competition platforms.", "Baseline forecasting models were evaluated across eight error metrics, and results are publicly available.", "The new .tsf file format is introduced for storing time series data, offering flexibility and non-redundancy.",]
lessons_learned = [ "Global forecasting models can leverage cross-series information, offering advantages over traditional univariate models.", "A comprehensive benchmarking archive is crucial for advancing research in time series forecasting.", "The new .tsf format provides an efficient way to store and manage time series data with meta-information.", "Feature analysis can help identify similarities and differences among time series datasets, aiding model development.", "Publicly available resources, such as datasets and baseline evaluations, can significantly benefit the research community.",]
data_sources = [ "Competition platforms (e.g., M1, M3, M4 datasets)", "R packages", "Kaggle platform", "Johns Hopkins repository", "Domain-specific platforms",]
cleaning_steps = [ "Imputation techniques were used to handle missing values in datasets with missing data.",]

[Griliches1986]
description = "The paper by Zvi Griliches discusses the challenges and issues associated with economic data in econometrics. It highlights the inherent imperfections in economic data, which are often collected by governmental bodies as a by-product of other activities, and the separation between data collection and analysis. The paper emphasizes the importance of understanding both the economic behavior model and the measurement model in data analysis. It also discusses the progress made in data availability and the development of new subfields in econometrics, despite the persistent challenges."
empirical_facts = [ "Economic data collection often originates from governmental activities like tax and customs collections.", "There is a separation between data collection and analysis, with econometricians typically not involved in the primary data collection process.", "Recent years have seen the development of longitudinal microdata sets and computerized databases, improving data accessibility.", "Significant progress has been made in labor force and income data, but firm-level data remains limited.",]
lessons_learned = [ "Econometricians must consider both the economic behavior model and the measurement model in data analysis.", "The imperfections in economic data provide challenges but also opportunities for econometricians to develop new methodologies.", "Involvement in primary data collection is complex and requires careful consideration of question design and data relevance.", "The availability of larger data sets leads to more complex models, which can still suffer from issues like insignificance and measurement errors.",]
data_sources = [ "Longitudinal microdata sets such as the Michigan PSID tapes and Ohio State NLS surveys.", "Public Use Samples from the U.S. Population Census and the Current Population Surveys.", "CRISP and Compustat databases for financial data and security prices.",]
cleaning_steps = "- Not applicable"

[Gu2020]
description = "The paper conducts a comparative analysis of machine learning methods for measuring asset risk premiums, demonstrating significant economic gains for investors using these forecasts. It identifies trees and neural networks as the best-performing methods due to their ability to capture nonlinear predictor interactions. The study highlights the potential of machine learning to enhance empirical asset pricing by accommodating a wide range of predictor variables and complex functional forms. The research underscores the practical applications of machine learning in investment strategies, such as market timing and portfolio choice."
empirical_facts = [ "Machine learning methods, particularly trees and neural networks, significantly outperform traditional regression-based strategies in predicting asset risk premiums.", "A portfolio strategy using neural network forecasts for the S&P 500 achieves an annualized out-of-sample Sharpe ratio of 0.77, compared to 0.51 for a buy-and-hold strategy.", "A long-short decile spread strategy based on stock-level neural network forecasts earns an annualized out-of-sample Sharpe ratio of 1.35, more than doubling the performance of leading regression-based strategies.", "Machine learning methods agree on a dominant set of predictive signals, including variations on momentum, liquidity, and volatility.", "The study investigates nearly 30,000 individual stocks over 60 years, using a predictor set that includes over 900 baseline signals.",]
lessons_learned = [ "Machine learning methods provide substantial improvements in predicting asset returns by capturing complex nonlinear interactions among predictors.", "The flexibility of machine learning allows for the inclusion of a vast number of predictor variables, enhancing the accuracy of risk premium measurement.", "Machine learning can effectively address empirical challenges in asset pricing, such as high-dimensional predictor sets and ambiguous functional forms.", "The practical applications of machine learning in finance justify its growing role in the fintech industry, particularly for tasks like market timing and portfolio choice.",]
data_sources = [ "Data on nearly 30,000 individual stocks from 1957 to 2016.", "Predictor set includes 94 characteristics for each stock, interactions with eight aggregate time-series variables, and 74 industry sector dummy variables.",]
cleaning_steps = "- Not applicable"

[Harvey2016]
description = "The paper by Harvey, Liu, and Zhu introduces a new framework for evaluating the statistical significance of factors in asset pricing research, arguing that traditional significance thresholds are inadequate due to extensive data mining. They propose a higher threshold for significance, with a t-statistic greater than 3.0, to account for multiple testing issues. The authors provide historical cutoffs for significance from 1967 to the present and project future thresholds, suggesting that many claimed findings in financial economics may be false."
empirical_facts = [ "The paper analyzes 313 papers and identifies 316 factors related to cross-sectional return patterns.", "Historical significance thresholds are provided, with a recommended t-statistic greater than 3.0 for new factors.", "The study projects minimum t-statistics through 2032, assuming the current rate of factor discovery continues.",]
lessons_learned = [ "Traditional significance levels are inadequate due to the large number of factors tested in asset pricing research.", "A multiple testing framework is essential to accurately evaluate the significance of new factors.", "Economic theory-derived factors should have a lower significance hurdle compared to empirically derived factors.",]
data_sources = [ "- Not applicable",]
cleaning_steps = [ "- Not applicable",]

[He2017]
description = "The paper investigates the role of financial intermediaries, specifically primary dealers, in asset pricing across various asset classes. It finds that shocks to the equity capital ratio of these intermediaries have significant explanatory power for the variation in expected returns across different markets, including equities, bonds, derivatives, commodities, and currencies. The study suggests that intermediary capital risk is procyclical and that primary dealers act as marginal investors in these markets, providing a unified perspective on asset pricing. The findings challenge traditional consumption-based models by emphasizing the importance of intermediary capital in understanding risk premia."
empirical_facts = [ "Shocks to the equity capital ratio of primary dealers explain cross-sectional differences in expected returns across seven asset classes.", "Intermediary capital risk is procyclical, indicating countercyclical leverage among intermediaries.", "The estimated price of risk on intermediary capital shocks is consistently positive across all asset classes.", "Primary dealers are identified as marginal investors in multiple asset markets, influencing asset prices.", "Placebo tests show that non-primary dealers and nonfinancial firms do not exhibit similar pricing power.",]
lessons_learned = [ "Financial intermediaries play a crucial role in asset pricing across diverse markets.", "Intermediary capital ratios serve as a significant factor in explaining asset returns, challenging traditional household-based models.", "The procyclical nature of intermediary capital risk suggests that intermediaries have higher marginal value of wealth during low capital periods.", "A unified pricing kernel based on intermediary capital can effectively explain asset returns across different classes.", "Differences in risk prices across asset classes may indicate specialization among intermediaries, although the study assumes a homogeneous intermediary model.",]
data_sources = [ "New York Fed's primary dealer list", "CRSP/Compustat data", "Datastream data",]
cleaning_steps = [ "Matched primary dealer list with publicly traded holding companies data.", "Calculated the intermediary capital ratio using market equity and book debt data.",]

[Hu2018]
description = "The paper introduces FinTSB, a comprehensive benchmark for financial time series forecasting, addressing key limitations in existing evaluation frameworks. It categorizes movement patterns, standardizes evaluation metrics, and incorporates real-world trading constraints to enhance the robustness and applicability of empirical assessments. Extensive experiments demonstrate the platform's ability to guide model selection under varying market conditions, providing a novel tool for improving and evaluating financial time series forecasting methods."
empirical_facts = [ "Existing evaluation frameworks for financial time series forecasting face issues such as Diversity Gap, Standardization Deficit, and Real-World Mismatch.", "FinTSB categorizes movement patterns into four specific parts and standardizes metrics across three dimensions: ranking, portfolio, and error.", "The benchmark incorporates real-world trading constraints like transaction fees and market regulations to simulate realistic scenarios.",]
lessons_learned = [ "A comprehensive benchmark like FinTSB can significantly enhance the evaluation of financial time series forecasting methods.", "Standardizing evaluation metrics is crucial for fair and consistent performance comparisons across different models.", "Incorporating real-world trading constraints is essential for accurately assessing the practical applicability of forecasting methods.",]
data_sources = [ "Historical financial stock data from multiple financial markets.",]
cleaning_steps = [ "Tokenization of sensitive information.", "Pre-processing of historical stock data to mitigate distribution shifts.", "Categorization of data into distinct movement patterns based on daily change rates.",]

[Jensen2023]
description = "The paper challenges the notion of a replication crisis in financial economics by developing a Bayesian model to assess the replicability of asset pricing factors. The authors find that most factors can be replicated and are significant in a global context, covering 93 countries. They argue that the large number of observed factors strengthens rather than weakens the evidence for these factors. The study also introduces a new global dataset and makes it publicly accessible to advance replication efforts in finance."
empirical_facts = [ "The replication rate of asset pricing factors rises from 35% to 82.4% when using a Bayesian approach compared to traditional OLS methods.", "The study identifies 153 factors across 93 countries, demonstrating significant out-of-sample performance.", "The Bayesian model shows a posterior Bayesian false discovery rate (FDR) of only 0.1%, indicating a low risk of false discoveries.", "Factors are clustered into 13 themes, most of which are significant parts of the tangency portfolio.",]
lessons_learned = [ "A Bayesian approach can provide a more nuanced understanding of factor replicability compared to traditional frequentist methods.", "The large number of factors in financial economics can be a strength, as it allows for more robust statistical inferences.", "Global data can enhance the external validity of financial research findings.", "Replication efforts benefit from making data and methodologies publicly accessible.",]
data_sources = [ "Global dataset of 153 factors across 93 countries", "Data available at https://jkpfactors.com and through Wharton Research Data Services (WRDS)",]
cleaning_steps = [ "Use of tercile spreads instead of deciles for factor construction", "Exclusion of factors based on IBES data due to short history", "Use of capped value-weighting instead of straight value-weights", "Lagging accounting data by four months consistently",]

[Jermann2020]
description = "The paper by Urban J. Jermann addresses the phenomenon of negative swap spreads, where fixed rates for 30-year interest rate swaps have been below Treasury rates since 2008. It presents a model that incorporates frictions in bond holding, which limits arbitrage and explains why negative swap spreads are not surprising. The model shows that these spreads can align with empirical observations without needing large demand imbalances, and it highlights the role of regulatory changes post-financial crisis in sustaining these negative spreads."
empirical_facts = [ "Since October 2008, 30-year interest rate swap spreads have been negative, with swap rates below Treasury rates.", "The Bank for International Settlements reported a notional amount of 320 trillion USD for interest rate swaps in the first half of 2015.", "Empirical evidence shows a negative relation between term spreads and swap spreads, consistent with the model's predictions.", "Swap spreads declined sharply in 2015, coinciding with banks' disclosure of supplementary leverage ratios.",]
lessons_learned = [ "Negative swap spreads can occur naturally in the presence of market frictions that limit arbitrage opportunities.", "Regulatory changes, such as increased capital requirements, have made it more costly for banks to hold Treasuries, contributing to negative swap spreads.", "The model suggests that swap spreads should be negative when the TED spread is smaller than the term spread.", "Limited arbitrage models can provide insights into pricing anomalies in financial markets.",]
data_sources = "- Not applicable"
cleaning_steps = "- Not applicable"

[Jorion2019]
description = "The paper by Jorion and Schwarz addresses the issue of backfill bias in hedge fund databases, highlighting the inadequacy of common methods like truncation for removing this bias. They propose using listing dates to eliminate backfill bias and introduce a novel method to infer these dates when unavailable. Their findings show that backfill bias can significantly distort empirical results, affecting cross-sectional analyses and fund characteristics, but not performance persistence or flow-performance relations."
empirical_facts = [ "Backfill bias can overstate average returns by 0.50% per month.", "Truncation methods leave a significant portion of backfilled returns, with 35% or 31% remaining after truncating 12 or 24 months, respectively.", "42% of returns in TASS or HFR databases are backfilled without any removal.", "Cross-sectional relations, such as between fund age, size, and performance, are biased by backfilled returns.", "Performance persistence and flow-performance relations are robust to backfill bias.",]
lessons_learned = [ "Listing dates should be used to control for backfill bias in hedge fund data.", "Truncation methods are insufficient for removing backfill bias due to heterogeneous backfill periods.", "Backfill bias can lead to misleading empirical results, particularly in cross-sectional analyses.", "The proposed method for inferring listing dates can effectively correct for backfill bias in databases lacking this information.", "Combining databases using the earliest add date for funds listed in multiple databases can preserve unbiased returns.",]
data_sources = [ "Hedge Fund Research (HFR)", "TASS", "BarclayHedge", "Morningstar",]
cleaning_steps = [ "Use listing dates to remove backfilled returns.", "Infer listing dates using sequential identification numbers when not available.", "Group funds by similar identification numbers to establish add dates based on overlapping returns.",]

[Kelly2023]
description = "The paper by Bryan T. Kelly and Dacheng Xiu surveys the emerging literature on machine learning applications in financial markets. It highlights the potential of machine learning to handle large information sets and ambiguous functional forms, which are common in finance. The paper aims to bridge the gap between financial economists and machine learning experts by showcasing successful applications and suggesting future research directions. It emphasizes the dual benefit of machine learning models in enhancing scientific understanding and improving real-world financial decision-making."
empirical_facts = [ "Asset prices are predictions reflecting investors' expectations of future payoffs.", "Financial markets involve large conditioning information sets and ambiguous functional forms.", "Machine learning methods can integrate diverse data sources and approximate unknown data-generating functions.", "The literature has identified hundreds of stock-level predictive characteristics and dozens of macroeconomic predictors.",]
lessons_learned = [ "Machine learning offers tools to handle the complexity and high dimensionality of financial data.", "Flexible models can accommodate various functional forms and non-linear interactions in financial data.", "Integrating large information sets into models can make them more realistic and reflective of market dynamics.", "Machine learning can help bridge the gap between the information sets available to researchers and market participants.",]
data_sources = "- Not applicable"
cleaning_steps = "- Not applicable"

[Lettau2014]
description = "The paper introduces the downside risk capital asset pricing model (DR-CAPM) to explain the cross section of currency returns, highlighting that high-yield currencies exhibit stronger co-movement with market returns during downturns. This model successfully extends to equities, commodities, sovereign bonds, and other assets, offering a unified risk perspective across these classes. The DR-CAPM outperforms traditional models that are limited to specific asset classes by jointly rationalizing returns across diverse markets."
empirical_facts = [ "High-yield currencies earn higher excess returns than low-yield currencies, especially during market downturns.", "The DR-CAPM model explains the cross section of returns across various asset classes, not just currencies.", "Traditional models fail to jointly price multiple asset classes effectively.",]
lessons_learned = [ "Accounting for downside risk is crucial in understanding asset returns across different markets.", "A unified risk model can provide better insights than asset-specific models.", "Market conditions significantly affect the risk premium associated with different asset classes.",]
data_sources = [ "Data shared by George Constantinides, Eugene Fama, Kenneth French, Andrea Frazzini, Jens Jackwerth, Yoshio Nozawa, Lasse Pedersen, Alexi Savov, Fan Yang, and Adrien Verdelhan.",]
cleaning_steps = "- Not applicable"

[Lim2021]
description = "The paper introduces the Temporal Fusion Transformer (TFT), an attention-based architecture designed for interpretable multi-horizon time series forecasting. TFT combines high-performance forecasting with insights into temporal dynamics by using recurrent layers for local processing and self-attention layers for long-term dependencies. It employs specialized components like static covariate encoders and gating mechanisms to enhance feature selection and suppress irrelevant inputs. The model demonstrates significant performance improvements over existing benchmarks and provides interpretability through practical use cases."
empirical_facts = [ "TFT shows significant performance improvements over existing benchmarks in multi-horizon forecasting.", "The architecture effectively handles a variety of input types, including static covariates and time-varying inputs.", "TFT provides interpretability by identifying globally-important variables, persistent temporal patterns, and significant events.",]
lessons_learned = [ "Attention-based models can enhance interpretability in time series forecasting by highlighting relevant temporal dynamics.", "Incorporating specialized components like static covariate encoders and gating mechanisms can improve model performance and interpretability.", "Designing models with suitable inductive biases for multi-horizon forecasting can lead to significant performance gains.",]
data_sources = "- Not applicable"
cleaning_steps = "- Not applicable"

[Makridakis1982]
description = "The paper presents the results of a large-scale forecasting competition aimed at evaluating the accuracy of various extrapolative (time series) methods. It involved seven experts using 24 different methods to forecast up to 1001 time series across different time horizons. The study found significant differences in accuracy among the methods, highlighting that no single method consistently outperformed others across all scenarios. The findings emphasize the importance of selecting a forecasting method based on specific situational needs rather than seeking a universally 'best' method."
empirical_facts = [ "The competition involved 24 forecasting methods applied to up to 1001 time series.", "Forecasting horizons varied from 6 to 18 periods depending on the data frequency (monthly, quarterly, yearly).", "Accuracy was assessed using multiple measures, including Mean Average Percentage Error (MAPE), Mean Square Error (MSE), and others.", "The study found that more complex methods were not necessarily more accurate than simpler ones.", "The Box-Jenkins method required the most time for model fitting, while other methods were automated.",]
lessons_learned = [ "There is no universally best forecasting method; the choice depends on the specific context and requirements.", "Complexity in forecasting methods does not guarantee higher accuracy.", "Empirical evaluations are crucial for understanding the strengths and weaknesses of different forecasting methods.", "Automated methods can provide reliable forecasts with less human intervention and time investment.", "Forecasting accuracy can lead to significant practical benefits and cost savings.",]
data_sources = [ "Time series data covering various domains such as macro, micro, industry, and demographic data.", "Data were collected from firms, industries, and nations, with different starting and ending dates.",]
cleaning_steps = [ "Excluded series with MAPE greater than 1000% from accuracy calculations.", "Used a systematic random sample of 111 series for some methods due to time constraints.",]

[Makridakis2000]
description = "The M3-Competition is an extensive empirical study comparing the forecasting accuracy of various time series methods across 3003 series. The study confirms previous findings that simpler methods can outperform complex ones and that combining methods often yields better accuracy. The competition also highlights the variability in method performance depending on the accuracy measure and forecasting horizon. The implications for forecasting theory and practice are significant, suggesting a need for more empirical validation in the field."
empirical_facts = [ "Statistically sophisticated methods do not necessarily provide more accurate forecasts than simpler methods.", "The performance ranking of forecasting methods varies with the accuracy measure used.", "Combining different forecasting methods generally results in better accuracy than individual methods.", "The accuracy of forecasting methods is influenced by the length of the forecasting horizon.",]
lessons_learned = [ "Empirical validation is crucial in assessing the effectiveness of forecasting methods.", "Simple forecasting methods can be surprisingly effective, challenging the preference for complex models.", "Combining forecasts from multiple methods can enhance accuracy.", "The choice of accuracy measure can significantly impact the perceived performance of forecasting methods.",]
data_sources = [ "3003 time series data covering various categories such as micro, industry, macro, finance, demographic, and others.",]
cleaning_steps = [ "Ensured all time series data were strictly positive, substituting negative forecasted values with zero to avoid issues with MAPE measures.",]

[Makridakis2018]
description = "The M4 Competition aimed to enhance forecasting accuracy by significantly increasing the number of series, incorporating machine learning methods, and evaluating both point forecasts and prediction intervals. The competition revealed that hybrid approaches combining statistical and machine learning features were the most effective, with the top method being 10% more accurate than the benchmark. Pure machine learning methods performed poorly in comparison. The findings suggest that combining methods, especially those integrating statistical and machine learning techniques, is crucial for improving forecasting accuracy."
empirical_facts = [ "Out of the 17 most accurate methods, 12 were combinations of mostly statistical approaches.", "The hybrid approach combining statistical and ML features was 10% more accurate than the combination benchmark.", "The second most accurate method was a combination of seven statistical methods and one ML method.", "The two most accurate methods correctly specified the 95% prediction intervals.", "Pure ML methods performed poorly, with none surpassing the combination benchmark.",]
lessons_learned = [ "Combining statistical and machine learning methods enhances forecasting accuracy.", "Hybrid approaches can significantly outperform traditional benchmarks.", "Pure machine learning methods may not always be the best choice for forecasting.", "Correctly specifying prediction intervals is a critical aspect of accurate forecasting.", "Open competitions like M4 can drive innovation and improvement in forecasting methods.",]
data_sources = [ "M4 dataset available at https://www.m4.unic.ac.cy/the-dataset/", "M4comp2018 R package",]
cleaning_steps = [ "Participants were required to deposit code and descriptions of their methods on GitHub for verification.", "The competition organizers checked the accuracy and reproducibility of proprietary methods.",]

[Makridakis2020]
description = "The M4 Competition significantly expanded upon previous forecasting competitions by including 100,000 time series and evaluating 61 forecasting methods. It introduced high-frequency data and prediction intervals, aiming to improve forecasting accuracy and the practice of forecasting. The competition's findings highlighted the limitations of pure machine learning methods and emphasized the importance of reproducibility and benchmarking in forecasting research. The M4 dataset is expected to serve as a valuable resource for testing new forecasting methods."
empirical_facts = [ "The M4 Competition included 100,000 time series across six data frequencies and six application domains.", "A total of 61 forecasting methods were evaluated, including both traditional and machine learning approaches.", "The competition found that pure machine learning methods performed poorly compared to other methods.", "The dataset was sourced from a larger database containing 900,000 continuous time series.", "Participants were required to provide both point forecasts and prediction intervals for evaluation.",]
lessons_learned = [ "Forecasting competitions can drive innovation and improve forecasting practices by providing empirical benchmarks.", "Reproducibility and transparency in forecasting research are crucial for advancing the field.", "High-frequency data and prediction intervals add valuable dimensions to forecasting accuracy assessments.", "Machine learning methods may require further refinement to outperform traditional methods in time series forecasting.",]
data_sources = [ "ForeDeCk database compiled at the National Technical University of Athens",]
cleaning_steps = [ "Time series were scaled to prevent negative observations and values lower than 10.", "Information that could lead to the identification of original series was removed.", "Low-volume and intermittent time series were excluded to maintain continuity with past competitions.",]

[Makridakis2022]
description = "The paper presents the results and findings of the M5 'Accuracy' competition, which aimed to improve forecasting accuracy for hierarchical unit sales data from Walmart. The competition involved predicting 42,840 time series and highlighted the effectiveness of LightGBM, a decision tree-based machine learning method, which was used by the top competitors. The study confirms previous findings that simple methods like exponential smoothing remain competitive and discusses the implications for retail forecasting practices."
empirical_facts = [ "The M5 competition focused on forecasting 42,840 time series of Walmart's hierarchical unit sales data.", "LightGBM was the most effective method, used by all top 50 competitors.", "Simple methods like exponential smoothing were still competitive, especially at the product or product-store level.", "The competition utilized the root mean squared scaled error (RMSSE) as the performance measure.",]
lessons_learned = [ "Machine learning methods, particularly LightGBM, can significantly enhance forecasting accuracy in retail sales.", "Simple and computationally inexpensive methods remain relevant and competitive.", "Forecasting methods need to be coherent across hierarchical levels to ensure accuracy.", "The choice of performance measure, such as RMSSE, is crucial for evaluating forecasting accuracy effectively.",]
data_sources = [ "Kaggle platform data set of Walmart's unit sales for 3,049 products.",]
cleaning_steps = [ "Ensured forecasts were coherent across hierarchical levels.", "Used RMSSE to handle intermittency and scale independence in the data.",]

[McCracken2016]
description = "The paper introduces FRED-MD, a comprehensive monthly macroeconomic database designed to facilitate empirical analysis by providing a standardized, easily accessible, and regularly updated dataset. The database, which includes 134 U.S. economic indicators, aims to simplify data management and enhance research reproducibility. The authors demonstrate that factors derived from FRED-MD have similar predictive capabilities to those from the Stock-Watson dataset. Additionally, the paper suggests that diffusion indexes from these factors could be valuable for analyzing business cycle chronology."
empirical_facts = [ "FRED-MD comprises 134 monthly U.S. economic indicators.", "The database is updated monthly and is publicly accessible.", "Factors extracted from FRED-MD have similar predictive content to those from the Stock-Watson dataset.",]
lessons_learned = [ "Standardized and regularly updated datasets can significantly reduce the overhead of macroeconometric analysis.", "Publicly accessible databases enhance the reproducibility and comparability of research findings.", "Diffusion indexes constructed from factor estimates can be useful for studying business cycle chronology.",]
data_sources = [ "Federal Reserve Economic Data (FRED)", "Global Insights Basic Economics Database (GSI)", "National Income and Product Accounts (NIPA)", "Bureau of Labor Statistics (BLS)", "International Monetary Fund (IMF)",]
cleaning_steps = [ "Adjusting series for inflation using price indices.", "Seasonal adjustment using ARIMA X12.", "Splicing series to account for changes in data definitions or availability.", "Comparing new and old data over overlapping samples to check for irregularities.",]

[Menkhoff2012]
description = "The paper investigates the relationship between global foreign exchange (FX) volatility risk and the returns from carry trade strategies, which involve borrowing in low interest rate currencies and investing in high interest rate currencies. It finds that high interest rate currencies are negatively related to innovations in global FX volatility, leading to low returns during times of unexpected high volatility, while low interest rate currencies provide a hedge. The study demonstrates that volatility risk is a more significant factor than liquidity risk in explaining carry trade returns, and that global FX volatility is a key driver of risk premia across various asset classes."
empirical_facts = [ "High interest rate currencies are negatively related to innovations in global FX volatility.", "Carry trades yield high returns, which are compensation for time-varying risk.", "Global FX volatility is a pervasive risk factor in the cross section of FX excess returns.", "Volatility risk dominates liquidity risk in explaining carry trade returns.",]
lessons_learned = [ "Carry trade returns can be understood as compensation for volatility risk.", "Global FX volatility is a significant factor in pricing risk premia across different asset classes.", "Volatility risk is more critical than liquidity risk in the context of carry trades.", "The forward premium puzzle can be partly explained by volatility risk factors.",]
data_sources = "- Not applicable"
cleaning_steps = "- Not applicable"

[Nie2022]
description = "The paper introduces a novel Transformer-based model, PatchTST, for multivariate time series forecasting and self-supervised representation learning. The model employs a patching mechanism to segment time series into subseries-level patches and a channel-independence approach where each channel shares the same Transformer weights. These innovations reduce computational complexity, enhance long-term forecasting accuracy, and improve representation learning. The model outperforms state-of-the-art Transformer models and demonstrates superior performance in self-supervised pretraining tasks."
empirical_facts = [ "PatchTST achieves a Mean Squared Error (MSE) of 0.349, outperforming other models in the study.", "The patching mechanism reduces the number of input tokens, decreasing computational complexity and memory usage quadratically.", "The model demonstrates improved performance with longer look-back windows, reducing MSE from 0.518 to 0.397 with an increase in window size from 96 to 336.",]
lessons_learned = [ "Patching enhances the locality of time series data, capturing comprehensive semantic information.", "Channel-independence allows for efficient processing of multivariate time series by treating each channel separately.", "Self-supervised learning with PatchTST can outperform supervised training on large datasets, highlighting the model's representation learning capability.",]
data_sources = "- Not applicable"
cleaning_steps = "- Not applicable"

[Nozawa2017]
description = "The paper by Yoshio Nozawa decomposes the variation in credit spreads for corporate bonds into expected returns and expected credit losses using a variance decomposition approach. By employing a log-linearized pricing identity and vector autoregression on data from 1973 to 2011, the study finds that expected returns contribute significantly to the cross-sectional variance of credit spreads, almost as much as expected credit losses. The research highlights that while individual bond credit spreads are influenced by both components, market-wide variations are primarily driven by risk premiums."
empirical_facts = [ "Expected returns contribute nearly as much to the cross-sectional variance of credit spreads as expected credit losses.", "About half of the volatility in credit spreads is due to changing expected excess returns.", "The volatility ratio of implied long-run expected credit loss to credit spread volatility is 0.67, while that of risk premium volatility is 0.52.", "Investment-grade bonds' credit spread variation is largely due to risk premium variation, whereas high-yield bonds are more influenced by expected credit loss.", "Market-wide credit spread variations are mainly driven by risk premiums due to diversification effects.",]
lessons_learned = [ "Variance decomposition can effectively separate the contributions of expected returns and credit losses to credit spread variations.", "Risk premiums play a significant role in the time-series variation of credit spreads at the market level.", "The relationship between risk premiums, expected credit loss, and credit spreads varies nonlinearly with the credit rating of bonds.", "The study's approach does not rely on structural models of default, offering a model-independent perspective on credit spread decomposition.", "The findings align with the distress anomaly, where stocks of firms near default earn lower expected returns.",]
data_sources = [ "Lehman Brothers Fixed Income Database", "Mergent FISD/NAIC Database", "TRACE", "DataStream", "Moody's Default Risk Service",]
cleaning_steps = [ "Ensured completeness of price observations upon default using Moody's Default Risk Service.", "Defined defaults according to Moody's criteria, excluding technical defaults.",]

[Oreshkin2020]
description = "The paper introduces N-BEATS, a deep learning architecture for univariate time series forecasting that outperforms traditional statistical methods. The architecture is notable for its use of backward and forward residual links and a deep stack of fully-connected layers, which allow it to achieve state-of-the-art performance on datasets like M3, M4, and TOURISM. N-BEATS is designed to be interpretable and applicable across various domains without requiring time-series-specific components. The study demonstrates that deep learning primitives alone can effectively solve a wide range of forecasting problems."
empirical_facts = [ "N-BEATS improves forecast accuracy by 11% over a statistical benchmark and by 3% over the M4 competition winner.", "The architecture achieves state-of-the-art performance on M3, M4, and TOURISM datasets.", "N-BEATS does not rely on time-series-specific components, suggesting deep learning primitives are sufficient for diverse forecasting tasks.",]
lessons_learned = [ "Deep learning architectures can outperform traditional statistical methods in time series forecasting.", "Interpretable deep learning models can be designed without significant loss of accuracy.", "Residual blocks and deep fully-connected layers are effective components for time series forecasting models.",]
data_sources = [ "M3 competition dataset", "M4 competition dataset", "TOURISM competition dataset",]
cleaning_steps = "- Not applicable"

[Orphanides2001]
description = "The paper by Athanasios Orphanides critically examines the use of real-time data in formulating monetary policy rules, specifically using Taylor's rule as an example. It highlights the discrepancies between policy recommendations derived from real-time data and those based on ex post revised data, demonstrating that reliance on revised data can lead to misleading policy evaluations. The study finds that forward-looking policy rules, which account for real-time forecasts, provide a more accurate depiction of historical policy actions than traditional Taylor-type rules, particularly during the period from 1987 to 1992."
empirical_facts = [ "Real-time policy recommendations differ significantly from those obtained using ex post revised data.", "Within-year revisions of policy recommendations have a standard deviation exceeding that of the quarterly change in the federal funds rate.", "Forward-looking policy rules describe policy more accurately than Taylor-type rules when using real-time data.", "Data revisions introduce noise and potential bias in estimating policy reaction functions.",]
lessons_learned = [ "Monetary policy rules based on real-time data can differ substantially from those based on revised data.", "Relying on ex post revised data can lead to misleading interpretations of historical policy actions.", "Forward-looking specifications that use real-time forecasts can provide a more accurate depiction of policy.", "Data revisions can obscure the true historical pattern of policy and introduce bias in econometric evaluations.",]
data_sources = [ "Federal Reserve staff forecasts", "Real-time estimates of output, potential output, and inflation",]
cleaning_steps = [ "Constructed a database of current quarter estimates/forecasts based on real-time information.", "Reconstructed policy recommendations using real-time data to compare with those based on revised data.",]

[Palhares2012]
description = "This dissertation investigates the relationship between cash-flow maturity and risk premia in the credit default swap (CDS) market. The main contribution is the identification of a conditional CDS market model that explains the cross-sectional variation in returns by maturity. The study finds that average returns decrease with maturity, and this variation is captured by betas with respect to a portfolio that sells short-maturity CDSs and buys long-maturity CDSs. The findings highlight that short-term CDSs are riskier, especially during turbulent periods, and the dynamics of CDS-market betas are crucial in understanding risk premia."
empirical_facts = [ "Average returns of CDS portfolios decrease with maturity.", "A portfolio strategy that sells short-maturity CDSs and buys long-maturity CDSs (LSM) had an annualized Sharpe ratio of 0.95.", "Short-maturity CDS spreads are more volatile than long-maturity spreads during turbulent periods.", "The LSM portfolio prices the cross-section of CD CDS portfolios sorted on maturity.",]
lessons_learned = [ "Short-term CDSs are riskier and exhibit higher volatility during turbulent periods.", "A conditional CDS market model can effectively price the cross-section of CDS returns by maturity.", "The dynamics of CDS-market betas are critical in understanding the variation in risk premia.", "The LSM portfolio provides insights into the horizon of uncertainty that investors fear.",]
data_sources = [ "Single-name CDSs of BBB-rated firms", "CDX-NAIG index", "ITRAXX-Europe index",]
cleaning_steps = [ "Construction of holding-period returns of constant-duration CDS portfolios.", "Scaling of CDS portfolio returns by a measure of their CDS spread sensitivity.",]

[Prater2024]
description = "The paper evaluates the performance of Long Short-Term Memory (LSTM) neural networks in time-series forecasting using the Libra framework. It highlights that LSTMs, even with parameter tuning, do not outperform the median measures of Libra across various domains such as finance and demographics. The study emphasizes the importance of multi-objective optimization in forecasting and provides a paradigm for applying a wide range of performance measures. The findings suggest that LSTMs' performance is limited by data characteristic variance and hardware constraints."
empirical_facts = [ "Libra framework consists of datasets from four domains: economics, finance, human access, and nature, each with 100 heterogeneous univariate time series.", "Libra datasets have diverse frequency and length, with time series ranging from 20 to 372,864 data points.", "LSTMs do not outperform the median measures of Libra in time-series forecasting.", "Libra provides three evaluation types: one-step-ahead forecast, multi-step-ahead forecast, and rolling-origin forecast.", "The study uses 10 performance measures to evaluate forecasting methods, including sMAPE, MASE, and time-to-result.",]
lessons_learned = [ "Standardized benchmarks like Libra are crucial for evaluating and comparing time-series forecasting methods.", "LSTMs may not always be the best choice for time-series forecasting due to data characteristic variance and hardware constraints.", "Multi-objective optimization is important in forecasting to consider various performance measures.", "The diversity in time-series data characteristics requires careful selection of forecasting methods and parameters.", "Benchmarking against multiple evaluation measures helps prevent overfitting and provides a more generalized performance assessment.",]
data_sources = [ "Libra framework datasets", "Publicly available datasets", "M-Competitions datasets",]
cleaning_steps = "- Not applicable"

[Prokhorenkova]
description = "The paper introduces CatBoost, a new gradient boosting toolkit that addresses prediction shift caused by target leakage in existing gradient boosting algorithms. The main contributions are the development of ordered boosting and a novel algorithm for processing categorical features, which together mitigate target leakage. The authors demonstrate that CatBoost outperforms other state-of-the-art implementations, such as XGBoost and LightGBM, across various datasets. The paper provides a detailed analysis of the target leakage problem and presents empirical evidence supporting the effectiveness of the proposed solutions."
empirical_facts = [ "CatBoost outperforms XGBoost and LightGBM on a diverse set of machine learning tasks.", "Ordered boosting and the new algorithm for categorical features effectively address prediction shift.", "Existing gradient boosting implementations suffer from target leakage, leading to prediction shifts.",]
lessons_learned = [ "Target leakage can cause significant prediction shifts in gradient boosting models.", "Ordered boosting and proper handling of categorical features can mitigate target leakage.", "Using target statistics as numerical features is effective but requires careful handling to avoid leakage.",]
data_sources = "- Not applicable"
cleaning_steps = "- Not applicable"

[Qiu2024]
description = "The paper introduces TFB, a comprehensive and fair benchmarking framework for Time Series Forecasting (TSF) methods. TFB addresses existing shortcomings in dataset coverage, method bias, and evaluation consistency by including datasets from 10 diverse domains and supporting a wide range of forecasting methods and evaluation strategies. The framework enables fair comparisons by providing a flexible and scalable evaluation pipeline. The authors demonstrate TFB's utility by evaluating 21 univariate and 14 multivariate forecasting methods across numerous datasets, offering insights into the performance of different methods under various conditions."
empirical_facts = [ "TFB includes datasets from 10 different domains, such as traffic, electricity, and health, to ensure comprehensive coverage.", "The framework evaluates 21 Univariate Time Series Forecasting (UTSF) methods on 8,068 time series and 14 Multivariate Time Series Forecasting (MTSF) methods on 25 datasets.", "Statistical methods like VAR and Linear Regression outperform some state-of-the-art methods on specific datasets.", "Transformer-based methods excel on datasets with marked seasonality and nonlinear patterns.",]
lessons_learned = [ "Broad domain coverage in datasets is crucial for comprehensive evaluation of forecasting methods.", "Eliminating stereotype bias against traditional methods can reveal competitive performance in certain scenarios.", "Consistent and flexible evaluation pipelines are essential for fair comparisons across different forecasting methods.", "Methods that account for dependencies between channels can significantly enhance performance in multivariate forecasting.",]
data_sources = [ "Traffic datasets", "Electricity datasets", "Health datasets", "Stock market datasets", "Economic datasets",]
cleaning_steps = [ "Datasets were organized according to a taxonomy to ensure diverse characteristics.", "Data was standardized to ensure consistent evaluation across different methods.", "An automated end-to-end pipeline was implemented to streamline data loading and experiment configuration.",]

[Reinhart2010]
description = "The paper by Reinhart and Rogoff investigates the relationship between high public debt levels, economic growth, and inflation using a comprehensive historical dataset spanning 44 countries over 200 years. The authors find that while the link between debt and growth is weak at normal debt levels, countries with public debt exceeding 90% of GDP experience median growth rates about 1% lower, with average growth rates several percent lower. The relationship between public debt and growth is consistent across both advanced and emerging economies, but high debt levels are associated with higher inflation in emerging markets only. The study highlights the severe impact of external debt on growth in emerging markets when it exceeds 60% of GDP."
empirical_facts = [ "Median growth rates for countries with public debt over 90% of GDP are about 1% lower than those with lower debt levels.", "Average growth rates are several percent lower for countries with public debt over 90% of GDP.", "In emerging markets, high public debt levels are associated with higher inflation, unlike in advanced economies.", "External debt exceeding 60% of GDP in emerging markets leads to a 2% decline in annual growth.", "External debt levels in advanced countries average nearly 200% of GDP, particularly high in Europe.",]
lessons_learned = [ "High public debt levels can significantly hinder economic growth, especially when exceeding 90% of GDP.", "The impact of public debt on inflation varies between advanced and emerging economies.", "External debt poses a more severe threat to growth in emerging markets compared to public debt.", "The historical context and nature of debt accumulation (e.g., wartime vs. peacetime) can influence its economic impact.", "Managing high public debt levels is crucial for maintaining economic stability and growth.",]
data_sources = [ "International Monetary Fund, World Economic Outlook", "OECD", "World Bank, Global Development Finance", "Reinhart and Rogoff (2009b)",]
cleaning_steps = [ "Data spans 44 countries over 200 years, requiring harmonization across different political systems and historical contexts.", "Categorization of debt levels into four groups for analysis: below 30%, 30-60%, 60-90%, and above 90% of GDP.", "Adjustment for inflation to analyze real public debt changes since 2007.",]

[Ronn1989]
description = "The paper by Aimee Gerbarg Ronn and Ehud I. Ronn develops and tests arbitrage bounds for a box spread strategy, which involves using four options to create a riskless lending position. The authors derive a no-arbitrage condition for box spreads and test it using data from the Chicago Board Options Exchange. Their findings suggest that the box spread's payoff is independent of the stock price, allowing for conservative testing using bid-ask quotes. The study also explores the economic profitability of box spread lending over multiple trading days."
empirical_facts = [ "The box spread strategy involves a combination of four options to create a riskless lending position.", "The no-arbitrage condition for box spreads is tested using Chicago Board Options Exchange data.", "Empirical tests were conducted using data from January 2, 1981, and extended to seven additional trading days from 1977 to 1984.",]
lessons_learned = [ "Box spreads can be used to create a position equivalent to riskless lending.", "The payoff of a box spread is independent of the terminal stock price, simplifying the testing of arbitrage conditions.", "Using bid-ask quotes instead of transaction prices provides conservative estimates of arbitrage opportunities.",]
data_sources = [ "Berkeley Options Data Base for CBOE prices", "Data Resources, Inc., for the term structure of riskless interest rates",]
cleaning_steps = [ "Hourly intervals of trading days were scanned for occurrences of four bid-ask quotes corresponding to a box spread combination.", "The latest hourly bid-ask quote for each option was collected for analysis.", "Data were sorted by month of expiration and by size of the implied riskless interest rate.",]

[Salinas2020]
description = "The paper introduces DeepAR, a probabilistic forecasting method using autoregressive recurrent neural networks to improve forecast accuracy across multiple related time series. It addresses challenges faced by classical forecasting methods, such as handling diverse magnitudes and skewed distributions in time series data. The authors demonstrate that DeepAR outperforms state-of-the-art methods on real-world datasets by learning global models and leveraging deep learning techniques. This approach minimizes manual intervention and provides accurate probabilistic forecasts, which are crucial for decision-making under uncertainty."
empirical_facts = [ "DeepAR provides more accurate probabilistic forecasts than state-of-the-art methods on several real-world datasets.", "The method is capable of handling time series with widely varying magnitudes and skewed distributions.", "DeepAR can forecast items with little or no historical data by learning from similar items.", "The model incorporates a negative binomial likelihood for count data, addressing non-Gaussian noise in time series.",]
lessons_learned = [ "Deep learning techniques can effectively address challenges in probabilistic forecasting that classical methods struggle with.", "Global models trained on multiple related time series can improve forecast accuracy and reduce manual intervention.", "Probabilistic forecasts enable better decision-making under uncertainty by providing consistent quantile estimates.", "Incorporating appropriate likelihood functions is crucial for improving forecast accuracy in demand forecasting.",]
data_sources = [ "- Not applicable",]
cleaning_steps = [ "- Not applicable",]

[Siriwardane2021]
description = "The paper 'Segmented Arbitrage' by Emil Siriwardane, Adi Sunderam, and Jonathan Wallen explores arbitrage opportunities across various financial markets, including equity, corporate bond, FX, and Treasury markets. The authors estimate arbitrage spreads using no-arbitrage conditions such as put-call parity and covered interest rate parity. The study finds persistent arbitrage spreads in different markets, particularly after the 2008 financial crisis, indicating market inefficiencies. The paper uses data from Bloomberg and other sources to compute these spreads and discusses the implications of these findings for market participants."
empirical_facts = [ "CIP deviations are observed among G10 currencies against the USD, persisting after the 2008-09 financial crisis.", "Equity box arbitrage spreads are computed using put-call parity, with data from SPX options and OIS rates.", "Treasury futures arbitrage spreads are calculated using futures-implied riskless rates and maturity-matched OIS rates.", "Swap spreads have been negative for several floating reference rates and tenors since the 2008 Global Financial Crisis.",]
lessons_learned = [ "Arbitrage opportunities persist in financial markets, even in the presence of no-arbitrage conditions.", "Market inefficiencies can be identified through the analysis of arbitrage spreads across different asset classes.", "The choice of benchmark rates, such as OIS, is crucial in accurately measuring arbitrage spreads.", "The persistence of arbitrage spreads suggests potential barriers to arbitrage, such as transaction costs or market frictions.",]
data_sources = [ "Bloomberg", "Data from van Binsbergen et al. (2019)", "CBOE for SPX options data", "Realized dividends from Bloomberg",]
cleaning_steps = [ "Use of mid-prices for spot and forward exchange rates to estimate CIP deviations.", "Updating series from van Binsbergen et al. (2019) using their methodology for minute-level SPX options data.", "Filtering Treasury futures data to include only observations with positive trading volume.", "Using linearly interpolated OIS swap curves to compute forward rates.",]

[Sullivan1999]
description = "The paper applies White's Reality Check bootstrap methodology to evaluate the performance of technical trading rules while accounting for data-snooping bias. By expanding the universe of trading rules and using 100 years of daily data on the Dow Jones Industrial Average, the study provides a comprehensive test of these rules' performance. The findings reveal that while certain trading rules outperformed the benchmark in the historical sample, this was not the case in the out-of-sample period from 1987-1996, suggesting limited economic value of these rules during that time."
empirical_facts = [ "The study evaluates nearly 8,000 parameterizations of trading rules using 100 years of DJIA data.", "Certain trading rules outperformed the benchmark during the 1897-1986 period, even after adjusting for data-snooping.", "In the out-of-sample period (1987-1996), the best-performing trading rule did not significantly outperform the benchmark.", "No evidence was found that trading rules outperformed the benchmark when applied to S&P 500 index futures from 1984-1996.",]
lessons_learned = [ "Data-snooping can significantly bias the perceived performance of technical trading rules.", "The Reality Check bootstrap methodology is effective in correcting for data-snooping bias.", "Historical success of trading rules may not translate to future periods, highlighting the importance of out-of-sample testing.", "Technical trading rules may not provide economic value when transaction costs and short-sale constraints are considered.",]
data_sources = [ "Daily data on the Dow Jones Industrial Average from 1897 to 1996.", "Price data on the Standard and Poor's 500 index futures from 1984 to 1996.",]
cleaning_steps = [ "The study uses a bootstrap methodology to create numerous samples for statistical significance testing.", "Parameterizations of trading rules are applied to historical data to generate returns for performance evaluation.",]

[Tan2020]
description = "The paper introduces Time Series Extrinsic Regression (TSER), a task that predicts a continuous scalar value from time series data, differing from time series classification and forecasting. The authors benchmark existing algorithms on a novel archive of 19 TSER datasets, finding that the adapted TSC algorithm Rocket achieves the highest accuracy. The study highlights the need for further research to improve machine learning models for TSER tasks."
empirical_facts = [ "The Rocket algorithm, when adapted for regression, outperforms other TSC and ML algorithms like XGBoost and Random Forest in TSER tasks.", "The study uses a novel archive of 19 TSER datasets across various domains.", "Most methods achieve similar accuracies, indicating a need for more specialized TSER algorithms.",]
lessons_learned = [ "Existing TSC algorithms can be adapted for TSER tasks, but there is room for improvement.", "TSER requires algorithms that account for the temporal nature of data, unlike traditional regression methods.", "Further research in TSER has the potential to significantly enhance the accuracy of predictive models.",]
data_sources = [ "- Not applicable",]
cleaning_steps = [ "- Not applicable",]

[VanBinsbergen2022]
description = "This paper estimates risk-free interest rates that are unaffected by convenience yields on safe assets, inferring them from risky asset prices without relying on specific risk models. The authors find that the convenience yield on Treasuries is about 40 basis points, increases for maturities below three months, and quadruples during financial crises. Additionally, both conventional and unconventional monetary stimulus significantly reduce these estimated rates more than corresponding Treasury yields, indicating a broad impact on rates beyond the fixed-income market."
empirical_facts = [ "The estimated convenience yield on Treasuries is approximately 40 basis points.", "Convenience yields are larger for maturities below three months.", "During financial crises, the convenience yield on Treasuries quadruples.", "Monetary stimulus reduces estimated risk-free rates more than Treasury yields.",]
lessons_learned = [ "Risk-free interest rates can be estimated from risky asset prices without specific risk models.", "Convenience yields significantly affect the yields on safe assets like Treasuries.", "Monetary policy has a broad impact on interest rates beyond the fixed-income market.",]
data_sources = "- Not applicable"
cleaning_steps = "- Not applicable"

[Vaswani2017]
description = "The paper introduces the Transformer, a novel neural network architecture for sequence transduction tasks that relies entirely on attention mechanisms, eliminating the need for recurrent or convolutional networks. The Transformer demonstrates superior performance in machine translation tasks, achieving state-of-the-art BLEU scores on the WMT 2014 English-to-German and English-to-French translation tasks. The model's architecture allows for significant parallelization, reducing training time and computational costs. Key innovations include the use of multi-head self-attention and position-wise feed-forward networks."
empirical_facts = [ "The Transformer model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task, surpassing the previous best by over 2 BLEU.", "On the WMT 2014 English-to-French translation task, the Transformer achieves a BLEU score of 41.0, setting a new single-model state-of-the-art.", "The Transformer can be trained in as little as 12 hours on eight P100 GPUs, demonstrating significant improvements in training efficiency.",]
lessons_learned = [ "Attention mechanisms can replace recurrent and convolutional networks in sequence transduction tasks, offering improved parallelization and efficiency.", "Multi-head attention allows the model to attend to information from different representation subspaces, enhancing its ability to capture dependencies.", "The use of scaled dot-product attention helps mitigate issues with large dot product magnitudes, improving gradient stability.",]
data_sources = "- Not applicable"
cleaning_steps = "- Not applicable"

[Wu2021]
description = "The paper introduces Autoformer, a novel architecture for long-term time series forecasting that addresses the limitations of traditional Transformer models. Autoformer incorporates a decomposition architecture and an Auto-Correlation mechanism to improve efficiency and accuracy in modeling complex temporal patterns. The model achieves a 38% relative improvement in accuracy across six benchmarks, spanning applications in energy, traffic, economics, weather, and disease. By leveraging series periodicity, Autoformer enhances information utilization and reduces computational complexity."
empirical_facts = [ "Autoformer achieves a 38% relative improvement in long-term forecasting accuracy over six benchmarks.", "The model covers five practical applications: energy, traffic, economics, weather, and disease.", "Autoformer introduces an Auto-Correlation mechanism that outperforms self-attention in both efficiency and accuracy.", "The complexity of Autoformer's Auto-Correlation mechanism is reduced to O(L log L) for a series of length L.",]
lessons_learned = [ "Decomposition can be effectively integrated into deep models to enhance long-term forecasting capabilities.", "Utilizing series periodicity can improve the discovery of dependencies and aggregation of information.", "Auto-Correlation mechanisms can provide more efficient and accurate alternatives to traditional self-attention mechanisms in Transformers.", "Progressive decomposition allows for better handling of intricate temporal patterns in time series forecasting.",]
data_sources = "- Not applicable"
cleaning_steps = "- Not applicable"

[Xu2025]
description = "The paper introduces FinMultiTime, a large-scale, multimodal financial time-series dataset that aligns four distinct modalities: financial news, structured financial tables, K-line technical charts, and stock price time series across the S&P 500 and HS 300 markets. Covering 5,105 stocks from 2009 to 2025, this dataset provides minute-level, daily, and quarterly resolutions. The study finds that the scale and quality of data significantly enhance prediction accuracy, and multimodal fusion offers moderate gains in Transformer models. The dataset is fully reproducible and can be seamlessly updated to reflect real-time market conditions."
empirical_facts = [ "FinMultiTime covers 5,105 stocks from 2009 to 2025 in the US and China, totaling 112.6 GB.", "The dataset includes four modalities: financial news, structured tables, technical charts, and stock price time series.", "It provides data at minute-level, daily, and quarterly resolutions.", "Experiments show that high-quality, large-scale data reduces prediction error and improves trend-direction accuracy.",]
lessons_learned = [ "Multimodal data integration enhances the richness of financial time-series analysis.", "High-quality sentiment and long-term trend information are critical for accurate predictions.", "A reproducible data pipeline is essential for maintaining up-to-date datasets in dynamic markets.", "Multimodal fusion can yield moderate improvements in model performance, particularly with Transformer models.",]
data_sources = [ "Yahoo Finance API for US stock data", "Tushare API for Chinese stock data", "Nasdaq news scraping strategy for news sentiment data", "SEC Submissions and Company Facts APIs for structured financial tables",]
cleaning_steps = [ "Segmenting daily OHLCV data into semi-annual windows for chart generation.", "Converting raw RGB charts to 8-bit grayscale to reduce input dimensionality.", "Extracting and aligning four distinct data modalities across constituent stocks.", "Curating a panel of accounting variables to represent financial health.",]

[Yang2013]
description = "The paper identifies a 'slope' factor in the cross-section of commodity futures returns, where high-basis commodity futures exhibit higher loadings on this factor compared to low-basis futures. This slope factor, combined with a level factor, explains most of the average excess returns of commodity futures portfolios sorted by basis. The study finds a significant correlation between this slope factor and investment shocks, which are indicative of technological progress in producing new capital. A competitive dynamic equilibrium model of commodity production is proposed to endogenize this correlation, successfully reproducing cross-sectional futures returns and passing various asset pricing tests."
empirical_facts = [ "High-basis commodity futures tend to have higher expected returns than low-basis futures.", "There is a 10% annual return spread between high-basis and low-basis commodity futures portfolios.", "The basis spread in commodity futures is similar to the return spread in high and low forward rate currencies in the foreign exchange market.",]
lessons_learned = [ "The slope factor is a crucial component in explaining the cross-sectional returns of commodity futures.", "Investment shocks, representing technological progress, are significantly correlated with the slope factor in commodity futures.", "Understanding the source of the basis spread is essential due to its large economic magnitude.",]
data_sources = "- Not applicable"
cleaning_steps = "- Not applicable"

[Zeng2022]
description = "The paper questions the effectiveness of Transformer-based models for long-term time series forecasting (LTSF) due to their inherent temporal information loss caused by the permutation-invariant self-attention mechanism. It introduces a simple one-layer linear model, LTSF-Linear, which outperforms existing sophisticated Transformer-based models across nine real-life datasets. The findings suggest that Transformers may not be as effective for LTSF as previously thought and encourage revisiting their use in other time series analysis tasks. The study also explores the impact of various design elements in Transformer-based models on their ability to extract temporal relations."
empirical_facts = [ "LTSF-Linear, a simple one-layer linear model, outperforms Transformer-based LTSF models on nine benchmark datasets.", "The performance margin of LTSF-Linear over Transformer models ranges from 20% to 50%.", "Transformer-based models fail to reduce forecasting errors with increased look-back window sizes, contrary to expectations.", "The study covers diverse datasets including traffic, energy, economics, weather, and disease predictions.",]
lessons_learned = [ "Transformers may not effectively capture temporal relations in time series data due to their permutation-invariant self-attention mechanism.", "Simple linear models can outperform complex Transformer-based models in LTSF tasks.", "The effectiveness of Transformers in time series forecasting should be re-evaluated, especially for tasks like anomaly detection.", "Design elements like positional encoding and input embedding significantly impact the performance of Transformer-based models.",]
data_sources = [ "- Traffic data", "- Energy data", "- Economic data", "- Weather data", "- Disease prediction data",]
cleaning_steps = "- Not applicable"

[Zhao2006]
description = "The paper provides a comprehensive study of feature preprocessing techniques in time series forecasting, emphasizing their impact on forecasting accuracy. The authors evaluate various existing techniques, such as feature selection and extraction, and demonstrate their empirical performance using real-world datasets. The study aims to guide researchers in selecting and integrating effective feature preprocessing methods with forecasting models, ultimately enhancing prediction accuracy. The paper also outlines the general procedure of feature preprocessing and reviews several well-known techniques."
empirical_facts = [ "Feature preprocessing significantly enhances forecasting accuracy in time series models.", "Various techniques, including feature selection and extraction, are evaluated for their empirical performance.", "The study uses real-world datasets to demonstrate the effectiveness of different preprocessing methods.",]
lessons_learned = [ "Effective feature preprocessing is crucial for improving the accuracy of time series forecasting models.", "Feature selection and extraction can help in identifying relevant features and reducing noise in data.", "Different preprocessing techniques should be chosen based on the characteristics of the data and specific requirements of the analysis.",]
data_sources = "- Not applicable"
cleaning_steps = "- Not applicable"

[Zhou2020]
description = "The paper introduces 'Informer', a novel transformer-based model designed to address the challenges of long sequence time-series forecasting (LSTF). Informer improves upon traditional transformers by introducing a ProbSparse self-attention mechanism, which reduces time complexity and memory usage to O(L log L), and a generative style decoder that enhances inference speed. Extensive experiments demonstrate that Informer significantly outperforms existing methods, offering a more efficient solution to LSTF problems."
empirical_facts = [ "Informer achieves O(L log L) time complexity and memory usage with its ProbSparse self-attention mechanism.", "The generative style decoder predicts long time-series sequences in a single forward operation, improving inference speed.", "Informer significantly outperforms existing methods on four large-scale datasets for LSTF tasks.",]
lessons_learned = [ "Efficient self-attention mechanisms can drastically improve the scalability of transformer models for long sequence tasks.", "Generative style decoders can enhance prediction speed by avoiding step-by-step inference.", "Addressing both computational and memory efficiency is crucial for applying transformer models to real-world LSTF problems.",]
data_sources = "- Not applicable"
cleaning_steps = "- Not applicable"
