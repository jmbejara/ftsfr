\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{natbib}  % Required for JPE bibliography style
\usepackage{booktabs} % For better tables
\usepackage{multirow} % For tables with merged cells
\usepackage{subfigure} % For subfigures
\usepackage{hyperref} % For links

\title{An Open-Source Macro-Finance Time Series Forecasting Benchmark}
\author{Jeremiah Bejarano\thanks{[Affiliation and contact information]} \and [Coauthor Names]\thanks{[Affiliations]}}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
This paper presents a macro-finance data repository for benchmarking the
performance of global time series forecasting methods. A standardized benchmark
is crucial for comparing the performance of different forecasting methods, so as
to allow apples-to-apples comparisons across different forecasting methods and
to discourage cherry-picking of results. Despite the ubiquity of time series
forecasting methods in macroeconomics and finance, a standardized repository of
data for benchmarking in these domains does not exist. One reason for this is
that many of these datasets are available only through paid subscriptions and
are thus not freely distributable. To address this issue, we instead provide a
set of scripts that automate the download, data cleaning, and assembly of data
sets from the Wharton Research Data Services (WRDS) platform and the Bloomberg
Terminal. Since many academic institutions have access to WRDS or a Bloomberg
Terminal, contingent on having access to the appropriate WRDS or Bloomberg
subscriptions, each researcher will be able to exactly reproduce each of the
datasets provided in this repository. Furthermore, the datasets that we provide
are cleaned and formatted in a way that is common within the corresponding
academic literature. Our aim is to facilitate the process of benchmarking global
times series forecasting methods on data from these domains. In this light, we
demonstrate the utility of our repository by showcasing the performance of
several baseline forecasting methods using a variety of error metrics, providing
baseline performance metrics which researchers can use to compare forecasting
approaches. We hope that this repository will enhance research
that increases the precision of financial and economic forecasts and contribute
to research related to promoting financial stability and supporting a
well-functioning economy.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

% TODO: Write introduction emphasizing the critical need for standardized benchmarks in macro-finance
% - Establish that time series forecasting is ubiquitous and prevalent in macroeconomics and finance domains
% - Highlight the current gap: no existing standardized benchmarks for evaluating forecasting algorithms in macro-finance
% - Explain why apples-to-apples comparisons are impossible without standardized datasets
% - Stress that researchers currently evaluate their own models on arbitrarily chosen datasets, making comparisons meaningless
% - Position this work as filling a critical gap in quantitative finance research
% - Emphasize that this complements (not substitutes) the Monash benchmark
% - Make clear that the lack of standardization is particularly problematic in finance where data often requires subscriptions

[Introduction content to be filled in...]

\section{Literature Review and Related Work}
\label{sec:literature}

% TODO: Position the paper relative to existing benchmarks and establish the canonical nature of chosen cleaning methods
% - Review the Monash Time Series Forecasting Archive and explain how FTSFR follows their established template
% - Introduce He, Kelly, Manela (2017) as the anchor paper for asset class selection and cleaning methods
% - Introduce Siriwardane, Sunderam, & Wallen (2022) "Segmented Arbitrage" as the anchor for arbitrage spread construction
% - Emphasize that the choice to follow these papers is deliberate - not doing anything novel is a FEATURE, not a bug
% - Acknowledge that some dataset choices are somewhat ad hoc but are anchored by these survey-style papers

\subsection{Time Series Forecasting Benchmarks}
% Review of existing benchmarking archives

\subsection{Asset Pricing Literature}
% He, Kelly, Manela (2017) and their approach to asset class selection

\subsection{Market Segmentation and Arbitrage}
% Siriwardane et al. (2022) and arbitrage spread construction

[Literature review content to be filled in...]

\section{Data Repository Overview}
\label{sec:repository_overview}

% TODO: Provide high-level description of the repository structure and contents
% - Describe the automated data pulling framework
% - List the major categories of data
% - Explain the dual purpose: both for forecasting benchmarks AND for replicating important finance papers
% - Note that replication code for many of these papers was not previously publicly available

\subsection{Repository Architecture}
% Technical structure and automation framework

\subsection{Data Categories}
% Major categories of financial and economic data

\subsection{Data Sources and Access Requirements}
% WRDS, Bloomberg, and other data sources

[Repository overview content to be filled in...]

\section{Asset Class Datasets}
\label{sec:asset_classes}

% TODO: Document each asset class dataset with its canonical cleaning method
% For EACH asset class, create a subsection that includes:
% - Which specific paper's cleaning method is being replicated
% - Why this dataset/asset class is important for macro-finance
% - Key cleaning decisions and their justifications
% - Summary statistics that match the original paper (proving correct replication)

Following \cite{He2017}, we include comprehensive datasets across multiple asset classes, each cleaned according to canonical methods from the academic literature.

\subsection{Equity Markets}
\label{sec:equity}
% Fama-French 25 portfolios and CRSP universe
% Explain exclusion of small stocks, ADRs, etc. following Fama-French (1993)

\subsection{US Treasuries}
\label{sec:treasuries}
% CRSP maturity-sorted portfolios Treasuries
The US Treasury bond returns data comes from the \textbf{CRSP Treasury Database} accessed through WRDS, following the methodology of Gurkaynak, Sack, and Wright (2007). The raw CRSP Treasury data is obtained from WRDS and then subjected to the filtering criteria and return calculation methodology specified in Gurkaynak, Sack, and Wright (2007), which provides a comprehensive framework for constructing Treasury yield curves and return series. The data includes daily time series containing bid/ask prices, accrued interest, yields, and returns for Treasury securities, along with issue information containing security characteristics like CUSIP, maturity dates, coupon rates, and issue types. The coverage spans noncallable bonds and notes from 1970-2023, with key fields including Treasury record identifiers, CRSP IDs, CUSIPs, prices, yields, returns, and outstanding amounts.

The data standardization process involves several key steps following the established methodology. First, daily quotes are merged with issue information to create a comprehensive dataset with security characteristics and trading data. Return processing utilizes unadjusted returns representing simple price changes plus accrued interest, with daily returns converted to monthly returns using compound return calculation. For maturity grouping, days to maturity are converted to years, and ten maturity groups are created using 0.5-year intervals from 0 to 5 years, with each bond assigned to a maturity group based on remaining time to maturity. Portfolio construction groups bonds by date and maturity group, calculates mean returns for each maturity group, and creates a time series of portfolio returns for each maturity group. Data quality measures include removing observations with missing returns or maturity information, filtering for bonds and notes only while excluding bills and other security types, and employing month-end dates for portfolio aggregation. The final output is a dataset with monthly Treasury bond portfolio returns across different maturity groups, suitable for fixed income analysis and comparison with other bond market data. The constructed Treasury bond returns are validated against the He, Kelly, and Manela (2017) dataset, which provides a benchmark for Treasury bond returns across different maturity groups, demonstrating strong alignment in the time series patterns and return magnitudes across maturity groups.
% The US Treasury bond returns data comes from the \textbf{CRSP Treasury Database} accessed through WRDS, following the methodology of Gurkaynak, Sack, and Wright (2007). The data includes:

% \begin{itemize}
%     \item \textbf{Daily time series}: Contains daily bid/ask prices, accrued interest, yields, and returns for Treasury securities
%     \item \textbf{Issue information}: Contains security characteristics like CUSIP, maturity dates, coupon rates, and issue types  
%     \item \textbf{Coverage}: Noncallable bonds and notes from 1970-2023
%     \item \textbf{Key fields}: Treasury record identifiers, CRSP IDs, CUSIPs, prices, yields, returns, and outstanding amounts
% \end{itemize}

% \paragraph{Data Standardization:}

% The raw CRSP Treasury data is obtained from WRDS and then subjected to the filtering criteria and return calculation methodology specified in Gurkaynak, Sack, and Wright (2007), which provides a comprehensive framework for constructing Treasury yield curves and return series.

% \begin{enumerate}
%     \item \textbf{Data Consolidation}: Daily quotes are merged with issue information to create a comprehensive dataset with security characteristics and trading data

%     \item \textbf{Return Processing}: 
%     \begin{itemize}
%         \item Unadjusted returns represent simple price changes plus accrued interest
%         \item Daily returns are converted to monthly returns using compound return calculation
%     \end{itemize}

%     \item \textbf{Maturity Grouping}:
%     \begin{itemize}
%         \item Days to maturity are converted to years
%         \item Ten maturity groups are created using 0.5-year intervals from 0 to 5 years
%         \item Each bond is assigned to a maturity group based on remaining time to maturity
%     \end{itemize}

%     \item \textbf{Portfolio Construction}:
%     \begin{itemize}
%         \item Bonds are grouped by date and maturity group
%         \item Mean returns are calculated for each maturity group
%         \item A time series of portfolio returns is created for each maturity group
%     \end{itemize}

%     \item \textbf{Data Quality}:
%     \begin{itemize}
%         \item Observations with missing returns or maturity information are removed
%         \item The sample is filtered for bonds and notes only (bills and other security types are excluded)
%         \item Month-end dates are employed for portfolio aggregation
%     \end{itemize}
% \end{enumerate}

% The final output is a dataset with monthly Treasury bond portfolio returns across different maturity groups, suitable for fixed income analysis and comparison with other bond market data.
% The constructed Treasury bond returns are validated against the He, Kelly, and Manela (2017) dataset, which provides a benchmark for Treasury bond returns across different maturity groups. Figure \textbf{PLACEHOLDER} shows the correlation between the two datasets, demonstrating strong alignment in the time series patterns and return magnitudes across maturity groups.

\subsection{Corporate Bonds}
\label{sec:corporate_bonds}
% Nozawa (2017) yield-spread sorted portfolios
The corporate bond returns data comes from the \textbf{TRACE (Trade Reporting and Compliance Engine)} dataset available at openbondassetpricing.com, following the methodology of Nozawa (2017). The data includes:

\begin{itemize}
    \item \textbf{Market microstructure adjustments}: Corrections for market microstructure noise (MMN) to enhance bond price and return reliability
    \item \textbf{Data filters}: Stringent quality filters including U.S.-domiciled firms only, exclusion of private placements and convertible bonds
    \item \textbf{Coverage}: Corporate bonds with sufficient outstanding amounts and complete information
    \item \textbf{Key fields}: MMN-adjusted clean prices, amount outstanding, monthly returns, and credit spreads
\end{itemize}

\paragraph{Data Standardization:}

The cleaning procedure follows the rigorous framework established by Nozawa (2017) and adopted by He, Kelly, and Manela (HKM).

\begin{enumerate}
    \item \textbf{Bond Selection Criteria}:
    \begin{itemize}
        \item Bonds with floating rate coupons are excluded
        \item Non-callable option features (puts, convertibles) are excluded, while callable bonds are retained
        \item Observations where corporate bond price exceeds matched Treasury price are removed
        \item Bonds with price less than \$0.01 per \$1 face value are dropped
    \end{itemize}

    \item \textbf{Return Processing}:
    \begin{itemize}
        \item Return reversals are eliminated if the product of adjacent returns is less than -0.04
        \item Monthly returns are computed to avoid reinvestment assumptions
    \end{itemize}

    \item \textbf{Synthetic Treasury Construction}:
    \begin{itemize}
        \item Synthetic Treasury bonds with identical cash flow structure are constructed for each corporate bond
        \item Treasury prices are based on Federal Reserve constant-maturity yield data
        \item Excess returns and credit spreads are computed in price terms rather than yield spreads
    \end{itemize}

    \item \textbf{Portfolio Construction}:
    \begin{itemize}
        \item Bonds are sorted into deciles based on credit spread (CS) within each date
        \item Value-weighted portfolio returns are calculated using bond values as weights
        \item Bond value is computed as the product of MMN-adjusted clean price and amount outstanding
    \end{itemize}

    \item \textbf{Data Quality}:
    \begin{itemize}
        \item Defaults are verified using Moody's Default Risk Service
        \item CRSP and Compustat data supplement equity and accounting information
        \item Callable bonds are accounted for using fixed effects in regression models
    \end{itemize}
\end{enumerate}

The final output is a dataset with monthly corporate bond portfolio returns across credit spread deciles, suitable for credit risk analysis and comparison with other bond market data.
The constructed corporate bond returns are validated against the He, Kelly, and Manela (2017) dataset, which provides a benchmark for corporate bond returns across different credit spread deciles. Figure \textbf{PLACEHOLDER} shows the time series comparison between the two datasets, demonstrating strong alignment in return patterns across deciles, particularly during volatile periods such as the 2008 financial crisis.

\subsection{Sovereign Bonds}
\label{sec:sovereign}
% Borri and Verdelhan (2012) portfolios

\subsection{Options}
\label{sec:options}
% Constantinides, Jackwerth and Savov (2013) S&P 500 portfolios

\subsection{Foreign Exchange}
\label{sec:fx}
% Lettau et al. (2014) and Menkhoff et al. (2012)

\subsection{Commodities}
\label{sec:commodities}
% Yang (2013) methodology

\subsection{Credit Default Swaps}
\label{sec:cds}
% Markit data following Palhares (2013)

[Asset class content to be filled in...]

\section{Arbitrage Spread Datasets}
\label{sec:arbitrage}

% TODO: Document each arbitrage spread construction
% For EACH arbitrage spread:
% - Explain the economic intuition behind the trade
% - Detail the exact construction methodology
% - Show replication of statistics from Siriwardane et al.
% - Discuss what violations of these spreads tell us about market segmentation

Following \cite{Siriwardane2021}, we construct various arbitrage spreads that measure market segmentation.

\subsection{Covered Interest Parity (CIP)}
% Construction using spot, forward FX and interest rates
     

During periods of market stress, such as the 2008 financial crisis and the 2020 COVID-19
pandemic, covered interest parity (CIP) may no longer hold as the forward–spot differential 
no longer exactly offsets interest‐rate differentials. Factors contributing to this phenomenon include:
Heightened Counterparty‐Credit Risk, Liquidity Constraints, or Regulatory Pressures.

In other periods, deviations from CIP typically stem from market inefficiencies and are 
small in magnitude and are short-lived in timeframe due to arbitrage activities.

Our analysis examines CIP deviations across eight G10 currencies against the USD, 
using data from 1999 onwards sourced through Bloomberg Terminal.

\paragraph{The dataset includes:}
\begin{itemize}
    \item Spot exchange rates for each currency pair
    \item 3-month forward points for each currency pair
    \item 3-month Overnight Index Swap (OIS) rates for each currency
\end{itemize}

\paragraph{Data Standardization:}
\begin{itemize}
    \item Forward points are scaled appropriately (per 10,000 for most currencies, per 100 for JPY)
    \item Currencies conventionally quoted as USD-per-foreign-currency (EUR, GBP, AUD, NZD) are converted to reciprocal rates for consistency
    \item OIS rates serve as our risk-free benchmark to align with other arbitrage spread studies
\end{itemize}

We successfully replicate the CIP spreads as calculated by Siriwardane et al. (2023),
as seen in this side by side comparison of our calculated currency respective
arbitrage spreads versus those reported in their paper.

\begin{figure}[htbp]
  \centering
  \begin{minipage}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{logo.png}
    \\[1ex] % small vertical gap
    {\small (a) Findings in Siriwardane et al. (2023)}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{logo.png}
    \\[1ex]
    {\small (b) Our findings}
  \end{minipage}
  \caption{IMAGES ARE PLACE HOLDERS}
  \label{fig:comparison-minipage}
\end{figure}

During the overlapping periods of both datasets, the CIP spreads are nearly identical.
Our findings confirm the presence of CIP deviations, particularly during periods of
market stress, consistent with the conclusions drawn by Siriwardane et al. (2023).


\subsection{Box Spread Arbitrage}
% Options-based arbitrage using put-call parity

\subsection{Equity Spot-Futures Basis}
% Cash vs futures arbitrage

\subsection{Treasury Spot-Futures Basis}
% Government bond basis trades

\subsection{Treasury-Swap Spread}
% Treasury vs interest rate swap arbitrage

\subsection{TIPS-Treasury Spread}
% Inflation-linked vs nominal bond spreads

\subsection{CDS-Bond Basis}
% Credit default swap vs cash bond arbitrage

Credit Default Swap-bond arbitrage stems from the assumption that the credit risk of a company should 
be reflected in both its corporate bond spread and its CDS spread. 
Theoretically, these two measures should be equal, leading to a basisof zero. 
However, in reality, the basis often deviates from zero due to market inefficiencies, 
liquidity frictions, and various risks. When this happens, an arbitrage opportunity arises.


The authors define the CDS basis (CB) as

\begin{equation}
  CB_{i,t,\tau} \;=\; CDS_{i,t,\tau} \;-\; FR_{i,t,\tau},
\end{equation}

where:
\begin{itemize}
  \item $FR_{i,t,\tau}$ = time $t$ floating‐rate spread implied by a fixed‐rate corporate bond issued by firm $i$ at tenor $\tau$,
  \item $CDS_{i,t,\tau}$ = time $t$ Credit Default Swap (CDS) par spread for firm $i$ with tenor $\tau$.
\end{itemize}

A negative basis implies an investor could earn a positive arbitrage profit by going long the bond and purchasing CDS protection. The investor would pay a lower par spread than the coupon of the bond itself and then receive value from the default.

The value of $FR$ is substituted by the paper with Z‐spread which we also modify in our construction. We will go into the substitution in detail later.

The value of $CDS$ is interpolated by the authors using a cubic spline function.

Given the CDS spread from above, traditional construction of a risk‐free rate for implied arbitrage implies the following return:

\begin{equation}
  rfr^{CDS}_{i,t,\tau} \;=\; y_{t,\tau} \;-\; CB_{i,t,\tau},
\end{equation}

where:
\begin{itemize}
  \item $y_{t,\tau}$ = maturity‐matched Treasury yield at time $t$.
\end{itemize}

The risk‐free rate then can be seen as the Treasury yield in addition to the basis received when executing the CDS basis trade (investor benefits from negative basis).



\begin{figure}[htbp]
  \centering
  \begin{minipage}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{logo.png}
    \\[1ex] % small vertical gap
    {\small (a) Findings in Siriwardane et al. (2023)}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{logo.png}
    \\[1ex]
    {\small (b) Our findings}
  \end{minipage}
  \caption{IMAGES ARE PLACE HOLDERS}
  \label{fig:comparison-minipage}
\end{figure}




[Arbitrage spread content to be filled in...]

\section{Additional Macro-Finance Datasets}
\label{sec:additional_datasets}

% TODO: Document datasets for macroeconomic forecasting and financial stability monitoring

\subsection{Bank Call Report Data}
% NYU archive (Schnabl)

\subsection{Treasury Yield Curve}
% Fed data following Gürkaynak et al.

\subsection{Macroeconomic Indicators}
% FRED-MD and other macro series

\subsection{Bank-Specific Metrics}
% Using WRDS Bank Premium

[Additional datasets content to be filled in...]

\section{Replication Results and Validation}
\label{sec:replication}

% TODO: Demonstrate successful replication of all source papers
% - Create comprehensive replication tables
% - Include specific sections for key papers
% - Document any data quality issues discovered
% - Explain any necessary deviations from original methodologies

\subsection{He, Kelly, Manela (2017) Replication}
% Factor loadings and test portfolio returns

\subsection{Siriwardane et al. (2022) Replication}
% Arbitrage spread statistics

\subsection{Individual Dataset Replications}
% Nozawa, Borri & Verdelhan, etc.

[Replication results to be filled in...]

\section{Baseline Forecasting Methodology}
\label{sec:methodology}

% TODO: Define forecasting approach following Monash template
% - Justify choice of error metrics
% - Select baseline forecasting models
% - Define evaluation methodology

\subsection{Error Metrics}
% MASE, sMAPE, RMSE, etc.

\subsection{Baseline Models}
% Traditional, ML, and Deep Learning models
TODO: Arsh: Add bullet points provide a short 2-3 sentence description of each model. What is the basic idea behind each model? Who developed it? What package are we using to implement it? 

\begin{itemize}
    \item ARIMA: Standard Autoregressive Integrated Moving Average model based on the statsmodel implementation.\cite{Box2013}
    \begin{itemize}
        \item \textbf{Forecasting Type: }Local
        \item \textbf{Package Used: }\texttt{\href{https://unit8co.github.io/darts/generated_api/darts.models.forecasting.arima.html}{darts}}
    \end{itemize}
    \item ETS: We use the Holt-Winter's exponential smoothing model\cite{Winters1960}. % Please add this citation from the URL: https://www.sciencedirect.com/science/article/abs/pii/S0169207003001134
    \begin{itemize}
        \item \textbf{Forecasting Type: }Local
        \item \textbf{Package Used: }\texttt{\href{https://unit8co.github.io/darts/generated_api/darts.models.forecasting.exponential_smoothing.html}{darts}}
    \end{itemize}
    \item Simple Exponential Smoothing: simple exponential moving average on a time-series\cite{Brown2004}.
    \begin{itemize}
        \item \textbf{Forecasting Type: }Local
        \item \textbf{Package Used: }\texttt{\href{https://unit8co.github.io/darts/generated_api/darts.models.forecasting.exponential_smoothing.html}{darts}}
    \end{itemize}
    \item TBATS: Uses Box-Cox transformations, ARMA error corrections and Fourier representations\cite{DeLivera2011}.
    \begin{itemize}
        \item \textbf{Forecasting Type: }Local
        \item \textbf{Package Used: }\texttt{\href{https://unit8co.github.io/darts/generated_api/darts.models.forecasting.sf_tbats.html}{darts}}
    \end{itemize}
    \item Theta: Splits a time-series into multiple Theta lines which are extrpolated separately and their combination is taken as the forecast\cite{Assimakopoulos2000}.
    \begin{itemize}
        \item \textbf{Forecasting Type: }Local
        \item \textbf{Package Used: }\texttt{\href{https://unit8co.github.io/darts/generated_api/darts.models.forecasting.theta.html\#darts.models.forecasting.theta.Theta}{darts}}
    \end{itemize}
    \item Prophet: decomposable time-series model similar to Generalized Additive Model(GAM) developed by Facebook\cite{Taylor2018}.
    \begin{itemize}
        \item \textbf{Forecasting Type: }Local
        \item \textbf{Package Used: }\texttt{\href{https://unit8co.github.io/darts/generated_api/darts.models.forecasting.prophet_model.html}{darts}}
    \end{itemize}
    \item PR (Pooled Regression): Generic gaussian linear model\cite{Trapero2015}
    \begin{itemize}
        \item \textbf{Forecasting Type: }Global
        \item \textbf{Package Used: }\texttt{\href{https://unit8co.github.io/darts/generated_api/darts.models.forecasting.sklearn_model.html\#darts.models.forecasting.sklearn_model.SKLearnModel}{darts}} and \texttt{\href{https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.TweedieRegressor.html\#sklearn.linear_model.TweedieRegressor}{scikit-learn}}
    \end{itemize}
    \item CatBoost: Gradient boosting algorithm for dealing with categorical data and a leaf value calculation trick which reduces overfitting\cite{Prokhorenkova}.
    \begin{itemize}
        \item \textbf{Forecasting Type: }Global
        \item \textbf{Package Used: }\texttt{\href{https://unit8co.github.io/darts/generated_api/darts.models.forecasting.catboost_model.html}{darts}}
    \end{itemize}
    \item FFNN (Feed-Forward Neural Network): Simple neural network which is a collection of weight tensors with activation functions in between\cite{Goodfellow2016}.
    \begin{itemize}
        \item \textbf{Forecasting Type: }Global
        \item \textbf{Package Used: }\texttt{\href{https://ts.gluon.ai/stable/api/gluonts/gluonts.torch.model.simple_feedforward.html}{gluonts}} with \texttt{torch} backend
    \end{itemize}
    \item Transformer: A deep network architecture utilising attention mechanisms for deducing dependencies in the input and the output\cite{Vaswani2017}.
    \begin{itemize}
        \item \textbf{Forecasting Type: }Global
        \item \textbf{Package Used: }\texttt{\href{https://unit8co.github.io/darts/generated_api/darts.models.forecasting.transformer_model.html}{darts}}
    \end{itemize}
    \item DeepAR: An Autoregressive Recurrent Neural Network(RNN) with Long Short-Term Memory(LSTM).\cite{Salinas2020}
    \begin{itemize}
        \item \textbf{Forecasting Type: }Global
        \item \textbf{Package Used: }\texttt{\href{https://ts.gluon.ai/stable/api/gluonts/gluonts.torch.model.deepar.module.html}{gluonts}} with \texttt{torch} backend
    \end{itemize}
    \item WaveNet: Dilated deep neural network employing convolution layers. Originally made to generate sound waveforms\cite{Oord2016} but adapted to use in forecasting. % PLEASE ADD CITATION FROM URL: https://arxiv.org/abs/1703.04691
    \begin{itemize}
        \item \textbf{Forecasting Type: }Global
        \item \textbf{Package Used: }\texttt{\href{https://ts.gluon.ai/stable/api/gluonts/gluonts.torch.model.wavenet.html}{gluonts}} with \texttt{torch} backend
    \end{itemize}
    \item D-Linear: Transformer-based deep model which is a combination of a decomposition scheme with linear layers\cite{Zeng2022}.
    \begin{itemize}
        \item \textbf{Forecasting Type: }Global
        \item \textbf{Package Used: }\texttt{\href{https://unit8co.github.io/darts/generated_api/darts.models.forecasting.dlinear.html}{darts}}
    \end{itemize}
    \item N-Linear: Transformer-based deep model which addresses distribution shifts in the dataset by using a normalization trick\cite{Zeng2022}.
    \begin{itemize}
        \item \textbf{Forecasting Type: }Global
        \item \textbf{Package Used: }\texttt{\href{https://unit8co.github.io/darts/generated_api/darts.models.forecasting.nlinear.html}{darts}}
    \end{itemize}
    \item N-BEATS: Deep neural network with fully-connected layers along with residual links\cite{Oreshkin2020}.
    \begin{itemize}
        \item \textbf{Forecasting Type: }Global
        \item \textbf{Package Used: }\texttt{\href{https://unit8co.github.io/darts/generated_api/darts.models.forecasting.nbeats.html}{darts}}
    \end{itemize}
    \item N-HiTS: Enhances N-BEATS by using multi-rate data sampling and multi-scale interpolation\cite{Challu2022}.
    \begin{itemize}
        \item \textbf{Forecasting Type: }Global
        \item \textbf{Package Used: }\texttt{\href{https://unit8co.github.io/darts/generated_api/darts.models.forecasting.nhits.html\#darts.models.forecasting.nhits.NHiTSModel}{darts}}
    \end{itemize}
    \item Autoformer: Transformer-based deep network employing decomposition and Auto-Correlation\cite{Wu2021}.
    \begin{itemize}
        \item \textbf{Forecasting Type: }Global
        \item \textbf{Package Used: }Nixtla's \texttt{\href{https://nixtlaverse.nixtla.io/neuralforecast/models.autoformer.html}{neuralforecast}}
    \end{itemize}
    \item Informer: Transformer-based architecture using a ProbSparse self-attention mechanism and a generative-style decoder among other techniques\cite{Zhou2020}.
    \begin{itemize}
        \item \textbf{Forecasting Type: }Global
        \item \textbf{Package Used: }Nixtla's \texttt{\href{https://nixtlaverse.nixtla.io/neuralforecast/models.informer.html}{neuralforecast}}
    \end{itemize}
    \item PatchTST: Transformer-based model breaking a time series into a collection of patches used as tokens and utilises channel independence\cite{Nie2022}.
    \begin{itemize}
        \item \textbf{Forecasting Type: }Global
        \item \textbf{Package Used: }\texttt{\href{https://ts.gluon.ai/stable/api/gluonts/gluonts.torch.model.patch_tst.html}{gluonts}} with \texttt{torch} backend
    \end{itemize}
    \item Temporal Fusion Transformer: Transformer-based model employing recurrent layers and self-attention layers with a focus on interpretablility\cite{Lim2021}
    \begin{itemize}
        \item \textbf{Forecasting Type: }Global
        \item \textbf{Package Used: }\texttt{\href{https://ts.gluon.ai/stable/api/gluonts/gluonts.torch.model.tft.module.html}{gluonts}} with \texttt{torch} backend
    \end{itemize}
    \item Time-series Dense Encoder(TiDE): Multi-layer perceptron based architecture utilising encoder and decoder blocks\cite{Das2023a}.
    \begin{itemize}
        \item \textbf{Forecasting Type: }Global
        \item \textbf{Package Used: }\texttt{\href{https://unit8co.github.io/darts/generated_api/darts.models.forecasting.tide_model.html}{darts}}
    \end{itemize}
    \item TimesFM: Pre-trained transformer-based decoder-only deep model utilising patches similar to PatchTST\cite{Das2023}.
    \begin{itemize}
        \item \textbf{Forecasting Type: }Global
        \item \textbf{Package Used: }\texttt{\href{https://github.com/google-research/timesfm}{timesfm}} python package
    \end{itemize}
\end{itemize}

\subsection{Evaluation Framework}
% Train/test splits, structural breaks, etc.

[Methodology content to be filled in...]

\section{Baseline Results and Analysis}
\label{sec:results}

% TODO: Present comprehensive forecasting results across all datasets
% - Create results tables in exact same format as Monash paper
% - Analyze patterns in results
% - Compare with Monash findings where applicable
% - Discuss implications for practitioners

\subsection{Overall Performance}
% Summary tables across all datasets

\subsection{Results by Asset Class}
% Detailed analysis for each asset class

\subsection{Results by Model Type}
% Comparison of traditional vs ML vs DL approaches

\subsection{Comparison with Monash Benchmarks}
% How financial data differs from general time series

[Results content to be filled in...]

\section{Implementation and Usage}
\label{sec:implementation}

% TODO: Explain technical architecture and usage
% - Document the automation framework
% - Provide clear usage instructions
% - Explain design decisions
% - Include code examples

\subsection{Installation and Setup}
% Requirements and installation process

\subsection{Data Pipeline Architecture}
% dodo.py and automation framework

\subsection{Adding New Datasets}
% Extensibility and contribution guidelines

[Implementation content to be filled in...]

\section{Reproducibility}
\label{sec:reproducibility}

% TODO: Ensure full reproducibility of all results
% - Document exact data vintage used
% - Provide checksums for all processed datasets
% - Include random seeds
% - Detail computational environment

[Reproducibility content to be filled in...]

\section{Conclusions and Future Work}
\label{sec:conclusion}

% TODO: Summarize contributions and outline future extensions
% - Recap main contributions
% - Discuss potential additional datasets
% - Propose enhanced forecasting methods
% - Suggest infrastructure improvements
% - Note community contribution guidelines

[Conclusion content to be filled in...]

\section*{Acknowledgments}

We would like to thank the following individuals. With their permission, we have adapted and used pieces of their code in this repository: Om Mehta and Kunj Shah for their replication of the Covered Interest Rate Parity (CIP) arbitrage spreads; Kyle Parran and Duncan Park for their replication of commodity futures returns.

% TODO: Add additional acknowledgments

\appendix

\section{Data Descriptions}
\label{app:data_descriptions}
% Detailed descriptions of each dataset

\section{Additional Results}
\label{app:additional_results}
% Extended results tables and robustness checks

\bibliographystyle{jpe}
\bibliography{bibliography}

\end{document}