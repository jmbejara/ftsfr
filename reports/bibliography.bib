@article{Aksu2024,
abstract = {Time series foundation models excel in zero-shot forecasting, handling diverse tasks without explicit training. However, the advancement of these models has been hindered by the lack of comprehensive benchmarks. To address this gap, we introduce the General Time Series Forecasting Model Evaluation, GIFT-Eval, a pioneering benchmark aimed at promoting evaluation across diverse datasets. GIFT-Eval encompasses 23 datasets over 144,000 time series and 177 million data points, spanning seven domains, 10 frequencies, multivariate inputs, and prediction lengths ranging from short to long-term forecasts. To facilitate the effective pretraining and evaluation of foundation models, we also provide a non-leaking pretraining dataset containing approximately 230 billion data points. Additionally, we provide a comprehensive analysis of 17 baselines, which includes statistical models, deep learning models, and foundation models. We discuss each model in the context of various benchmark characteristics and offer a qualitative analysis that spans both deep learning and foundation models. We believe the insights from this analysis, along with access to this new standard zero-shot time series forecasting benchmark, will guide future developments in time series foundation models. Code, data, and the leaderboard can be found at https://github.com/SalesforceAIResearch/gift-eval .},
archivePrefix = {arXiv},
arxivId = {2410.10393},
author = {Aksu, Taha and Woo, Gerald and Liu, Juncheng and Liu, Xu and Liu, Chenghao and Savarese, Silvio and Xiong, Caiming and Sahoo, Doyen},
eprint = {2410.10393},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Aksu et al. - 2024 - GIFT-Eval A Benchmark For General Time Series Forecasting Model Evaluation.pdf:pdf},
month = {oct},
title = {{GIFT-Eval: A Benchmark For General Time Series Forecasting Model Evaluation}},
url = {https://arxiv.org/pdf/2410.10393},
year = {2024}
}
@article{Assimakopoulos2000,
abstract = {This paper presents a new univariate forecasting method. The method is based on the concept of modifying the local curvature of the time-series through a coefficient 'Theta' (the Greek letter $\theta$), that is applied directly to the second differences of the data. The resulting series that are created maintain the mean and the slope of the original data but not their curvatures. These new time series are named Theta-lines. Their primary qualitative characteristic is the improvement of the approximation of the long-term behavior of the data or the augmentation of the short-term features, depending on the value of the Theta coefficient. The proposed method decomposes the original time series into two or more different Theta-lines. These are extrapolated separately and the subsequent forecasts are combined. The simple combination of two Theta-lines, the Theta = 0 (straight line) and Theta = 2 (double local curves) was adopted in order to produce forecasts for the 3003 series of the M3 competition. The method performed well, particularly for monthly series and for microeconomic data. {\textcopyright} 2000 International Institute of Forecasters. Published by Elsevier Science B.V. All rights reserved.},
author = {Assimakopoulos, V. and Nikolopoulos, K.},
doi = {10.1016/S0169-2070(00)00066-2},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Assimakopoulos, Nikolopoulos - 2000 - The theta model a decomposition approach to forecasting.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {M3-competition,Time series,Univariate forecasting method},
month = {oct},
number = {4},
pages = {521--530},
publisher = {Elsevier},
title = {{The theta model: a decomposition approach to forecasting}},
url = {https://www.sciencedirect.com/science/article/abs/pii/S0169207000000662?via%3Dihub https://linkinghub.elsevier.com/retrieve/pii/S0169207000000662},
volume = {16},
year = {2000}
}
@article{Bagnall2018,
abstract = {In 2002, the UCR time series classification archive was first released with sixteen datasets. It gradually expanded, until 2015 when it increased in size from 45 datasets to 85 datasets. In October 2018 more datasets were added, bringing the total to 128. The new archive contains a wide range of problems, including variable length series, but it still only contains univariate time series classification problems. One of the motivations for introducing the archive was to encourage researchers to perform a more rigorous evaluation of newly proposed time series classification (TSC) algorithms. It has worked: most recent research into TSC uses all 85 datasets to evaluate algorithmic advances. Research into multivariate time series classification, where more than one series are associated with each class label, is in a position where univariate TSC research was a decade ago. Algorithms are evaluated using very few datasets and claims of improvement are not based on statistical comparisons. We aim to address this problem by forming the first iteration of the MTSC archive, to be hosted at the website www.timeseriesclassification.com. Like the univariate archive, this formulation was a collaborative effort between researchers at the University of East Anglia (UEA) and the University of California, Riverside (UCR). The 2018 vintage consists of 30 datasets with a wide range of cases, dimensions and series lengths. For this first iteration of the archive we format all data to be of equal length, include no series with missing data and provide train/test splits.},
archivePrefix = {arXiv},
arxivId = {1811.00075},
author = {Bagnall, Anthony and Dau, Hoang Anh and Lines, Jason and Flynn, Michael and Large, James and Bostrom, Aaron and Southam, Paul and Keogh, Eamonn},
eprint = {1811.00075},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Bagnall et al. - 2018 - The UEA multivariate time series classification archive, 2018.pdf:pdf},
month = {oct},
title = {{The UEA multivariate time series classification archive, 2018}},
url = {https://arxiv.org/pdf/1811.00075},
year = {2018}
}
@article{Barndorff-Nielsen2009,
abstract = {Realized kernels use high-frequency data to estimate daily volatility of individual stock prices. They can be applied to either trade or quote data. Here we provide the details of how we suggest implementing them in practice. We compare the estimates based on trade and quote data for the same stock and find a remarkable level of agreement. We identify some features of the high-frequency data, which are challenging for realized kernels. They are when there are local trends in the data, over periods of around 10 minutes, where the prices and quotes are driven up or down. These can be associated with high volumes. One explanation for this is that they are due to non-trivial liquidity effects. {\textcopyright} The Author(s). Journal compilation {\textcopyright} Royal Economic Society 2009.},
author = {Barndorff-Nielsen, O. E. and Hansen, P. Reinhard and Lunde, A. and Shephard, N.},
doi = {10.1111/J.1368-423X.2008.00275.X},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Barndorff-Nielsen et al. - 2009 - Realized kernels in practice trades and quotes.pdf:pdf},
issn = {1368-4221},
journal = {The Econometrics Journal},
keywords = {HAC estimator,Long run variance estimator,Market frictions,Quadratic variation,Realized variance},
month = {nov},
number = {3},
pages = {C1--C32},
publisher = {Oxford Academic},
title = {{Realized kernels in practice: trades and quotes}},
url = {https://dx.doi.org/10.1111/j.1368-423X.2008.00275.x},
volume = {12},
year = {2009}
}
@article{Barth2021,
abstract = {We document the rise and fall of an arbitrage trade among hedge funds known as the Treasury cash-futures basis trade. This trade exploited a fundamental disconnect between cash and futures prices of Treasuries. We show that in recent years a replicating portfolio of Treasury bills and futures has been overvalued relative to Treasury notes and bonds, creating an opportunity for arbitrageurs. Using regulatory datasets on hedge fund exposures and repo transactions, we are able to both identify these arbitrage positions and estimate their aggregate size. We show that the basis trade became popular among hedge funds following 2016, rising to make up as much as half of all hedge fund Treasury positions and around a quarter of dealers' repo lending. We present a model and empirical evidence that link the rise in the basis trade to broader developments in the Treasury market, and shows how the trade could contribute to financial instability. In March of 2020, many of the risks of the trade materialized as Treasury market illiquidity associated with the COVID-19 pandemic led to large sales of these basis trade positions among hedge funds. While Treasury market disruptions spurred hedge funds to sell Treasuries, the unwinding of the basis trade was likely a consequence rather than the primary cause of the stress. Prompt intervention by the Federal Reserve may have prevented the trade from accelerating the deterioration of Treasury market functioning. Our results underscore the importance of non-bank actors in the current structure of the Treasury market, and suggest this structure could create risks going forward.},
author = {Barth, Daniel and Kahn, R Jay},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Barth, Kahn - 2021 - Hedge Funds and the Treasury Cash-Futures Disconnect(2).pdf:pdf},
journal = {Office of Financial Research Working Paper Series},
keywords = {G12,G13,G23,Treasuries,basis trade,futures,hedge fund,liquidity JEL Codes: E43,repo,securities dealers},
title = {{Hedge Funds and the Treasury Cash-Futures Disconnect}},
url = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3817544},
volume = {21-01},
year = {2021}
}
@article{Bauer2021,
abstract = {In many areas of decision making, forecasting is an essential pillar. Consequently, there are many different forecasting methods. According to the "No-Free-Lunch Theorem", there is no single forecasting method that performs best for all time series. In other words, each method has its advantages and disadvantages depending on the specific use case. Therefore, the choice of the forecasting method remains a mandatory expert task. However, expert knowledge cannot be fully automated. To establish a level playing field for evaluating the performance of time series forecasting methods in a broad setting, we propose Libra, a forecasting benchmark that automatically evaluates and ranks forecasting methods based on their performance in a diverse set of evaluation scenarios. The benchmark comprises four different use cases, each covering 100 heterogeneous time series taken from different domains. The data set was assembled from publicly available time series and was designed to exhibit much higher diversity than existing forecasting competitions. Based on this benchmark, we perform a comprehensive evaluation to compare different existing time series forecasting methods.},
author = {Bauer, Andr{\'{e}} and Z{\"{u}}fle, Marwin and Eismann, Simon and Grohmann, Johannes and Herbst, Nikolas and Kounev, Samuel},
doi = {10.1145/3427921.3450241;GROUPTOPIC:TOPIC:ACM-PUBTYPE>PROCEEDING;CTYPE:STRING:BOOK},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Bauer et al. - 2021 - Libra A Benchmark for Time Series Forecasting Methods.pdf:pdf},
isbn = {9781450381949},
journal = {ICPE 2021 - Proceedings of the ACM/SPEC International Conference on Performance Engineering},
keywords = {benchmarking,evaluation,time series forecasting},
month = {apr},
pages = {189--200},
publisher = {Association for Computing Machinery, Inc},
title = {{Libra: A Benchmark for Time Series Forecasting Methods}},
url = {https://dl.acm.org/doi/pdf/10.1145/3427921.3450241},
year = {2021}
}
@unpublished{Borri2011,
abstract = {Emerging countries tend to default when their economic conditions worsen. If harsh economic conditions in an emerging country correspond to similar conditions for the U.S. investor, then foreign sovereign bonds are particularly risky. We explore how this mechanism impacts the data and influences a general equilibrium model of optimal borrowing and default. Empirically, the higher the correlation between past foreign bond and U.S. market returns, the higher the average sovereign excess returns. In the model, sovereign defaults and bond prices depend not only on the borrowers' economic conditions, but also on the lenders' time-varying risk-aversion.},
author = {Borri, Nicola and Verdelhan, Adrien},
booktitle = {AFA 2010 Atlanta Meetings Paper},
doi = {10.2139/ssrn.1343746},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Borri, Verdelhan - 2011 - Sovereign Risk Premia(2).pdf:pdf},
issn = {1556-5068},
keywords = {Adrien Verdelhan,Asset pricing,Default risk,Nicola Borri,SSRN,Sovereign Risk Premia,Sovereign debt},
month = {sep},
publisher = {Elsevier BV},
title = {{Sovereign Risk Premia}},
url = {https://papers.ssrn.com/abstract=1343746 http://www.ssrn.com/abstract=1343746},
year = {2011}
}
@article{Bound2001,
abstract = {Economists have devoted increasing attention to the magnitude and consequences of measurement error in their data. Most discussions of measurement error are based on the "classical≓ assumption that errors in measuring a particular variable are uncorrelated with the true value of that variable, the true values of other variables in the model, and any errors in measuring those variables. In this survey, we focus on both the importance of measurement error in standard survey-based economic variables and on the validity of the classical assumption. We begin by summarizing the literature on biases due to measurement error, contrasting the classical assumption and the more general case. We then argue that, while standard methods will not eliminate the bias when measurement errors are not classical, one can often use them to obtain bounds on this bias. Validation studies allow us to assess the magnitude of measurement errors in survey data, and the validity of the classical assumption. In principle, they provide an alternative strategy for reducing or eliminating the bias due to measurement error. We then turn to the work of social psychologists and survey methodologists which identifies the conditions under which measurement error is likely to be important. While there are some important general findings on errors in measuring recall of discrete events, there is less direct guidance on continuous variables such as hourly wages or annual earnings. Finally, we attempt to summarize the validation literature on specific variables: annual earnings, hourly wages, transfer income, assets, hours worked, unemployment, job characteristics like industry, occupation, and union status, health status, health expenditures, and education. In addition to the magnitude of the errors, we also focus on the validity of the classical assumption. Quite often, we find evidence that errors are negatively correlated with true values. The usefulness of validation data in telling us about errors in survey measures can be enhanced if validation data is collected for a random portion of major surveys (rather than, as is usually the case, for a separate convenience sample for which validation data could be obtained relatively easily); if users are more actively involved in the design of validation studies; and if micro data from validation studies can be shared with researchers not involved in the original data collection. {\textcopyright} 2001 Elsevier Inc. All rights reserved.},
author = {Bound, John and Brown, Charles and Mathiowetz, Nancy},
doi = {10.1016/S1573-4412(01)05012-7},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Bound, Brown, Mathiowetz - 2001 - Measurement Error in Survey Data(2).pdf:pdf},
issn = {1573-4412},
journal = {Handbook of Econometrics},
month = {jan},
pages = {3705--3843},
publisher = {Elsevier},
title = {{Measurement Error in Survey Data}},
url = {https://www.sciencedirect.com/science/article/pii/S1573441201050127?casa_token=exR0Myc1jp4AAAAA:6nmwepqipdv1Edr4tvvb5KpECFh45b5auIAbEWOGiG-23G63UuTZ6QGHpITQB-54A4YKCvZsmQ},
volume = {5},
year = {2001}
}
@incollection{Box2013,
abstract = {George Box was born in Gravesend, Kent on 18 October 1919 and, after being educated at grammar school, went to the local polytechnic to study chemistry. When the war intervened he was posted to the British Army Engineers to work as a laboratory assistant in a chemical defence experiment station investigating the effects of poison gas. His job was to carry out tests on small animals and determine the effects of gassing and subsequent treatment but, as the test results varied considerably, Box realized that statistical analysis was required and that any such analysis would have to be done by himself! Being 1942, all that he could do was to purchase some books and teach himself enough statistics to analyze the data. This he certainly did `beyond the call of duty' and his work on experimental design in this area of pathology was recognized with the award of a British Empire Medal at the end of the war.},
author = {Box, George},
booktitle = {A Very British Affair: Six Britons and the Development of Time Series Analysis During the 20th Century},
doi = {10.1057/9781137291264_6},
isbn = {978-1-137-29126-4},
pages = {161--215},
publisher = {Palgrave Macmillan, London},
title = {{Box and Jenkins: Time Series Analysis, Forecasting and Control}},
url = {https://link.springer.com/chapter/10.1057/9781137291264_6},
year = {2013}
}
@book{Brown2004,
author = {Brown, RG},
publisher = {Courier Corporation},
title = {{Smoothing, forecasting and prediction of discrete time series}},
url = {https://books.google.com/books?hl=en&lr=&id=XXFNW_QaJYgC&oi=fnd&pg=PA1&dq=Smoothing,+Forecasting+and+Prediction+of+Discrete+Time+Series+Front+Cover+Robert+Goodell+Brown&ots=EP8dBWrbIc&sig=VQvwke5hTnmCIMvsMmoWcxtKmPU},
year = {2004}
}
@article{Brown1995,
abstract = {Empirical analysis of rates of return in finance implicitly condition on the security surviving into the sample. We investigate the implications of such conditioning on the time series of rates of return. In general this conditioning induces a spurious relationship between observed return and total risk for those securities that survive to be included in the sample. This result has immediate implications for the equity premium puzzle. We show how these results apply to other outstanding problems of empirical finance. Long‐term autocorrelation studies focus on the statistical relation between successive holding period returns, where the holding period is of possibly extensive duration. If the equity market survives, then we find that average return in the beginning is higher than average return near the end of the time period. For this reason, statistical measures of long‐term dependence are typically biased towards the rejection of a random walk. The result also has implications for event studies. There is a strong association between the magnitude of an earnings announcement and the postannouncement performance of the equity. This might be explained in part as an artefact of the stock price performance of firms in financial distress that survive an earnings announcement. The final example considers stock split studies. In this analysis we implicitly exclude securities whose price on announcement is less than the prior average stock price. We apply our results to this case, and find that the condition that the security forms part of our positive stock split sample suffices to explain the upward trend in event‐related cumulated excess return in the preannouncement period. 1995 The American Finance Association},
author = {Brown, Stephen J. and Goetzmann, William N. and Ross, Stephen A.},
doi = {10.1111/J.1540-6261.1995.TB04039.X;REQUESTEDJOURNAL:JOURNAL:15406261;WGROUP:STRING:PUBLICATION},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Brown, Goetzmann, Ross - 1995 - Survival.pdf:pdf},
issn = {0022-1082},
journal = {The Journal of Finance},
month = {jul},
number = {3},
pages = {853--873},
publisher = {John Wiley & Sons, Ltd},
title = {{Survival}},
url = {/doi/pdf/10.1111/j.1540-6261.1995.tb04039.x https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.1995.tb04039.x https://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.1995.tb04039.x},
volume = {50},
year = {1995}
}
@article{Brown1992,
abstract = {Recent evidence suggests that past mutual fund \nperformance predicts future performance. We \nanalyze the relationship between volatility and \nreturns in a sample that is truncated by survivor- \nship and show that this relationship gives rise to \nthe appearance of predictability. We present some \nnumerical examples to show that this effect can be \nstrong enough to account for the strength of the \nevidence favoring return predictability.},
author = {Brown, Stephen J. and Goetzmann, William and Ibbotson, Roger G. and Ross, Stephen A.},
doi = {10.1093/rfs/5.4.553},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Brown et al. - 1992 - Survivorship Bias in Performance Studies.pdf:pdf},
issn = {0893-9454},
journal = {Review of Financial Studies},
month = {oct},
number = {4},
pages = {553--580},
title = {{Survivorship Bias in Performance Studies}},
url = {https://academic.oup.com/rfs/article-abstract/5/4/553/1590264 https://academic.oup.com/rfs/article-lookup/doi/10.1093/rfs/5.4.553},
volume = {5},
year = {1992}
}
@article{Buehlmaier2018,
abstract = {We construct novel measures of financial constraints using textual analysis of firms' annual reports and investigate their impact on stock returns. Our three measures capture access to equity markets, debt markets, and external financial markets in general. In all cases, constrained firms earn higher returns, which move together and cannot be explained by the Fama and French (2015) factor model. A trading strategy based on financial constraints is most profitable for large, liquid stocks. Our results are strongest when we consider debt constraints. A portfolio based on this measure earns an annualized risk-adjusted excess return of 6.5%.},
author = {Buehlmaier, Matthias M.M. and Whited, Toni M},
doi = {10.1093/rfs/hhy007},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Buehlmaier, Whited - 2018 - Are financial constraints priced Evidence from textual analysis.pdf:pdf},
issn = {14657368},
journal = {Review of Financial Studies},
month = {jul},
number = {7},
pages = {2693--2728},
publisher = {Oxford Academic},
title = {{Are financial constraints priced? Evidence from textual analysis}},
url = {https://dx.doi.org/10.1093/rfs/hhy007 https://academic.oup.com/rfs/article/31/7/2693/4824924},
volume = {31},
year = {2018}
}
@book{Burghardt2005,
author = {Burghardt, Galen D. and Belton, Terrence M. and Lane, Morton and Papa, John},
edition = {3rd},
publisher = {McGraw Hill Library of Investment and Finance},
title = {{The Treasury Bond Basis: An In-Depth Analysis for Hedgers, Speculators and Arbitrageurs}},
url = {https://cir.nii.ac.jp/crid/1130282269235515392},
year = {2005}
}
@article{Carhart1997,
abstract = {Using a sample free of survivor bias, I demonstrate that common factors in stock returns and investment expenses almost completely explain persistence in equity mutual funds' mean and risk-adjusted returns. Hendricks, Patel and Zeckhauser's (1993) "hot hands" result is mostly driven by the one-year momentum effect of Jegadeesh and Titman (1993), but individual funds do not earn higher returns from following the momentum strategy in stocks. The only significant persistence not explained is concentrated in strong underperformance by the worst-return mutual funds. The results do not support the existence of skilled or informed mutual fund portfolio managers.},
author = {Carhart, Mark M.},
doi = {10.1111/J.1540-6261.1997.TB03808.X},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Carhart - 1997 - On Persistence in Mutual Fund Performance.pdf:pdf},
issn = {1540-6261},
journal = {The Journal of Finance},
month = {mar},
number = {1},
pages = {57--82},
publisher = {John Wiley & Sons, Ltd},
title = {{On Persistence in Mutual Fund Performance}},
url = {/doi/pdf/10.1111/j.1540-6261.1997.tb03808.x https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.1997.tb03808.x https://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.1997.tb03808.x},
volume = {52},
year = {1997}
}
@article{Challu2022,
abstract = {Recent progress in neural forecasting accelerated improvements in the performance of large-scale forecasting systems. Yet, long-horizon forecasting remains a very difficult task. Two common challenges afflicting the task are the volatility of the predictions and their computational complexity. We introduce N-HiTS, a model which addresses both challenges by incorporating novel hierarchical interpolation and multi-rate data sampling techniques. These techniques enable the proposed method to assemble its predictions sequentially, emphasizing components with different frequencies and scales while decomposing the input signal and synthesizing the forecast. We prove that the hierarchical interpolation technique can efficiently approximate arbitrarily long horizons in the presence of smoothness. Additionally, we conduct extensive large-scale dataset experiments from the long-horizon forecasting literature, demonstrating the advantages of our method over the state-of-the-art methods, where N-HiTS provides an average accuracy improvement of almost 20% over the latest Transformer architectures while reducing the computation time by an order of magnitude (50 times). Our code is available at bit.ly/3VA5DoT},
archivePrefix = {arXiv},
arxivId = {2201.12886},
author = {Challu, Cristian and Olivares, Kin G. and Oreshkin, Boris N. and Ramirez, Federico Garza and Mergenthaler-Canseco, Max and Dubrawski, Artur},
doi = {10.1609/aaai.v37i6.25854},
eprint = {2201.12886},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Challu et al. - 2022 - N-HiTS Neural Hierarchical Interpolation for Time Series Forecasting.pdf:pdf},
isbn = {9781577358800},
issn = {2159-5399},
journal = {Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023},
month = {jan},
pages = {6989--6997},
publisher = {AAAI Press},
title = {{N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting}},
url = {https://arxiv.org/pdf/2201.12886},
volume = {37},
year = {2022}
}
@article{Chen2022a,
abstract = {We examine the incremental information contained in economic research by leveraging unique features of the asset pricing literature. This field offers standardized performance measures, large scale replications, and naive data mining as an alternative to using economic research. We find that mining 29,000 accounting ratios for t-statistics over 2.0 leads to cross-sectional return predictability similar to peer-reviewed research. For both methods, about 50% of predictability remains after the original sample periods. Predictors supported by peer-reviewed risk explanations or equilibrium models underperform other predictors post-sample, suggesting peer review systematically mislabels mispricing as risk, though only 20% of predictors are labelled as risk. Data mining generates other features of economic research including the rise in returns as original sample periods end and the speed of post-sample decay. It also uncovers themes like investment, issuance, and accruals -- decades before they are published.},
archivePrefix = {arXiv},
arxivId = {2212.10317},
author = {Chen, Andrew Y and Lopez-Lira, Alejandro and Zimmermann, Tom and Bybee, Leland and Han, Yufeng and Jensen, Theis and Pontiff, Jeff and Santosh, Shri and For, Yinan Su and Bryzgalova, Svetlana and Clarke, Charlie and Cooper, Mike and Menkveld, Albert and Knox, Ben and Osambela, Emilio and Palazzo, Dino and Ringgenberg, Matt and Xiu, Dacheng and Zheng, Lingling},
eprint = {2212.10317},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Chen et al. - 2022 - Does Peer-Reviewed Research Help Predict Stock Returns.pdf:pdf},
keywords = {G0,G1 Keywords: peer review,JEL Classification: B4,data mining,economic theory,stock market anomalies},
month = {dec},
title = {{Does Peer-Reviewed Research Help Predict Stock Returns?}},
url = {https://arxiv.org/pdf/2212.10317},
year = {2022}
}
@article{Chen2022b,
abstract = {I develop simple and intuitive bounds for the false discovery rate (FDR) in cross-sectional return predictability publications. The simplest bounds require only summary statistics from previous papers, and show the FDR is at most 25% in eight out of nine previous studies. A more refined bound shows the FDR is at most 9%. These studies include Harvey, Liu, and Zhu (2016), who ``argue that most claimed findings in financial economics are likely false.'' I demonstrate how Harvey et al.'s own estimates imply an FDR of 9%, and that their conclusion stems from misinterpreting ``insignificant factor'' as ``false discovery.''},
archivePrefix = {arXiv},
arxivId = {2206.15365},
author = {Chen, Andrew Y.},
eprint = {2206.15365},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Chen - 2022 - Most claimed statistical findings in cross-sectional return predictability are likely true.pdf:pdf},
keywords = {q-fin.GN},
month = {jun},
title = {{Most claimed statistical findings in cross-sectional return predictability are likely true}},
url = {https://arxiv.org/pdf/2206.15365},
year = {2022}
}
@article{Chen2022,
abstract = {We provide data and code that successfully reproduces nearly all cross-sectional stock return predictors. Our 319 characteristics draw from previous meta-studies, but we differ by comparing our t-stats to the original papers{\^{a}}€™ results. For the 161 characteristics that were clearly significant in the original papers, 98% of our long-short portfolios find t-stats above 1.96. For the 44 characteristics that had mixed evidence, our reproductions find t-stats of 2 on average. A regression of reproduced t-stats on original long-short t-stats finds a slope of 0.88 and an R2 of 82%. Mean returns are monotonic in predictive signals at the characteristic level. The remaining 114 characteristics were insignificant in the original papers or are modifications of the originals created by Hou et al. (2020). These remaining characteristics are almost always significant if the original characteristic was also significant.},
author = {Chen, Andrew Y. and Zimmermann, Tom and Chen, Andrew Y. and Zimmermann, Tom},
doi = {10.1561/104.00000112},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Chen et al. - 2022 - Open Source Cross-Sectional Asset Pricing.pdf:pdf},
issn = {21645760},
journal = {Critical Finance Review},
keywords = {Asset pricing,Replication,Stock market anomalies},
month = {may},
number = {2},
pages = {207--264},
publisher = {now publishers},
title = {{Open Source Cross-Sectional Asset Pricing}},
url = {https://econpapers.repec.org/RePEc:now:jnlcfr:104.00000112},
volume = {11},
year = {2022}
}
@article{Constantinides2013,
abstract = {We construct a panel of S&P 500 Index call and put option portfolios, daily adjusted to maintain targeted maturity, moneyness, and unitmarket beta, and testmulti-factor pricing models. The standard linear factor methodology is applicable because the monthly portfolio returns have low skewness and are close to normal. We hypothesize that any one of crisis-related factors incorporating price jumps, volatility jumps, and liquidity (along with themarket) explains the cross-sectional variation in returns. Our hypothesis is not rejected, even when the factor premia are constrained to equal the corresponding premia in the cross-section of equities. The alphas of short-maturity out-of-the-money puts become economically and statistically insignificant.},
author = {Constantinides, George M and Jackwerth, Jens Carsten and Savov, Alexi and Franke, Gu¨nter and Golez, Ben and Grundy, Bruce and Jones, Christopher and Koijen, Ralph and Ruenzi, Stefan and Yaron, Amir},
doi = {10.1093/RAPSTU/RAT004},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Constantinides et al. - 2013 - The Puzzle of Index Option Returns.pdf:pdf},
issn = {2045-9920},
journal = {The Review of Asset Pricing Studies},
month = {dec},
number = {2},
pages = {229--257},
publisher = {Oxford Academic},
title = {{The Puzzle of Index Option Returns}},
url = {https://dx.doi.org/10.1093/rapstu/rat004},
volume = {3},
year = {2013}
}
@article{Das2023a,
abstract = {Recent work has shown that simple linear models can outperform several Transformer based approaches in long term time-series forecasting. Motivated by this, we propose a Multi-layer Perceptron (MLP) based encoder-decoder model, Time-series Dense Encoder (TiDE), for long-term time-series forecasting that enjoys the simplicity and speed of linear models while also being able to handle covariates and non-linear dependencies. Theoretically, we prove that the simplest linear analogue of our model can achieve near optimal error rate for linear dynamical systems (LDS) under some assumptions. Empirically, we show that our method can match or outperform prior approaches on popular long-term time-series forecasting benchmarks while being 5-10x faster than the best Transformer based model.},
archivePrefix = {arXiv},
arxivId = {2304.08424},
author = {Das, Abhimanyu and Kong, Weihao and Leach, Andrew and Mathur, Shaan and Sen, Rajat and Yu, Rose},
eprint = {2304.08424},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Das et al. - 2023 - Long-term Forecasting with TiDE Time-series Dense Encoder.pdf:pdf},
issn = {28358856},
journal = {Transactions on Machine Learning Research},
month = {apr},
publisher = {Transactions on Machine Learning Research},
title = {{Long-term Forecasting with TiDE: Time-series Dense Encoder}},
url = {https://arxiv.org/pdf/2304.08424},
volume = {2023},
year = {2023}
}
@article{Das2023,
abstract = {Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.},
archivePrefix = {arXiv},
arxivId = {2310.10688},
author = {Das, Abhimanyu and Kong, Weihao and Sen, Rajat and Zhou, Yichen},
eprint = {2310.10688},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Das et al. - 2023 - A decoder-only foundation model for time-series forecasting.pdf:pdf},
issn = {26403498},
journal = {Proceedings of Machine Learning Research},
month = {oct},
pages = {10148--10167},
publisher = {ML Research Press},
title = {{A decoder-only foundation model for time-series forecasting}},
url = {https://arxiv.org/pdf/2310.10688},
volume = {235},
year = {2023}
}
@article{Dau2019,
abstract = {The UCR time series archive-introduced in 2002, has become an important resource in the time series data mining community, with at least one thousand published papers making use of at least one data set from the archive. The original incarnation of the archive had sixteen data sets but since that time, it has gone through periodic expansions. The last expansion took place in the summer of 2015 when the archive grew from 45 to 85 data sets. This paper introduces and will focus on the new data expansion from 85 to 128 data sets. Beyond expanding this valuable resource, this paper offers pragmatic advice to anyone who may wish to evaluate a new algorithm on the archive. Finally, this paper makes a novel and yet actionable claim: of the hundreds of papers that show an improvement over the standard baseline 1-nearest neighbor classification , a fraction might be mis-attributing the reasons for their improvement. Moreover, the improvements claimed by these papers might have been achievable with a much simpler modification, requiring just a few lines of code.},
archivePrefix = {arXiv},
arxivId = {1810.07758},
author = {Dau, Hoang Anh and Bagnall, Anthony and Kamgar, Kaveh and Yeh, Chin Chia Michael and Zhu, Yan and Gharghabi, Shaghayegh and Ratanamahatana, Chotirat Annh and Keogh, Eamonn},
doi = {10.1109/JAS.2019.1911747},
eprint = {1810.07758},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Dau et al. - 2019 - The UCR time series archive.pdf:pdf},
issn = {23299274},
journal = {IEEE/CAA Journal of Automatica Sinica},
month = {nov},
number = {6},
pages = {1293--1305},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{The UCR time series archive}},
volume = {6},
year = {2019}
}
@article{DeLivera2011a,
abstract = {An innovations state space modeling framework is introduced for forecasting complex seasonal time series such as those with multiple seasonal periods, high-frequency seasonality, non-integer seasonality, and dual-calendar effects. The new framework incorporates Box-Cox transformations, Fourier representations with time varying coefficients, and ARMA error correction. Likelihood evaluation and analytical expressions for point forecasts and interval predictions under the assumption of Gaussian errors are derived, leading to a simple, comprehensive approach to forecasting complex seasonal time series. A key feature of the framework is that it relies on a new method that greatly reduces the computational burden in the maximum likelihood estimation. The modeling framework is useful for a broad range of applications, its versatility being illustrated in three empirical studies. In addition, the proposed trigonometric formulation is presented as a means of decomposing complex seasonal time series, and it is shown that this decomposition leads to the identification and extraction of seasonal components which are otherwise not apparent in the time series plot itself. {\textcopyright} 2011 American Statistical Association.},
author = {de Livera, Alysha M. and Hyndman, Rob J. and Snyder, Ralph D.},
doi = {10.1198/JASA.2011.TM09771;WEBSITE:WEBSITE:TFOPB;JOURNAL:JOURNAL:UASA18;PAGEGROUP:STRING:PUBLICATION},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Fourier series,Prediction intervals,Seasonality,State space models,Time series decomposition},
month = {dec},
number = {496},
pages = {1513--1527},
publisher = {Taylor & Francis},
title = {{Forecasting time series with complex seasonal patterns using exponential smoothing}},
url = {https://scholar.google.com/scholar_url?url=https://www.tandfonline.com/doi/pdf/10.1198/jasa.2011.tm09771%3Fcasa_token%3DSh2PcXcQZhIAAAAA:d_UpGdtoPvY2bJ3A4v72M98SxHDDDkBseNrhKVBzBA29KhNkCi3kaWnLuEYHimSJ_0Zl1GvrnYwuXA&hl=en&sa=T&oi=ucasa&ct=ucasa&ei=kZdtaNg},
volume = {106},
year = {2011}
}
@article{Dempster2025,
abstract = {We introduce MONSTER-the MONash Scalable Time Series Evaluation Repository-a collection of large datasets for time series classification. The field of time series classification has benefitted from common benchmarks set by the UCR and UEA time series classification repositories. However, the datasets in these benchmarks are small, with median sizes of 217 and 255 examples, respectively. In consequence they favour a narrow subspace of models that are optimised to achieve low classification error on a wide variety of smaller datasets, that is, models that minimise variance, and give little weight to computational issues such as scalability. Our hope is to diversify the field by introducing benchmarks using larger datasets. We believe that there is enormous potential for new progress in the field by engaging with the theoretical and practical challenges of learning effectively from larger quantities of data.},
archivePrefix = {arXiv},
arxivId = {2502.15122},
author = {Dempster, Angus and Foumani, Navid Mohammadi and Tan, Chang Wei and Miller, Lynn and Mishra, Amish and Salehi, Mahsa and Pelletier, Charlotte and Schmidt, Daniel F. and Webb, Geoffrey I.},
eprint = {2502.15122},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Dempster et al. - 2025 - MONSTER Monash Scalable Time Series Evaluation Repository.pdf:pdf},
keywords = {benchmark,bitter lesson,dataset,time series classification},
month = {feb},
title = {{MONSTER: Monash Scalable Time Series Evaluation Repository}},
url = {https://arxiv.org/pdf/2502.15122},
year = {2025}
}
@article{Deng2009,
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li Jia and Li, Kai and Fei-Fei, Li},
doi = {10.1109/CVPR.2009.5206848},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Deng et al. - 2009 - ImageNet A Large-Scale Hierarchical Image Database.pdf:pdf},
isbn = {9781424439911},
journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2009},
pages = {248--255},
publisher = {IEEE Computer Society},
title = {{ImageNet: A Large-Scale Hierarchical Image Database}},
year = {2009}
}
@article{Dickerson2023,
abstract = {Recent studies document strong empirical support for multifactor models that aim to explain the cross-sectional variation in corporate bond expected excess returns. We revisit these findings and provide evidence that common factor pricing in corporate bonds is exceedingly difficult to establish. Based on portfolio- and bond-level analyses, we demonstrate that previously proposed bond risk factors, with traded liquidity as the only marginal exception, do not have any incremental explanatory power over the corporate bond market factor. Consequently, this implies that the bond CAPM is not dominated by either traded- or nontraded-factor models in pairwise and multiple model comparison tests.},
author = {Dickerson, Alexander and Mueller, Philippe and Robotti, Cesare},
doi = {10.1016/J.JFINECO.2023.103707},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Dickerson, Mueller, Robotti - 2023 - Priced risk in corporate bonds(2).pdf:pdf},
issn = {0304-405X},
journal = {Journal of Financial Economics},
keywords = {Bond CAPM,Corporate bond pricing,Efficient frontier,Model misspecification and identification,Sharpe ratio},
month = {nov},
number = {2},
pages = {103707},
publisher = {North-Holland},
title = {{Priced risk in corporate bonds}},
url = {https://www.sciencedirect.com/science/article/pii/S0304405X23001393},
volume = {150},
year = {2023}
}
@article{Dickerson2024,
author = {Dickerson, Alexander and Robotti, Cesare and Rossetti, Giulio},
doi = {10.2139/SSRN.4575879},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Dickerson, Robotti, Rossetti - 2024 - Common pitfalls in the evaluation of corporate bond strategies.pdf:pdf},
journal = {SSRN Electronic Journal},
keywords = {Alexander Dickerson,Bidask bias,Cesare Robotti,Common pitfalls in the evaluation of corporate bon,Corporate bond anomalies,Corporate bond strategies,Data uncertainty,Empirical asset pricing,Giulio Rossetti,Look-ahead bias,Market microstructure noise,SSRN,Sharpe ratio},
month = {sep},
publisher = {Elsevier BV},
title = {{Common pitfalls in the evaluation of corporate bond strategies}},
url = {https://papers.ssrn.com/abstract=4575879},
year = {2024}
}
@article{Drechsler2017,
abstract = {We present a new channel for the transmission of monetary policy, the deposits channel. We show that when the Fed funds rate rises, banks widen the spreads they charge on deposits, and deposits flow out of the banking system. We present a model where this is due to market power in deposit markets. Consistent with the market power mechanism, deposit spreads increase more and deposits flow out more in concentrated markets. This is true even when we control for lending opportunities by only comparing different branches of the same bank. Since deposits are the main source of liquid assets for households, the deposits channel can explain the observed strong relationship between the liquidity premium and the Fed funds rate. Since deposits are also a uniquely stable funding source for banks, the deposits channel impacts bank lending. When the Fed funds rate rises, banks that raise deposits in concentrated markets contract their lending by more than other banks. Our estimates imply that the deposits channel can account for the entire transmission of monetary policy through bank balance sheets. JEL Codes: E52, E58, G12, G21.},
author = {Drechsler, Itamar and Savov, Alexi and Schnabl, Philipp},
doi = {10.1093/QJE/QJX019},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Drechsler, Savov, Schnabl - 2017 - The Deposits Channel of Monetary Policy.pdf:pdf},
issn = {0033-5533},
journal = {The Quarterly Journal of Economics},
month = {nov},
number = {4},
pages = {1819--1876},
publisher = {Oxford Academic},
title = {{The Deposits Channel of Monetary Policy}},
url = {https://dx.doi.org/10.1093/qje/qjx019},
volume = {132},
year = {2017}
}
@article{Du2023,
abstract = {We document a regime change in the Treasury market post-Global Financial Crisis (GFC): dealers switched from net short to net long Treasury bonds. We construct “net-long” and “net-short” curves that account for balance sheet and financing costs, and show that actual yields moved from the net short curve pre-GFC to the net long curve post-GFC. Our theory shows the regime shift caused negative swap spreads and co-movement among swap spreads, dealer positions, and covered-interest-parity violations. Furthermore, the effects of various monetary and regulatory policies are regime-dependent. We highlight Treasury supply as a plausible driver of this regime shift.},
author = {Du, Wenxin and H{\'{e}}bert, Benjamin and Li, Wenhao},
doi = {10.1016/J.JFINECO.2023.103722},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Du, H{\'{e}}bert, Li - 2023 - Intermediary balance sheets and the treasury yield curve(2).pdf:pdf},
issn = {0304-405X},
journal = {Journal of Financial Economics},
keywords = {Arbitrage,Covered interest parity,Intermediary asset pricing,Treasury bonds},
month = {dec},
number = {3},
pages = {103722},
publisher = {North-Holland},
title = {{Intermediary balance sheets and the treasury yield curve}},
url = {https://www.sciencedirect.com/science/article/pii/S0304405X23001629?casa_token=_wxgX9DeZS0AAAAA:rcSOjn22URbWm0GFi-xlPzzAatt7LuJE66AzytGlUBGp3Lt_dLf3jG30pMfJWidKHBdrFNRNGA},
volume = {150},
year = {2023}
}
@article{Du2018,
abstract = {We find that deviations from the covered interest rate parity (CIP) condition imply large, persistent, and systematic arbitrage opportunities in one of the largest asset markets in the world. Contrary to the common view, these deviations for major currencies are not explained away by credit risk or transaction costs. They are particularly strong for forward contracts that appear on banks' balance sheets at the end of the quarter, pointing to a causal effect of banking regulation on asset prices. The CIP deviations also appear significantly correlated with other fixed income spreads and with nominal interest rates.},
author = {Du, Wenxin and Tepper, Alexander and Verdelhan, Adrien},
doi = {10.1111/JOFI.12620},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Du, Tepper, Verdelhan - 2018 - Deviations from Covered Interest Rate Parity.pdf:pdf},
issn = {1540-6261},
journal = {The Journal of Finance},
month = {jun},
number = {3},
pages = {915--957},
publisher = {John Wiley & Sons, Ltd},
title = {{Deviations from Covered Interest Rate Parity}},
url = {https://onlinelibrary.wiley.com/doi/full/10.1111/jofi.12620 https://onlinelibrary.wiley.com/doi/abs/10.1111/jofi.12620 https://onlinelibrary.wiley.com/doi/10.1111/jofi.12620},
volume = {73},
year = {2018}
}
@article{Duffie1999,
abstract = {This review of the pricing of credit swaps, a form of derivative security that can be viewed as default insurance on loans or bonds, begins with a description of the credit swap contract, turns to pricing by reference to spreads over the risk-free rate of par floating-rate bonds of the same quality, and then considers model-based pricing. The role of asset swap spreads as a reference for pricing credit swaps is also considered.},
author = {Duffie, Darrell},
doi = {10.2469/FAJ.V55.N1.2243;WEBSITE:WEBSITE:TFOPB;PAGEGROUP:STRING:PUBLICATION},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Duffie - 1999 - Credit Swap Valuation.pdf:pdf},
issn = {0015198X},
journal = {Financial Analysts Journal},
number = {1},
pages = {73--87},
publisher = {CFA Institute},
title = {{Credit Swap Valuation}},
url = {https://www.tandfonline.com/doi/abs/10.2469/faj.v55.n1.2243},
volume = {55},
year = {1999}
}
@article{Elton1996,
abstract = {Mutual fund attrition can create problems for a researcher because funds that disappear tend to do so due to poor performance. In this article we estimate the size of the bias by tracking all funds that existed at the end of 1976. When a fund merges we calculate the return, taking into account the merger terms. This allows a precise estimate of survivorship bias. In addition, we examine characteristics of both mutual funds that merge and their partner funds. Estimates of survivorship bias over different horizons and using different models to evaluate performance are provided.},
author = {Elton, Edwin J. and Gruber, Martin J. and Blake, Christopher R.},
doi = {10.1093/RFS/9.4.1097},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Elton, Gruber, Blake - 1996 - Survivor Bias and Mutual Fund Performance.pdf:pdf},
issn = {0893-9454},
journal = {The Review of Financial Studies},
month = {oct},
number = {4},
pages = {1097--1120},
publisher = {Oxford Academic},
title = {{Survivor Bias and Mutual Fund Performance}},
url = {https://dx.doi.org/10.1093/rfs/9.4.1097},
volume = {9},
year = {1996}
}
@article{Fama1993,
abstract = {This paper identifies five common risk factors in the returns on stocks and bonds. There are three stock-market factors: an overall market factor and factors related to firm size and book-to-market equity. There are two bond-market factors, related to maturity and default risks. Stock returns have shared variation due to the stock-market factors, and they are linked to bond returns through shared variation in the bond-market factors. Except for low-grade corporates, the bond-market factors capture the common variation in bond returns. Most important, the five factors seem to explain average returns on stocks and bonds. {\textcopyright} 1993.},
author = {Fama, Eugene F. and French, Kenneth R.},
doi = {10.1016/0304-405X(93)90023-5},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Fama, French - 1993 - Common risk factors in the returns on stocks and bonds(2).pdf:pdf},
issn = {0304-405X},
journal = {Journal of Financial Economics},
month = {feb},
number = {1},
pages = {3--56},
publisher = {North-Holland},
title = {{Common risk factors in the returns on stocks and bonds}},
url = {https://www.sciencedirect.com/science/article/abs/pii/0304405X93900235},
volume = {33},
year = {1993}
}
@article{Fleckenstein2020,
abstract = {A long-standing asset pricing puzzle is that the funding rates in derivatives contracts often differ from those in cash markets. We propose that the cost of renting intermediary balance sheet space may help resolve this puzzle. We study a persistent basis in what is arguably the largest derivatives market, namely, the interest rate futures market. This basis is strongly related to exogenous measures of intermediary balance sheet usage and proxies for the balance sheet costs imposed by debt overhang problems and capital regulation. These results extend to the cash derivatives bases documented in many of the other largest financial markets.},
author = {Fleckenstein, Matthias and Longstaff, Francis A.},
doi = {10.1093/RFS/HHAA033},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Fleckenstein, Longstaff - 2020 - Renting Balance Sheet Space Intermediary Balance Sheet Rental Costs and the Valuation of Derivatives.pdf:pdf},
issn = {0893-9454},
journal = {The Review of Financial Studies},
month = {nov},
number = {11},
pages = {5051--5091},
publisher = {Oxford Academic},
title = {{Renting Balance Sheet Space: Intermediary Balance Sheet Rental Costs and the Valuation of Derivatives}},
url = {https://dx.doi.org/10.1093/rfs/hhaa033},
volume = {33},
year = {2020}
}
@article{Fleckenstein2014,
abstract = {We show that the price of a Treasury bond and an inflation-swapped Treasury Inflation-Protected Securities (TIPS) issue exactly replicating the cash flows of the Treasury bond can differ by more than $20 per $100 notional. Treasury bonds are almost always overvalued relative to TIPS. Total TIPS-Treasury mispricing has exceeded $56 billion, representing nearly 8% of the total amount of TIPS outstanding. We find direct evidence that the mispricing narrows as additional capital flows into the markets. This provides strong support for the slow-moving-capital explanation of arbitrage persistence.},
author = {Fleckenstein, Matthias and Longstaff, Francis A. and Lustig, Hanno},
doi = {10.1111/jofi.12032},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Fleckenstein, Longstaff, Lustig - 2014 - The TIPS-Treasury bond puzzle.pdf:pdf},
issn = {15406261},
journal = {Journal of Finance},
number = {5},
pages = {2151--2197},
publisher = {Blackwell Publishing Ltd},
title = {{The TIPS-Treasury bond puzzle}},
volume = {69},
year = {2014}
}
@unpublished{FleckensteinFrancisLongstaffHannoLustig2010,
abstract = {We show that the price of a Treasury bond and an inflation-swapped TIPS issue exactly replicating the cash flows of the Treasury bond can differ by more than $20 per $100 notional. Treasury bonds are almost always overvalued relative to TIPS. Total TIPS-Treasury mispricing has exceeded $56 billion, representing nearly eight percent of the total amount of TIPS outstanding. TIPS-Treasury mispricing is strongly related to supply factors such as Treasury debt issuance and the availability of collateral in the financial markets, and is correlated with other types of fixed-income arbitrages, These results pose a major puzzle to classical asset pricing theory. In addition, they raise the issue of why the Treasury issues TIPS, since in so doing it both gives up a valuable fiscal hedging option and leaves large amounts of money on the table.},
address = {Cambridge, MA},
author = {Fleckenstein, Matthias and Longstaff, Francis and Lustig, Hanno},
doi = {10.3386/w16358},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Fleckenstein, Longstaff, Lustig - 2010 - Why Does the Treasury Issue Tips The Tips-Treasury Bond Puzzle.pdf:pdf},
institution = {National Bureau of Economic Research},
keywords = {Francis A. Longstaff,Hanno Lustig,Matthias Fleckenstein},
month = {sep},
series = {National Bureau of Economic Research},
title = {{Why Does the Treasury Issue Tips? The Tips-Treasury Bond Puzzle}},
url = {https://www.nber.org/papers/w16358 http://www.nber.org/papers/w16358.pdf},
year = {2010}
}
@article{Fung2000,
abstract = {It is well known that the pro forma performance of a sample of investment funds contains biases. These biases are documented in Brown, Goetzmann, Ibbotson, and Ross (1992) using mutual funds as subjects. The organization structure of hedge funds, as private and often offshore vehicles, makes data collection a much more onerous task, amplifying the impact of performance measurement biases. Theis paper reviews these biases in hedge funds. We also propose using funds-of-hedge funds to measure aggregate hedge fund performance, based on the idea that the investment experience of hedge fund investors can be used to estimate the performance of hedge funds.},
author = {Fung, William and Hsieh, David A.},
doi = {10.2307/2676205},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Fung, Hsieh - 2000 - Performance Characteristics of Hedge Funds and Commodity Funds Natural vs. Spurious Biases.pdf:pdf},
issn = {1756-6916},
journal = {Journal of Financial and Quantitative Analysis},
month = {sep},
number = {3},
pages = {291--307},
publisher = {Cambridge University Press},
title = {{Performance Characteristics of Hedge Funds and Commodity Funds: Natural vs. Spurious Biases}},
url = {https://www.cambridge.org/core/journals/journal-of-financial-and-quantitative-analysis/article/abs/performance-characteristics-of-hedge-funds-and-commodity-funds-natural-vs-spurious-biases/8FE7B2824AB6EC3F0D299FBCA633FCAB},
volume = {35},
year = {2000}
}
@article{Godahewa2021,
abstract = {Many businesses and industries nowadays rely on large quantities of time series data making time series forecasting an important research area. Global forecasting models that are trained across sets of time series have shown a huge potential in providing accurate forecasts compared with the traditional univariate forecasting models that work on isolated series. However, there are currently no comprehensive time series archives for forecasting that contain datasets of time series from similar sources available for the research community to evaluate the performance of new global forecasting algorithms over a wide variety of datasets. In this paper, we present such a comprehensive time series forecasting archive containing 20 publicly available time series datasets from varied domains, with different characteristics in terms of frequency, series lengths, and inclusion of missing values. We also characterise the datasets, and identify similarities and differences among them, by conducting a feature analysis. Furthermore, we present the performance of a set of standard baseline forecasting methods over all datasets across eight error metrics, for the benefit of researchers using the archive to benchmark their forecasting algorithms.},
archivePrefix = {arXiv},
arxivId = {2105.06643},
author = {Godahewa, Rakshitha and Bergmeir, Christoph and Webb, Geoffrey I. and Hyndman, Rob J. and Montero-Manso, Pablo},
eprint = {2105.06643},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Godahewa et al. - 2021 - Monash Time Series Forecasting Archive.pdf:pdf},
keywords = {baseline evaluation,benchmark datasets,feature analysis,global time series forecasting},
month = {may},
title = {{Monash Time Series Forecasting Archive}},
url = {https://arxiv.org/abs/2105.06643v1},
year = {2021}
}
@book{Goodfellow2016,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
isbn = {0262035618},
publisher = {The MIT press},
title = {{Deep Learning}},
url = {http://www.deeplearningbook.org},
year = {2016}
}
@article{Griliches1986,
author = {Griliches, Zvi},
doi = {10.1016/S1573-4412(86)03005-2},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Griliches - 1986 - Chapter 25 Economic data issues(2).pdf:pdf},
issn = {1573-4412},
journal = {Handbook of Econometrics},
month = {jan},
pages = {1465--1514},
publisher = {Elsevier},
title = {{Chapter 25 Economic data issues}},
url = {https://www.sciencedirect.com/science/article/pii/S1573441286030052},
volume = {3},
year = {1986}
}
@article{Gu2020,
abstract = {We perform a comparative analysis of machine learning methods for the canonical problem of empirical asset pricing: measuring asset risk premiums. We demonstrate large economic gains to investors using machine learning forecasts, in some cases doubling the performance of leading regression-based strategies from the literature. We identify the best-performing methods (trees and neural networks) and trace their predictive gains to allowing nonlinear predictor interactions missed by other methods. All methods agree on the same set of dominant predictive signals, a set that includes variations on momentum, liquidity, and volatility. Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.},
author = {Gu, Shihao and Kelly, Bryan and Xiu, Dacheng},
doi = {10.1093/RFS/HHAA009},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Gu, Kelly, Xiu - 2020 - Empirical Asset Pricing via Machine Learning.pdf:pdf},
issn = {0893-9454},
journal = {The Review of Financial Studies},
month = {may},
number = {5},
pages = {2223--2273},
publisher = {Oxford Academic},
title = {{Empirical Asset Pricing via Machine Learning}},
url = {https://dx.doi.org/10.1093/rfs/hhaa009},
volume = {33},
year = {2020}
}
@article{Gurkaynak2007,
abstract = {The discount function, which determines the value of all future nominal payments, is the most basic building block of finance and is usually inferred from the Treasury yield curve. It is therefore surprising that researchers and practitioners do not have available to them a long history of high-frequency yield curve estimates. This paper fills that void by making public the Treasury yield curve estimates of the Federal Reserve Board at a daily frequency from 1961 to the present. We use a well-known and simple smoothing method that is shown to fit the data very well. The resulting estimates can be used to compute yields or forward rates for any horizon. We hope that the data, which are posted on the website http://www.federalreserve.gov/pubs/feds/2006 and which will be updated quarterly, will provide a benchmark yield curve that will be useful to applied economists. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
author = {G{\"{u}}rkaynak, Refet S. and Sack, Brian and Wright, Jonathan H.},
doi = {10.1016/J.JMONECO.2007.06.029},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/G{\"{u}}rkaynak, Sack, Wright - 2007 - The U.S. Treasury yield curve 1961 to the present(3).pdf:pdf},
issn = {0304-3932},
journal = {Journal of Monetary Economics},
keywords = {High-frequency data,On the run premia,Treasury market,Yield curve},
month = {nov},
number = {8},
pages = {2291--2304},
publisher = {North-Holland},
title = {{The U.S. Treasury yield curve: 1961 to the present}},
url = {https://www.sciencedirect.com/science/article/pii/S0304393207000840?casa_token=sBgB3KQh31UAAAAA:upQ35XDjoK8h0G5uyjbBCWVPDeBzRKcveoQNlhmktccNALNUE1rmmdirdUDj-fft3NcJivGQnw},
volume = {54},
year = {2007}
}
@article{Gurkaynak2010,
abstract = {For over ten years, the Treasury has issued index-linked debt. This paper describes the methodology for fitting a smoothed yield curve to these securities that is used at the Federal Reserve Board every day, and makes the estimates public. Comparison with the corresponding nominal yield curve allows measures of inflation compensation to be computed. We discuss the interpretation of inflation compensation, and provide evidence that it is not a pure measure of inflation expectations being distorted by inflation risk premium and liquidity premium components. We attempt to estimate the TIPS liquidity premium and to extract underlying inflation expectations.},
author = {G{\"{u}}rkaynak, Refet S. and Sack, Brian and Wright, Jonathan H.},
doi = {10.1257/MAC.2.1.70},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/G{\"{u}}rkaynak, Sack, Wright - 2010 - The TIPS Yield Curve and Inflation Compensation(2).pdf:pdf},
issn = {1945-7707},
journal = {American Economic Journal: Macroeconomics},
keywords = {Debt Management,Deflation,Inflation,Interest Rates: Determination,National Debt,Price Level,Sovereign Debt,Term Structure,and Effects},
month = {jan},
number = {1},
pages = {70--92},
publisher = {American Economic Association},
title = {{The TIPS Yield Curve and Inflation Compensation}},
volume = {2},
year = {2010}
}
@article{Hanson2023,
abstract = {We develop a model in which long-term swap spreads are determined by end users' demand for swaps, constrained dealers' supply of swaps, and the risk of future imbalances between demand and supply. Exploiting the sign restrictions implied by our model, we estimate these unobserved demand and supply factors using data on swap spreads and a proxy for dealers' swap arbitrage positions. We find that demand and supply play equally important roles in driving the observed variation in swap spreads. Yet, as predicted by the model, demand plays a more important role in shaping the expected returns on swap spread arbitrage, which embed a premium for bearing future demand-supply imbalance risk. Hedging activity from mortgage investors seems to play a key role in driving the demand for swaps. By contrast, the supply of swaps is closely linked to proxies for the tightness of dealers' constraints. Finally, our analysis helps explain the relationship between swap spreads and other no-arbitrage violations. Abstract We develop a model in which long-term swap spreads are determined by end users' demand},
author = {Hanson, Samuel Gregory and Malkhozov, Aytek and Venter, Gyuri},
doi = {10.2139/SSRN.4301140},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Hanson, Malkhozov, Venter - 2023 - Demand-and-Supply Imbalance Risk and Long-Term Swap Spreads.pdf:pdf},
journal = {SSRN Electronic Journal},
keywords = {Aytek Malkhozov,Demand-and-Supply Imbalance Risk and Long-Term Swa,Gyuri Venter,SSRN,Samuel Gregory Hanson,intermediary capital constraints,limits to arbitrage,swap spreads},
month = {jul},
publisher = {Elsevier BV},
title = {{Demand-and-Supply Imbalance Risk and Long-Term Swap Spreads}},
url = {https://papers.ssrn.com/abstract=4301140},
year = {2023}
}
@article{Harvey2016,
abstract = {Hundreds of papers and factors attempt to explain the cross-section of expected returns. Given this extensive data mining, it does not make sense to use the usual criteria for establishing significance. Which hurdle should be used for current research? Our paper introduces a new multiple testing framework and provides historical cutoffs from the first empirical tests in 1967 to today. A new factor needs to clear a much higher hurdle, with a t-statistic greater than 3.0. We argue that most claimed research findings in financial economics are likely false.},
author = {Harvey, Campbell R. and Liu, Yan and Zhu, Heqing},
doi = {10.1093/RFS/HHV059},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Harvey, Liu, Zhu - 2016 - {\ldots} and the Cross-Section of Expected Returns(2).pdf:pdf},
issn = {0893-9454},
journal = {The Review of Financial Studies},
month = {jan},
number = {1},
pages = {5--68},
publisher = {Oxford Academic},
title = {{{\ldots} and the Cross-Section of Expected Returns}},
url = {https://dx.doi.org/10.1093/rfs/hhv059},
volume = {29},
year = {2016}
}
@article{Hazelkorn2023,
abstract = {Deviations from the law of one price between futures and spot prices—the futures-cash basis—capture information about liquidity demand for equity market exposure in global markets. We show that the basis comoves with dealer and investor futures positions, is contemporaneously positively correlated with futures and spot market returns, and negatively predicts futures and spot returns. These findings are consistent with the futures-cash basis reflecting liquidity demand that is common to futures and cash equity markets. We find persistent supply-demand imbalances for equity index exposure reflected in the basis, giving rise to an annual premium of 5% to 6%.},
author = {Hazelkorn, Todd M. and Moskowitz, Tobias J. and Vasudevan, Kaushik},
doi = {10.1111/JOFI.13198;JOURNAL:JOURNAL:15406261;CTYPE:STRING:JOURNAL},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Hazelkorn, Moskowitz, Vasudevan - 2023 - Beyond Basis Basics Liquidity Demand and Deviations from the Law of One Price.pdf:pdf},
issn = {15406261},
journal = {Journal of Finance},
month = {feb},
number = {1},
pages = {301--345},
publisher = {John Wiley and Sons Inc},
title = {{Beyond Basis Basics: Liquidity Demand and Deviations from the Law of One Price}},
url = {/doi/pdf/10.1111/jofi.13198 https://onlinelibrary.wiley.com/doi/abs/10.1111/jofi.13198 https://onlinelibrary.wiley.com/doi/10.1111/jofi.13198},
volume = {78},
year = {2023}
}
@article{He2017,
abstract = {We find that shocks to the equity capital ratio of financial intermediaries—Primary Dealer counterparties of the New York Federal Reserve—possess significant explanatory power for cross-sectional variation in expected returns. This is true not only for commonly studied equity and government bond market portfolios, but also for other more sophisticated asset classes such as corporate and sovereign bonds, derivatives, commodities, and currencies. Our intermediary capital risk factor is strongly procyclical, implying countercyclical intermediary leverage. The price of risk for intermediary capital shocks is consistently positive and of similar magnitude when estimated separately for individual asset classes, suggesting that financial intermediaries are marginal investors in many markets and hence key to understanding asset prices.},
author = {He, Zhiguo and Kelly, Bryan and Manela, Asaf},
doi = {10.1016/J.JFINECO.2017.08.002},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/He, Kelly, Manela - 2017 - Intermediary asset pricing New evidence from many asset classes(2).pdf:pdf},
issn = {0304-405X},
journal = {Journal of Financial Economics},
keywords = {Intermediary capital,Leverage cycles,Primary dealers,Sophisticated asset classes},
month = {oct},
number = {1},
pages = {1--35},
publisher = {North-Holland},
title = {{Intermediary asset pricing: New evidence from many asset classes}},
volume = {126},
year = {2017}
}
@article{Hu2018,
abstract = {Financial time series (FinTS) record the behavior of human-brain-augmented decision-making, capturing valuable historical information that can be leveraged for profitable investment strategies. Not surprisingly, this area has attracted considerable attention from researchers, who have proposed a wide range of methods based on various backbones. However, the evaluation of the area often exhibits three systemic limitations: 1. Failure to account for the full spectrum of stock movement patterns observed in dynamic financial markets. (Diversity Gap), 2. The absence of unified assessment protocols undermines the validity of cross-study performance comparisons. (Standardization Deficit), and 3. Neglect of critical market structure factors, resulting in inflated performance metrics that lack practical applicability. (Real-World Mismatch). Addressing these limitations, we propose FinTSB, a comprehensive and practical benchmark for financial time series forecasting (FinTSF). To increase the variety, we categorize movement patterns into four specific parts, tokenize and pre-process the data, and assess the data quality based on some sequence characteristics. To eliminate biases due to different evaluation settings, we standardize the metrics across three dimensions and build a user-friendly, lightweight pipeline incorporating methods from various backbones. To accurately simulate real-world trading scenarios and facilitate practical implementation, we extensively model various regulatory * Equal contribution † Corresponding author: Dawei Cheng Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference, Date, Preprint. ACM ISBN 978-1-4503-XXXX-X/18/06 https://doi.org/XXXXXXX.XXXXXXX constraints, including transaction fees, among others. Finally, we conduct extensive experiments on FinTSB, highlighting key insights to guide model selection under varying market conditions. Overall, FinTSB provides researchers with a novel and comprehensive platform for improving and evaluating FinTSF methods. The code is available at https://github.com/TongjiFinLab/FinTSBenchmark.},
archivePrefix = {arXiv},
arxivId = {2502.18834v1},
author = {Hu, Yifan and Li, Yuante and Liu, Peiyuan and Zhu, Yuxia and Li, Naiqi and Dai, Tao and Cheng, Dawei and Jiang, Changjun and Xia, Shu-tao},
doi = {XXXXXXX.XXXXXXX},
eprint = {2502.18834v1},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Hu et al. - 2018 - FinTSB A Comprehensive and Practical Benchmark for Financial Time Series Forecasting Shu-tao Xia.pdf:pdf},
journal = {Proceedings of  (Conference)},
keywords = {Benchmark,Computational Finance,Financial Time Series,Quantitative Trading},
month = {feb},
title = {{FinTSB: A Comprehensive and Practical Benchmark for Financial Time Series Forecasting Shu-tao Xia}},
url = {https://arxiv.org/pdf/2502.18834v1},
volume = {1},
year = {2018}
}
@article{Jensen2023,
abstract = {Several papers argue that financial economics faces a replication crisis because the majority of studies cannot be replicated or are the result of multiple testing of too many factors. We develop and estimate a Bayesian model of factor replication that leads to different conclusions. The majority of asset pricing factors (i) can be replicated; (ii) can be clustered into 13 themes, the majority of which are significant parts of the tangency portfolio; (iii) work out-of-sample in a new large data set covering 93 countries; and (iv) have evidence that is strengthened (not weakened) by the large number of observed factors.},
author = {Jensen, Theis Ingerslev and Kelly, Bryan and Pedersen, Lasse Heje},
doi = {10.1111/JOFI.13249;PAGEGROUP:STRING:PUBLICATION},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Jensen, Kelly, Pedersen - 2023 - Is There a Replication Crisis in Finance.pdf:pdf},
issn = {15406261},
journal = {Journal of Finance},
month = {oct},
number = {5},
pages = {2465--2518},
publisher = {John Wiley and Sons Inc},
title = {{Is There a Replication Crisis in Finance?}},
url = {/doi/pdf/10.1111/jofi.13249 https://onlinelibrary.wiley.com/doi/abs/10.1111/jofi.13249 https://onlinelibrary.wiley.com/doi/10.1111/jofi.13249},
volume = {78},
year = {2023}
}
@article{Jermann2020,
abstract = {Since October 2008, fixed rates for interest rate swaps with a 30-year maturity have been mostly below Treasury rates with the same maturity. Under standard assumptions, this implies the existence of arbitrage opportunities. This paper presents a model for pricing interest rate swaps, where frictions for holding bonds limit arbitrage. I analytically show that negative swap spreads should not be surprising. In the calibrated model, swap spreads can reasonably match empirical counterparts without the need for large demand imbalances in the swap market. Empirical evidence is consistent with the relation between term spreads and swap spreads in the model. Received April 16, 2017; editorial decision Januray 3, 2019 by Editor Stijn Van Nieuwerburgh.},
author = {Jermann, Urban J.},
doi = {10.1093/RFS/HHZ030},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Jermann - 2020 - Negative Swap Spreads and Limited Arbitrage(2).pdf:pdf},
issn = {0893-9454},
journal = {The Review of Financial Studies},
month = {jan},
number = {1},
pages = {212--238},
publisher = {Oxford Academic},
title = {{Negative Swap Spreads and Limited Arbitrage}},
url = {https://dx.doi.org/10.1093/rfs/hhz030},
volume = {33},
year = {2020}
}
@article{Jorion2019,
abstract = {Researchers have long known about backfill bias in hedge fund databases. The most common treatments include either retaining all backfilled returns or truncating a fixed number of returns from each return series. However, we show that truncation largely preserves backfilled returns and document that either of these backfill treatments can lead to biased empirical findings, including cross-sectional results. Thus, our findings show that the best practice for empirical tests is to remove returns prior to the listing date. Because most databases do not have listing dates, we propose a novel method to infer unavailable listing dates.},
author = {Jorion, Philippe and Schwarz, Christopher},
doi = {10.1093/rfs/hhz024},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Jorion, Schwarz - 2019 - The Fix Is In Properly Backing out Backfill Bias.pdf:pdf},
issn = {0893-9454},
journal = {The Review of Financial Studies},
month = {dec},
number = {12},
pages = {5048--5099},
publisher = {Oxford Academic},
title = {{The Fix Is In: Properly Backing out Backfill Bias}},
url = {https://dx.doi.org/10.1093/rfs/hhz024 https://academic.oup.com/rfs/article/32/12/5048/5345572},
volume = {32},
year = {2019}
}
@article{Kelly2023,
abstract = {Financial Machine Learning},
author = {Kelly, Bryan and Xiu, Dacheng},
doi = {10.1561/0500000064},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Kelly, Xiu - 2023 - Financial Machine Learning(2).pdf:pdf},
isbn = {9781680839685},
issn = {1567-2395},
journal = {Foundations and Trends{\textregistered} in Finance},
keywords = {Asset Pricing,Data Mining,Deep Learning,Dimension Reduction,Econometrics,Finance,Financial Econometrics,Financial Markets,Machine Learning,Panel Data,Text Mining,Time Series Analysis},
month = {nov},
number = {3-4},
pages = {205--363},
publisher = {Now Publishers, Inc.},
title = {{Financial Machine Learning}},
url = {http://dx.doi.org/10.1561/0500000064},
volume = {13},
year = {2023}
}
@misc{Lee2023,
author = {Lee, Justina},
booktitle = {Bloomberg},
month = {dec},
title = {{A Grad-School Number-Cruncher Shakes Up the World of Bond Quants}},
year = {2023}
}
@article{Lettau2014,
abstract = {The downside risk capital asset pricing model (DR-CAPM) can price the cross section of currency returns. The market-beta differential between high and low interest rate currencies is higher conditional on bad market returns, when the market price of risk is also high, than it is conditional on good market returns. Correctly accounting for this variation is crucial for the empirical performance of the model. The DR-CAPM can jointly rationalize the cross section of equity, equity index options, commodity, sovereign bond and currency returns, thus offering a unified risk view of these asset classes. In contrast, popular models that have been developed for a specific asset class fail to jointly price other asset classes.},
author = {Lettau, Martin and Maggiori, Matteo and Weber, Michael},
doi = {10.1016/J.JFINECO.2014.07.001},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Lettau, Maggiori, Weber - 2014 - Conditional risk premia in currency markets and other asset classes(2).pdf:pdf},
issn = {0304-405X},
journal = {Journal of Financial Economics},
keywords = {Carry trade,Commodity basis,Downside risk,Equity cross section},
month = {nov},
number = {2},
pages = {197--225},
publisher = {North-Holland},
title = {{Conditional risk premia in currency markets and other asset classes}},
url = {https://www.sciencedirect.com/science/article/pii/S0304405X14001378?casa_token=I86SZLZWcW0AAAAA:hz3692U1muFtgjDPyxGqiYogHmZmE-_E_Bq6qx8n8TqNK4-VSaFSZ2r_QGYp28WOXRIOPxb7QQ},
volume = {114},
year = {2014}
}
@article{Lim2021,
abstract = {Multi-horizon forecasting often contains a complex mix of inputs – including static (i.e. time-invariant) covariates, known future inputs, and other exogenous time series that are only observed in the past – without any prior information on how they interact with the target. Several deep learning methods have been proposed, but they are typically ‘black-box' models that do not shed light on how they use the full range of inputs present in practical scenarios. In this paper, we introduce the Temporal Fusion Transformer (TFT) – a novel attention-based architecture that combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics. To learn temporal relationships at different scales, TFT uses recurrent layers for local processing and interpretable self-attention layers for long-term dependencies. TFT utilizes specialized components to select relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of scenarios. On a variety of real-world datasets, we demonstrate significant performance improvements over existing benchmarks, and highlight three practical interpretability use cases of TFT.},
archivePrefix = {arXiv},
arxivId = {1912.09363},
author = {Lim, Bryan and Arık, Sercan and Loeff, Nicolas and Pfister, Tomas},
doi = {10.1016/j.ijforecast.2021.03.012},
eprint = {1912.09363},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Lim et al. - 2021 - Temporal Fusion Transformers for interpretable multi-horizon time series forecasting.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Attention mechanisms,Deep learning,Explainable AI,Interpretability,Multi-horizon forecasting,Time series},
month = {oct},
number = {4},
pages = {1748--1764},
publisher = {Elsevier B.V.},
title = {{Temporal Fusion Transformers for interpretable multi-horizon time series forecasting}},
url = {https://arxiv.org/pdf/1912.09363},
volume = {37},
year = {2021}
}
@article{Makridakis1982,
abstract = {In the last few decades many methods have become available for forecasting. As always, when alternatives exist, choices need to be made so that an appropriate forecasting method can be selected and used for the specific situation being considered. This paper reports the results of a forecasting competition that provides information to facilitate such choice. Seven experts in each of the 24 methods forecasted up to 1001 series for six up to eighteen time horizons. The results of the competition are presented in this paper whose purpose is to provide empirical evidence about differences found to exist among the various extrapolative (time series) methods used in the competition. Copyright {\textcopyright} 1982 John Wiley & Sons, Ltd.},
author = {Makridakis, S. and Andersen, A. and Carbone, R. and Fildes, R. and Hibon, M. and Lewandowski, R. and Newton, J. and Parzen, E. and Winkler, R.},
doi = {10.1002/FOR.3980010202;REQUESTEDJOURNAL:JOURNAL:1099131X;WGROUP:STRING:PUBLICATION},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Makridakis et al. - 1982 - The accuracy of extrapolation (time series) methods Results of a forecasting competition.pdf:pdf},
issn = {1099131X},
journal = {Journal of Forecasting},
keywords = {Accuracy,Comparison,Empirical study,Evaluation,Forecasting,Time series},
month = {apr},
number = {2},
pages = {111--153},
publisher = {John Wiley & Sons, Ltd},
title = {{The accuracy of extrapolation (time series) methods: Results of a forecasting competition}},
url = {/doi/pdf/10.1002/for.3980010202 https://onlinelibrary.wiley.com/doi/abs/10.1002/for.3980010202 https://onlinelibrary.wiley.com/doi/10.1002/for.3980010202},
volume = {1},
year = {1982}
}
@article{Makridakis2000,
abstract = {This paper describes the M3-competition, the latest of the M-competitions. It explains the reasons for conducting the competition and summarizes its results and conclusions. In addition, the paper compares such results/conclusions with those of the previous two M-competitions as well as with those of other major empirical studies. Finally, the implications of these results and conclusions are considered, their consequences for both the theory and practice of forecasting are explored and directions for future research are contemplated. {\textcopyright} 2000 International Institute of Forecasters. Published by Elsevier Science B.V. All rights reserved.},
author = {Makridakis, Spyros and Hibon, Mich{\`{e}}le},
doi = {10.1016/S0169-2070(00)00057-1},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Makridakis, Hibon - 2000 - The M3-Competition results, conclusions and implications(2).pdf:pdf},
issn = {0169-2070},
journal = {International Journal of Forecasting},
keywords = {Comparative methods - Time series: Univariate,Forecasting accuracy,Forecasting competitions,Forecasting methods,M-competition},
month = {oct},
number = {4},
pages = {451--476},
publisher = {Elsevier},
title = {{The M3-Competition: results, conclusions and implications}},
url = {https://www.sciencedirect.com/science/article/pii/S0169207000000571?casa_token=3iX8BPQxXyIAAAAA:EdHotyUmJoNWxS5icXcYaDOTCfQs263PotOrIaRjxi8RKwiVcj5rZFYwzFUyFu3KxL603MksAQ},
volume = {16},
year = {2000}
}
@article{Makridakis2022,
abstract = {In this study, we present the results of the M5 “Accuracy” competition, which was the first of two parallel challenges in the latest M competition with the aim of advancing the theory and practice of forecasting. The main objective in the M5 “Accuracy” competition was to accurately predict 42,840 time series representing the hierarchical unit sales for the largest retail company in the world by revenue, Walmart. The competition required the submission of 30,490 point forecasts for the lowest cross-sectional aggregation level of the data, which could then be summed up accordingly to estimate forecasts for the remaining upward levels. We provide details of the implementation of the M5 “Accuracy” challenge, as well as the results and best performing methods, and summarize the major findings and conclusions. Finally, we discuss the implications of these findings and suggest directions for future research.},
author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
doi = {10.1016/J.IJFORECAST.2021.11.013},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Makridakis, Spiliotis, Assimakopoulos - 2022 - M5 accuracy competition Results, findings, and conclusions.pdf:pdf},
issn = {0169-2070},
journal = {International Journal of Forecasting},
keywords = {Accuracy,Forecasting competitions,M competitions,Machine learning,Retail sales forecasting,Time series},
month = {oct},
number = {4},
pages = {1346--1364},
publisher = {Elsevier},
title = {{M5 accuracy competition: Results, findings, and conclusions}},
url = {https://www.sciencedirect.com/science/article/pii/S0169207021001874},
volume = {38},
year = {2022}
}
@article{Makridakis2020,
abstract = {The M4 Competition follows on from the three previous M competitions, the purpose of which was to learn from empirical evidence both how to improve the forecasting accuracy and how such learning could be used to advance the theory and practice of forecasting. The aim of M4 was to replicate and extend the three previous competitions by: (a) significantly increasing the number of series, (b) expanding the number of forecasting methods, and (c) including prediction intervals in the evaluation process as well as point forecasts. This paper covers all aspects of M4 in detail, including its organization and running, the presentation of its results, the top-performing methods overall and by categories, its major findings and their implications, and the computational requirements of the various methods. Finally, it summarizes its main conclusions and states the expectation that its series will become a testing ground for the evaluation of new methods and the improvement of the practice of forecasting, while also suggesting some ways forward for the field.},
author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
doi = {10.1016/J.IJFORECAST.2019.04.014},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Makridakis, Spiliotis, Assimakopoulos - 2020 - The M4 Competition 100,000 time series and 61 forecasting methods.pdf:pdf},
issn = {0169-2070},
journal = {International Journal of Forecasting},
keywords = {Benchmarking methods,Forecasting accuracy,Forecasting competitions,M competitions,Machine learning methods,Practice of forecasting,Prediction intervals,Time series methods},
month = {jan},
number = {1},
pages = {54--74},
publisher = {Elsevier},
title = {{The M4 Competition: 100,000 time series and 61 forecasting methods}},
url = {https://www.sciencedirect.com/science/article/pii/S0169207019301128},
volume = {36},
year = {2020}
}
@article{Makridakis2018,
abstract = {The M4 competition is the continuation of three previous competitions started more than 45 years ago whose purpose was to learn how to improve forecasting accuracy, and how such learning can be applied to advance the theory and practice of forecasting. The purpose of M4 was to replicate the results of the previous ones and extend them into three directions: First significantly increase the number of series, second include Machine Learning (ML) forecasting methods, and third evaluate both point forecasts and prediction intervals. The five major findings of the M4 Competitions are: 1. Out Of the 17 most accurate methods, 12 were “combinations” of mostly statistical approaches. 2. The biggest surprise was a “hybrid” approach that utilized both statistical and ML features. This method's average sMAPE was close to 10% more accurate than the combination benchmark used to compare the submitted methods. 3. The second most accurate method was a combination of seven statistical methods and one ML one, with the weights for the averaging being calculated by a ML algorithm that was trained to minimize the forecasting. 4. The two most accurate methods also achieved an amazing success in specifying the 95% prediction intervals correctly. 5. The six pure ML methods performed poorly, with none of them being more accurate than the combination benchmark and only one being more accurate than Na{\"{i}}ve2. This paper presents some initial results of M4, its major findings and a logical conclusion. Finally, it outlines what the authors consider to be the way forward for the field of forecasting.},
author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
doi = {10.1016/J.IJFORECAST.2018.06.001},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Makridakis, Spiliotis, Assimakopoulos - 2018 - The M4 Competition Results, findings, conclusion and way forward(2).pdf:pdf},
issn = {0169-2070},
journal = {International Journal of Forecasting},
keywords = {Benchmarking methods,Forecasting accuracy,Forecasting competitions,M Competitions,Machine Learning (ML) methods,Practice of forecasting,Prediction intervals (PIs),Time series methods},
month = {oct},
number = {4},
pages = {802--808},
publisher = {Elsevier},
title = {{The M4 Competition: Results, findings, conclusion and way forward}},
url = {https://www.sciencedirect.com/science/article/pii/S0169207018300785?casa_token=AHcUuJMWRikAAAAA:ErZGK5_I8pED65iiRY4ZKiFxXVTgxwMkEzUy_G40h3ly0ZBVvGTcffOmGKb9sSzlFlJpWAWcDA},
volume = {34},
year = {2018}
}
@article{McCracken2016,
abstract = {This article describes a large, monthly frequency, macroeconomic database with the goal of establishing a convenient starting point for empirical analysis that requires “big data.” The dataset mimics the coverage of those already used in the literature but has three appealing features. First, it is designed to be updated monthly using the Federal Reserve Economic Data (FRED) database. Second, it will be publicly accessible, facilitating comparison of related research and replication of empirical work. Third, it will relieve researchers from having to manage data changes and revisions. We show that factors extracted from our dataset share the same predictive content as those based on various vintages of the so-called Stock–Watson dataset. In addition, we suggest that diffusion indexes constructed as the partial sum of the factor estimates can potentially be useful for the study of business cycle chronology. Supplementary materials for this article are available online.},
author = {McCracken, Michael W. and Ng, Serena},
doi = {10.1080/07350015.2015.1086655;WGROUP:STRING:PUBLICATION},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/McCracken, Ng - 2016 - FRED-MD A Monthly Database for Macroeconomic Research.pdf:pdf},
issn = {15372707},
journal = {Journal of Business and Economic Statistics},
keywords = {Big data,Diffusion index,Factors,Forecasting},
month = {oct},
number = {4},
pages = {574--589},
publisher = {American Statistical Association},
title = {{FRED-MD: A Monthly Database for Macroeconomic Research}},
url = {https://scholar.google.com/scholar_url?url=https://www.tandfonline.com/doi/pdf/10.1080/07350015.2015.1086655%3Fcasa_token%3DR5jztBNuYKEAAAAA:DFcxxG9HbGXKxUvohi4subiwnuX9axhyG1a6vvvHw5wE3vfBZrtL9opGRUr3t2M4DSkCbgeESRvkyg&hl=en&sa=T&oi=ucasa&ct=ucasa&ei=O2t},
volume = {34},
year = {2016}
}
@article{Menkhoff2012,
abstract = {We investigate the relation between global foreign exchange (FX) volatility risk and the cross section of excess returns arising from popular strategies that borrow in low interest rate currencies and invest in high interest rate currencies, so-called "carry trades." We find that high interest rate currencies are negatively related to innovations in global FX volatility, and thus deliver low returns in times of unexpected high volatility, when low interest rate currencies provide a hedge by yielding positive returns. Furthermore, we show that volatility risk dominates liquidity risk and our volatility risk proxy also performs well for pricing returns of other portfolios. {\textcopyright} 2012 the American Finance Association.},
author = {Menkhoff, Lukas and Sarno, Lucio and Schmeling, Maik and Schrimpf, Andreas},
doi = {10.1111/J.1540-6261.2012.01728.X;PAGE:STRING:ARTICLE/CHAPTER},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Menkhoff et al. - 2012 - Carry Trades and Global Foreign Exchange Volatility.pdf:pdf},
issn = {00221082},
journal = {Journal of Finance},
month = {apr},
number = {2},
pages = {681--718},
publisher = {John Wiley & Sons, Ltd},
title = {{Carry Trades and Global Foreign Exchange Volatility}},
url = {/doi/pdf/10.1111/j.1540-6261.2012.01728.x https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.2012.01728.x https://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.2012.01728.x},
volume = {67},
year = {2012}
}
@article{Nie2022,
abstract = {We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning performance, which outperforms supervised training on large datasets. Transferring of masked pre-trained representation on one dataset to others also produces SOTA forecasting accuracy. Code is available at: https://github.com/yuqinie98/PatchTST.},
archivePrefix = {arXiv},
arxivId = {2211.14730},
author = {Nie, Yuqi and Nguyen, Nam H. and Sinthong, Phanwadee and Kalagnanam, Jayant},
eprint = {2211.14730},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Nie et al. - 2022 - A Time Series is Worth 64 Words Long-term Forecasting with Transformers.pdf:pdf},
journal = {11th International Conference on Learning Representations, ICLR 2023},
month = {nov},
publisher = {International Conference on Learning Representations, ICLR},
title = {{A Time Series is Worth 64 Words: Long-term Forecasting with Transformers}},
url = {https://arxiv.org/pdf/2211.14730},
year = {2022}
}
@article{Nozawa2017,
abstract = {I decompose the variation of credit spreads for corporate bonds into changing expected returns and changing expectation of credit losses. Using a log-linearized pricing identity and a vector autoregression applied to microlevel data from 1973 to 2011, I find that expected returns contribute to the cross-sectional variance of credit spreads nearly as much as expected credit loss does. However, most of the time-series variation in credit spreads for the market portfolio corresponds to risk premiums.},
author = {Nozawa, Yoshio},
doi = {10.1111/JOFI.12524;WGROUP:STRING:PUBLICATION},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Nozawa - 2017 - What Drives the Cross-Section of Credit Spreads A Variance Decomposition Approach.pdf:pdf},
issn = {15406261},
journal = {Journal of Finance},
month = {oct},
number = {5},
pages = {2045--2072},
publisher = {Blackwell Publishing Ltd},
title = {{What Drives the Cross-Section of Credit Spreads?: A Variance Decomposition Approach}},
url = {/doi/pdf/10.1111/jofi.12524 https://onlinelibrary.wiley.com/doi/abs/10.1111/jofi.12524 https://onlinelibrary.wiley.com/doi/10.1111/jofi.12524},
volume = {72},
year = {2017}
}
@article{Oord2016,
abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
archivePrefix = {arXiv},
arxivId = {1609.03499},
author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
eprint = {1609.03499},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:pdf},
month = {sep},
title = {{WaveNet: A Generative Model for Raw Audio}},
url = {https://arxiv.org/pdf/1609.03499 http://arxiv.org/abs/1609.03499},
year = {2016}
}
@inproceedings{Oreshkin2020,
abstract = {We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.},
author = {Oreshkin, Boris N and Carpov, Dmitri and Chapados, Nicolas and {Bengio Mila}, Yoshua},
booktitle = {Eighth International Conference On Learning Representations},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Oreshkin et al. - 2020 - N-BEATS Neural basis expansion analysis for interpretable time series forecasting.pdf:pdf},
title = {{N-BEATS: Neural basis expansion analysis for interpretable time series forecasting}},
url = {https://iclr.cc/virtual_2020/poster_r1ecqn4YwB.html},
year = {2020}
}
@article{Orphanides2001,
abstract = {This paper examines the magnitude of informational problems associated with the implementation and interpretation of simple monetary policy rules. Using Taylor's rule as an example, I demonstrate that real-time policy recommendations differ considerably from those obtained with ex post revised data. Further, estimated policy reaction functions based on ex post revised data provide misleading descriptions of historical policy and obscure the behavior suggested by information available to the Federal Reserve in real time. These results indicate that reliance on the information actually available to policy makers in real time is essential for the analysis of monetary policy rules.},
author = {Orphanides, Athanasios},
doi = {10.1257/aer.91.4.964},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Orphanides - 2001 - Monetary policy rules based on real-time data.pdf:pdf},
issn = {00028282},
journal = {American Economic Review},
keywords = {Business Fluctuations,Central Banking,Cycles,Monetary Policy,and the Supply of Money and Credit: General},
month = {sep},
number = {4},
pages = {964--985},
publisher = {American Economic Association},
title = {{Monetary policy rules based on real-time data}},
url = {https://pubs.aeaweb.org/doi/10.1257/aer.91.4.964},
volume = {91},
year = {2001}
}
@article{Palhares2012,
author = {Palhares, D},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Palhares - 2012 - Cash-flow maturity and risk premia in cds markets.pdf:pdf},
journal = {Unpublished working paper},
title = {{Cash-flow maturity and risk premia in cds markets}},
url = {http://faculty.chicagobooth.edu/workshops/finance/past/pdf/paperpalharesnov2.pdf},
year = {2012}
}
@article{Prater2024,
abstract = {Optimizing the time-series forecasting performance is a multi-objective problem which enables the comparison of general applicability of methods across multiple use cases such as finance and demographics. Libra, a time-series forecasting framework which shifts the problem of optimization from minimizing single to multiple evaluation measures and use cases, is used as a benchmark to evaluate the performance of the Long Short-Term Memory (LSTM) neural network. LSTMs with parameter tuning have been shown to perform well with time-series forecasting. This paper applies LSTMs (mostly with standard parameters and variations of some of them) to the Libra framework and concludes that due to data characteristic variance and without increased hardware and time constraints LSTMs do not outperform the median measures of Libra.},
author = {Prater, Ryan and Hanne, Thomas and Dornberger, Rolf},
doi = {10.1080/08839514.2024.2377510;WEBSITE:WEBSITE:TFOPB;PAGEGROUP:STRING:PUBLICATION},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Prater, Hanne, Dornberger - 2024 - Generalized Performance of LSTM in Time-Series Forecasting.pdf:pdf},
issn = {10876545},
journal = {Applied Artificial Intelligence},
month = {dec},
number = {1},
publisher = {Taylor and Francis Ltd.},
title = {{Generalized Performance of LSTM in Time-Series Forecasting}},
url = {https://www.tandfonline.com/doi/pdf/10.1080/08839514.2024.2377510},
volume = {38},
year = {2024}
}
@article{Prokhorenkova,
abstract = {This paper presents the key algorithmic techniques behind CatBoost, a new gradient boosting toolkit. Their combination leads to CatBoost outperforming other publicly available boosting implementations in terms of quality on a variety of datasets. Two critical algorithmic advances introduced in CatBoost are the implementation of ordered boosting, a permutation-driven alternative to the classic algorithm, and an innovative algorithm for processing categorical features. Both techniques were created to fight a prediction shift caused by a special kind of target leakage present in all currently existing implementations of gradient boosting algorithms. In this paper, we provide a detailed analysis of this problem and demonstrate that proposed algorithms solve it effectively, leading to excellent empirical results.},
author = {Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
doi = {10.5555/3327757.3327770},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Prokhorenkova et al. - 2018 - Catboost Unbiased boosting with categorical features.pdf:pdf},
title = {{CatBoost: unbiased boosting with categorical features}},
url = {https://dl.acm.org/doi/pdf/10.5555/3327757.3327770},
year = {2018}
}
@article{Qiu2024,
abstract = {Time series are generated in diverse domains such as economic, traffic, health, and energy, where forecasting of future values has numerous important applications. Not surprisingly, many forecasting methods are being proposed. To ensure progress, it is essential to be able to study and compare such methods empirically in a comprehensive and reliable manner. To achieve this, we propose TFB, an automated benchmark for Time Series Forecasting (TSF) methods. TFB advances the state-of-the-art by addressing shortcomings related to datasets, comparison methods, and evaluation pipelines: 1) insufficient coverage of data domains, 2) stereotype bias against traditional methods, and 3) inconsistent and inflexible pipelines. To achieve better domain coverage, we include datasets from 10 different domains: traffic, electricity, energy, the environment, nature, economic, stock markets, banking, health, and the web. We also provide a time series characterization to ensure that the selected datasets are comprehensive. To remove biases against some methods, we include a diverse range of methods, including statistical learning, machine learning, and deep learning methods, and we also support a variety of evaluation strategies and metrics to ensure a more comprehensive evaluations of different methods. To support the integration of different methods into the benchmark and enable fair comparisons, TFB features a flexible and scalable pipeline that eliminates biases. Next, we employ TFB to perform a thorough evaluation of 21 Univariate Time Series Forecasting (UTSF) methods on 8,068 univariate time series and 14 Multivariate Time Series Forecasting (MTSF) methods on 25 datasets. The benchmark code and data are available at https://github.com/decisionintelligence/TFB.},
archivePrefix = {arXiv},
arxivId = {2403.20150},
author = {Qiu, Xiangfei and Hu, Jilin and Zhou, Lekui and Wu, Xingjian and Du, Junyang and Zhang, Buang and Guo, Chenjuan and Zhou, Aoying and Jensen, Christian S. and Sheng, Zhenli and Yang, Bin},
doi = {10.14778/3665844.3665863},
eprint = {2403.20150},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Qiu et al. - 2024 - TFB Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods.pdf:pdf},
isbn = {004/1/12019/1},
issn = {21508097},
journal = {Proceedings of the VLDB Endowment},
month = {mar},
number = {9},
pages = {2363--2377},
publisher = {VLDB Endowment},
title = {{TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods}},
url = {https://arxiv.org/pdf/2403.20150},
volume = {17},
year = {2024}
}
@article{Reinhart2010,
abstract = {In this paper, we exploit a new multi-country historical dataset on public (government) debt to search for a systemic relationship between high public debt levels, growth and inflation. 1 Our main result is that whereas the link between growth and debt seems relatively weak at "nor-mal" debt levels, median growth rates for countries with public debt over roughly 90 percent of GDP are about one percent lower than otherwise ; average (mean) growth rates are several percent lower. Surprisingly, the relationship between public debt and growth is remarkably similar across emerging markets and advanced economies. This is not the case for inflation. We find no systematic relationship between high debt levels and inflation for advanced economies as a group (albeit with individual country exceptions including the United States). By contrast , in emerging market countries, high public debt levels coincide with higher inflation. Our topic would seem to be a timely one. Public debt has been soaring in the wake of the recent global financial maelstrom, especially in the epicenter countries. This should not be surprising , given the experience of earlier severe financial crises. 2 Outsized deficits and epic bank bailouts may be useful in fighting a downturn, but what is the long-run macroeconomic impact, 1 In this paper "public debt" refers to gross central government debt. "Domestic public debt" is government debt issued under domestic legal jurisdiction. Public debt does not include debts carrying a government guarantee. Total gross external debt includes the external debts of all branches of government as well as private debt that is issued by domestic private entities under a foreign jurisdiction. 2 Reinhart and Rogoff (2009a, b) demonstrate that the aftermath of a deep financial crisis typically involves a protracted period of macroeconomic adjustment, particularly in employment and housing prices. On average, public debt rose by more than 80 percent within three years after a crisis. especially against the backdrop of graying populations and rising social insurance costs? Are sharply elevated public debts ultimately a manageable policy challenge? Our approach here is decidedly empirical, taking advantage of a broad new historical dataset on public debt (in particular, central government debt) first presented in Carmen M. Reinhart and Kenneth S. Rogoff (2008, 2009b). Prior to this dataset, it was exceedingly difficult to get more than two or three decades of public debt data even for many rich countries, and virtually impossible for most emerging markets. Our results incorporate data on 44 countries spanning about 200 years. Taken together, the data incorporate over 3,700 annual observations covering a wide range of political systems, institutions , exchange rate and monetary arrangements , and historic circumstances. We also employ more recent data on external debt, including debt owed both by governments and by private entities. For emerging markets, we find that there exists a significantly more severe threshold for total gross external debt (public and private)-which is almost exclusively denominated in a foreign currency-than for total public debt (the domestically issued component of which is largely denominated in home currency). When gross external debt reaches 60 percent of GDP, annual growth declines by about two percent; for levels of external debt in excess of 90 percent of GDP, growth rates are roughly cut in half. We are not in a position to calculate separate total external debt thresholds (as opposed to public debt thresholds) for advanced countries. The available time-series is too recent, beginning only in 2000. We do note, however, that external debt levels in advanced countries now average nearly 200 percent of GDP, with external debt levels being particularly high across Europe. The focus of this paper is on the longer term macroeconomic implications of much higher public and external debt. The final section, however , summarizes the historical experience of the United States in dealing with private sector},
author = {Reinhart, Carmen M. and Rogoff, Kenneth S.},
doi = {10.1257/aer.100.2.573},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Reinhart, Rogoff - 2010 - Growth in a Time of Debt.pdf:pdf},
issn = {0002-8282},
journal = {American Economic Review},
month = {may},
number = {2},
pages = {573--578},
title = {{Growth in a Time of Debt}},
url = {https://pubs.aeaweb.org/doi/pdf/10.1257/aer.100.2.573 https://pubs.aeaweb.org/doi/10.1257/aer.100.2.573},
volume = {100},
year = {2010}
}
@article{Rime2022,
abstract = {To understand deviations from covered interest parity (CIP), it is crucial to account for heterogeneity in funding costs across both banks and currency areas. For most market participants, the no-arbitrage relation holds fairly well when implemented using marginal funding costs and risk-free investment instruments. However, a few high-rated banks do enjoy CIP-arbitrage opportunities. Dealers avert inventory imbalances stemming from lower-rated banks' usage of FX swaps to obtain dollar funding by inducing opposite (arbitrage) flows from high-rated banks. Arbitrage trades are difficult to scale, however, because funding costs increase as soon as arbitrageurs increase positions.},
author = {Rime, Dagfinn and Schrimpf, Andreas and Syrstad, Olav},
doi = {10.1093/RFS/HHAC026},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Rime, Schrimpf, Syrstad - 2022 - Covered Interest Parity Arbitrage(2).pdf:pdf},
issn = {0893-9454},
journal = {The Review of Financial Studies},
month = {oct},
number = {11},
pages = {5185--5227},
publisher = {Oxford Academic},
title = {{Covered Interest Parity Arbitrage}},
url = {https://dx.doi.org/10.1093/rfs/hhac026},
volume = {35},
year = {2022}
}
@article{Ronn1989,
abstract = {This paper develops and tests arbitrage bounds for a combination of two option spread positions known as a box spread. This strategy involves the simultaneous use of four options and creates a position that is equivalent to riskless lending. The no-arbitrage conditions are compared to existing arbitrage bounds and are tested using Chicago Board Options Exchange data. Article published by Oxford University Press on behalf of the Society for Financial Studies in its journal, The Review of Financial Studies.},
author = {Ronn, Aimee Gerbarg and Ronn, Ehud I and Ronn, Aimee Gerbarg and Ronn, Ehud I},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Ronn et al. - 1989 - The Box Spread Arbitrage Conditions Theory, Tests, and Investment Strategies.pdf:pdf},
issn = {0893-9454},
journal = {The Review of Financial Studies},
number = {1},
pages = {91--108},
publisher = {Society for Financial Studies},
title = {{The Box Spread Arbitrage Conditions: Theory, Tests, and Investment Strategies}},
url = {https://econpapers.repec.org/RePEc:oup:rfinst:v:2:y:1989:i:1:p:91-108},
volume = {2},
year = {1989}
}
@article{Salinas2020,
abstract = {Probabilistic forecasting, i.e., estimating a time series' future probability distribution given its past, is a key enabler for optimizing business processes. In retail businesses, for example, probabilistic demand forecasts are crucial for having the right inventory available at the right time and in the right place. This paper proposes DeepAR, a methodology for producing accurate probabilistic forecasts, based on training an autoregressive recurrent neural network model on a large number of related time series. We demonstrate how the application of deep learning techniques to forecasting can overcome many of the challenges that are faced by widely-used classical approaches to the problem. By means of extensive empirical evaluations on several real-world forecasting datasets, we show that our methodology produces more accurate forecasts than other state-of-the-art methods, while requiring minimal manual work.},
archivePrefix = {arXiv},
arxivId = {1704.04110},
author = {Salinas, David and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim},
doi = {10.1016/J.IJFORECAST.2019.07.001},
eprint = {1704.04110},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Salinas et al. - 2020 - DeepAR Probabilistic forecasting with autoregressive recurrent networks.pdf:pdf},
issn = {0169-2070},
journal = {International Journal of Forecasting},
keywords = {Big data,Deep learning,Demand forecasting,Neural networks,Probabilistic forecasting},
month = {jul},
number = {3},
pages = {1181--1191},
publisher = {Elsevier},
title = {{DeepAR: Probabilistic forecasting with autoregressive recurrent networks}},
url = {https://www.sciencedirect.com/science/article/pii/S0169207019301888?via%3Dihub},
volume = {36},
year = {2020}
}
@article{Siriwardane2021,
abstract = {We use arbitrage activity in equity, fixed income, and foreign exchange markets to characterize the frictions and constraints facing intermediaries. The average pairwise correlation between the twenty-nine arbitrage spreads that we study is 22%. These low correlations are inconsistent with models in which an integrated intermediary sector faces a single constraint and sets all prices. We show that at least two types of segmentation drive arbitrage dynamics. First, funding is segmented-certain trades rely on specific funding sources so arbitrage spreads are sensitive to localized funding shocks. Second, balance sheets are segmented-intermediaries specialize in certain arbitrages so arbitrage spreads are sensitive to idiosyncratic balance sheet shocks. Our results suggest specialization on both the asset and liability sides of intermediary balance sheets is important for understanding their role in capital markets.},
author = {Siriwardane, Emil and Sunderam, Aditya and Wallen, Jonathan},
doi = {10.2139/SSRN.3960980},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Siriwardane, Sunderam, Wallen - 2021 - Segmented Arbitrage(3).pdf:pdf;:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Siriwardane, Sunderam, Wallen - 2021 - Segmented Arbitrage(2).pdf:pdf},
journal = {Available at SSRN},
keywords = {Aditya Sunderam,Emil Siriwardane,Jonathan Wallen,SSRN,Segmented Arbitrage,arbitrage,intermediary-based asset pricing,segmentation},
month = {nov},
publisher = {Elsevier BV},
title = {{Segmented Arbitrage}},
url = {https://papers.ssrn.com/abstract=3960980},
volume = {3960980},
year = {2021}
}
@article{Sullivan1999,
abstract = {In this paper we utilize White's Reality Check bootstrap methodology (White (1999)) to evaluate simple technical trading rules while quantifying the data‐snooping bias and fully adjusting for its effect in the context of the full universe from which the trading rules were drawn. Hence, for the first time, the paper presents a comprehensive test of performance across all technical trading rules examined. We consider the study of Brock, Lakonishok, and LeBaron (1992), expand their universe of 26 trading rules, apply the rules to 100 years of daily data on the Dow Jones Industrial Average, and determine the effects of data‐snooping.},
author = {Sullivan, Ryan and Timmermann, Allan and White, Halbert},
doi = {10.1111/0022-1082.00163},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Sullivan, Timmermann, White - 1999 - Data‐Snooping, Technical Trading Rule Performance, and the Bootstrap.pdf:pdf},
issn = {0022-1082},
journal = {The Journal of Finance},
month = {oct},
number = {5},
pages = {1647--1691},
publisher = {John Wiley & Sons, Ltd},
title = {{Data‐Snooping, Technical Trading Rule Performance, and the Bootstrap}},
url = {/doi/pdf/10.1111/0022-1082.00163 https://onlinelibrary.wiley.com/doi/abs/10.1111/0022-1082.00163 https://onlinelibrary.wiley.com/doi/10.1111/0022-1082.00163},
volume = {54},
year = {1999}
}
@article{Summers1991,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. The Penn World Table displays a set of national accounts economic time series covering many countries. Its expenditure entries are denominated in a common set of prices in a common currency so that real quantity comparisons can be made, both between countries and over time. It also provides information about relative prices within and between countries, as well as demographic data and capital stock estimates. This updated, revised, and expanded Mark 5 version of the table includes more countries, years, and variables of interest to economic researchers. The Table is available on personal computer diskettes and through BITNET.},
author = {Summers, Robert and Heston, Alan},
doi = {10.2307/2937941},
issn = {00335533},
journal = {The Quarterly Journal of Economics},
month = {may},
number = {2},
pages = {327},
publisher = {Oxford Academic},
title = {{The Penn World Table (Mark 5): An Expanded Set of International Comparisons, 1950-1988}},
url = {https://dx.doi.org/10.2307/2937941 https://academic.oup.com/qje/article-lookup/doi/10.2307/2937941},
volume = {106},
year = {1991}
}
@article{Tan2020,
abstract = {This paper studies Time Series Extrinsic Regression (TSER): a regression task of which the aim is to learn the relationship between a time series and a continuous scalar variable; a task closely related to time series classification (TSC), which aims to learn the relationship between a time series and a categorical class label. This task generalizes time series forecasting (TSF), relaxing the requirement that the value predicted be a future value of the input series or primarily depend on more recent values. In this paper, we motivate and study this task, and benchmark existing solutions and adaptations of TSC algorithms on a novel archive of 19 TSER datasets which we have assembled. Our results show that the state-of-the-art TSC algorithm Rocket, when adapted for regression, achieves the highest overall accuracy compared to adaptations of other TSC algorithms and state-of-the-art machine learning (ML) algorithms such as XGBoost, Random Forest and Support Vector Regression. More importantly, we show that much research is needed in this field to improve the accuracy of ML models. We also find evidence that further research has excellent prospects of improving upon these straightforward baselines.},
archivePrefix = {arXiv},
arxivId = {2006.12672},
author = {Tan, Chang Wei and Bergmeir, Christoph and Petitjean, Francois and Webb, Geoffrey I.},
doi = {10.1007/s10618-021-00745-9},
eprint = {2006.12672},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Tan et al. - 2020 - Time Series Extrinsic Regression.pdf:pdf},
issn = {1384-5810},
journal = {Data Mining and Knowledge Discovery},
month = {jun},
number = {3},
pages = {1032--1060},
publisher = {Springer Science and Business Media LLC},
title = {{Time Series Extrinsic Regression}},
url = {https://arxiv.org/pdf/2006.12672},
volume = {35},
year = {2020}
}
@article{Taylor2018,
abstract = {Forecasting is a common data science task that helps organizations with capacity planning, goal setting, and anomaly detection. Despite its importance, there are serious challenges associated with producing reliable and high-quality forecasts—especially when there are a variety of time series and analysts with expertise in time series modeling are relatively rare. To address these challenges, we describe a practical approach to forecasting “at scale” that combines configurable models with analyst-in-the-loop performance analysis. We propose a modular regression model with interpretable parameters that can be intuitively adjusted by analysts with domain knowledge about the time series. We describe performance analyses to compare and evaluate forecasting procedures, and automatically flag forecasts for manual review and adjustment. Tools that help analysts to use their expertise most effectively enable reliable, practical forecasting of business time series.},
author = {Taylor, Sean J. and Letham, Benjamin},
doi = {10.1080/00031305.2017.1380080},
issn = {15372731},
journal = {The American Statistician},
keywords = {Nonlinear regression,Statistical practice,Time series},
month = {jan},
number = {1},
pages = {37--45},
publisher = {American Statistical Association},
title = {{Forecasting at scale}},
url = {https://www.tandfonline.com/doi/abs/10.1080/00031305.2017.1380080?casa_token=hB5TwMQ_avsAAAAA:JMn1fmRcEzQNiblxcb7A6xM5Woi1AT8hWc6j1toYOmfFdTcT5OGZRbBD3EtGgT4PlinkzQFg8b4XCg},
volume = {72},
year = {2018}
}
@article{Trapero2015,
abstract = {Shorter product life cycles and aggressive marketing, among other factors, have increased the complexity of sales forecasting. Forecasts are often produced using a Forecasting Support System that integrates univariate statistical forecasting with managerial judgment. Forecasting sales under promotional activity is one of the main reasons to use expert judgment. Alternatively, one can replace expert adjustments by regression models whose exogenous inputs are promotion features (price, display, etc). However, these regression models may have large dimensionality as well as multicollinearity issues. We propose a novel promotional model that overcomes these limitations. It combines Principal Component Analysis to reduce the dimensionality of the problem and automatically identifies the demand dynamics. For items with limited history, the proposed model is capable of providing promotional forecasts by selectively pooling information across established products. The performance of the model is compared against forecasts provided by experts and statistical benchmarks, on weekly data; outperforming both substantially.},
author = {Trapero, Juan R. and Kourentzes, Nikolaos and Fildes, Robert},
doi = {10.1057/JORS.2013.174;WGROUP:STRING:PUBLICATION},
issn = {14769360},
journal = {Journal of the Operational Research Society},
keywords = {Promotional modelling,demand forecasting,judgmental adjustments,principal components analysis},
month = {feb},
number = {2},
pages = {299--307},
publisher = {Palgrave Macmillan Ltd.},
title = {{On the identification of sales forecasting models in the presence of promotions}},
url = {https://www.tandfonline.com/doi/abs/10.1057/jors.2013.174},
volume = {66},
year = {2015}
}
@article{VanBinsbergen2022,
abstract = {We estimate risk-free interest rates unaffected by convenience yields on safe assets. We infer them from risky asset prices without relying on any specific model of risk. We obtain interest rates and implied convenience yields with maturities up to three years at a minutely frequency. Our estimated convenience yield on Treasuries equals about 40 basis points, is larger below three months maturity, and quadruples during the financial crisis. In high-frequency event studies, conventional and unconventional monetary stimulus reduces our rates more than the corresponding Treasury yields, thus broadly affecting rates even outside the narrow confines of the fixed-income market.},
author = {van Binsbergen, Jules H. and Diamond, William F. and Grotteria, Marco},
doi = {10.1016/J.JFINECO.2021.06.012},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/van Binsbergen, Diamond, Grotteria - 2022 - Risk-free interest rates(2).pdf:pdf},
issn = {0304-405X},
journal = {Journal of Financial Economics},
keywords = {Convenience yield,Demand for safe assets,Monetary policy,Quantitative easing},
month = {jan},
number = {1},
pages = {1--29},
publisher = {North-Holland},
title = {{Risk-free interest rates}},
url = {https://www.sciencedirect.com/science/article/pii/S0304405X21002786?casa_token=R3v1kXleT3UAAAAA:kT4rgj1xYNfkR9yPsOPN8roJvDc9ouJ3IPe7hNCJVUjGjBQNA4if4o6xmO04G9u_CNSQTDqvnA},
volume = {143},
year = {2022}
}
@article{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
author = {Vaswani, Ashish and Brain, Google and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Vaswani et al. - 2017 - Attention is All you Need.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
title = {{Attention is All you Need}},
volume = {30},
year = {2017}
}
@article{Winters1960,
abstract = {The growing use of computers for mechanized inventory control and production planning has brought with it the need for explicit forecasts of sales and usage for individual products and materials. These forecasts must be made on a routine basis for thousands of products, so that they must be made quickly, and, both in terms of computing time and information storage, cheaply; they should be responsive to changing conditions. The paper presents a method of forecasting sales which has these desirable characteristics, and which in terms of ability to forecast compares favorably with other, more traditional methods. Several models of the exponential forecasting system are presented, along with several examples of application.},
author = {Winters, Peter R.},
doi = {10.1287/mnsc.6.3.324},
issn = {0025-1909},
journal = {Management Science},
number = {3},
pages = {324--342},
title = {{Forecasting Sales by Exponentially Weighted Moving Averages}},
url = {https://www.jstor.org/stable/2627346?saml_data=eyJpbnN0aXR1dGlvbklkcyI6WyI2YWQ1YjI5Yy0zMWI2LTQ1NTAtOTc1NS0yMjk4MGM3NGU3ZmMiXSwic2FtbFRva2VuIjoiMTliMTU1NzMtMGY4OC00YjBmLTkzOTQtNzAwNjI2YmZkZDQzIn0&seq=1},
volume = {6},
year = {1960}
}
@article{Wolpert1997,
abstract = {A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of "no free lunch" (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori "head-to-head" minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems' enforcing of a type of uniformity over all algorithms. {\textcopyright} 1997 IEEE.},
author = {Wolpert, David H. and Macready, William G.},
doi = {10.1109/4235.585893},
issn = {1089778X},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {Evolutionary algorithms,Information theory,Optimization},
number = {1},
pages = {67--82},
title = {{No free lunch theorems for optimization}},
volume = {1},
year = {1997}
}
@article{Wu2021,
abstract = {Extending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long-term energy consumption planning. This paper studies the long-term forecasting problem of time series. Prior Transformer-based models adopt various self-attention mechanisms to discover the long-range dependencies. However, intricate temporal patterns of the long-term future prohibit the model from finding reliable dependencies. Also, Transformers have to adopt the sparse versions of point-wise self-attentions for long series efficiency, resulting in the information utilization bottleneck. Going beyond Transformers, we design Autoformer as a novel decomposition architecture with an Auto-Correlation mechanism. We break with the pre-processing convention of series decomposition and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic process theory, we design the Auto-Correlation mechanism based on the series periodicity, which conducts the dependencies discovery and representation aggregation at the sub-series level. Auto-Correlation outperforms self-attention in both efficiency and accuracy. In long-term forecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative improvement on six benchmarks, covering five practical applications: energy, traffic, economics, weather and disease. Code is available at this repository: \url{https://github.com/thuml/Autoformer}.},
archivePrefix = {arXiv},
arxivId = {2106.13008},
author = {Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
eprint = {2106.13008},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Wu et al. - 2021 - Autoformer Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting.pdf:pdf},
isbn = {9781713845393},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
month = {jun},
pages = {22419--22430},
publisher = {Neural information processing systems foundation},
title = {{Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting}},
url = {https://arxiv.org/pdf/2106.13008},
volume = {27},
year = {2021}
}
@article{Xu2025,
abstract = {Pure time series forecasting tasks typically focus exclusively on numerical features; however, real-world financial decision-making demands the comparison and analysis of heterogeneous sources of information. Recent advances in deep learning and large scale language models (LLMs) have made significant strides in capturing sentiment and other qualitative signals, thereby enhancing the accuracy of financial time series predictions. Despite these advances, most existing datasets consist solely of price series and news text, are confined to a single market, and remain limited in scale. In this paper, we introduce FinMultiTime, the first large scale, multimodal financial time series dataset. FinMultiTime temporally aligns four distinct modalities financial news, structured financial tables, K-line technical charts, and stock price time series across both the S&P 500 and HS 300 universes. Covering 5,105 stocks from 2009 to 2025 in the United States and China, the dataset totals 112.6 GB and provides minute-level, daily, and quarterly resolutions, thus capturing short, medium, and long term market signals with high fidelity. Our experiments demonstrate that (1) scale and data quality markedly boost prediction accuracy; (2) multimodal fusion yields moderate gains in Transformer models; and (3) a fully reproducible pipeline enables seamless dataset updates.},
archivePrefix = {arXiv},
arxivId = {2506.05019},
author = {Xu, Wenyan and Xiang, Dawei and Liu, Yue and Wang, Xiyu and Ma, Yanxiang and Zhang, Liang and Xu, Chang and Zhang, Jiaheng},
eprint = {2506.05019},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Xu et al. - 2025 - FinMultiTime A Four-Modal Bilingual Dataset for Financial Time-Series Analysis.pdf:pdf},
isbn = {2506.05019v1},
keywords = {cs.CE},
month = {jun},
title = {{FinMultiTime: A Four-Modal Bilingual Dataset for Financial Time-Series Analysis}},
url = {https://arxiv.org/pdf/2506.05019},
year = {2025}
}
@article{Yang2013,
abstract = {I identify a "slope" factor in the cross section of commodity futures returns: high-basis commodity futures have higher loadings on this factor than low-basis commodity futures. Combined with a level factor (an index of commodity futures), this slope factor explains most of the average excess returns of commodity futures portfolios sorted by basis. More importantly, I find that this factor is significantly correlated with investment shocks, which represent the technological progress in producing new capital. I investigate a competitive dynamic equilibrium model of commodity production to endogenize this correlation. The model reproduces the cross-sectional futures returns and many asset pricing tests. {\textcopyright} 2013 Elsevier B.V.},
author = {Yang, Fan},
doi = {10.1016/J.JFINECO.2013.04.012},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Yang - 2013 - Investment shocks and the commodity basis spread(2).pdf:pdf},
issn = {0304-405X},
journal = {Journal of Financial Economics},
keywords = {Basis spread,Commodity futures,Investment shocks,Investment-based asset pricing},
month = {oct},
number = {1},
pages = {164--184},
publisher = {North-Holland},
title = {{Investment shocks and the commodity basis spread}},
url = {https://www.sciencedirect.com/science/article/pii/S0304405X13001360?casa_token=FYcuSZ3lD94AAAAA:BQBq-Ja8FB0JScYrNoQV_2LYSvB4pI_Tvdi5GGaf1juhcfNZ_033v8Hh-Cm-45UhSiL20fwoXw},
volume = {110},
year = {2013}
}
@article{Zeng2022,
abstract = {Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research in this work. Specifically, Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the \emph{permutation-invariant} self-attention mechanism inevitably results in temporal information loss. To validate our claim, we introduce a set of embarrassingly simple one-layer linear models named LTSF-Linear for comparison. Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based LTSF models in all cases, and often by a large margin. Moreover, we conduct comprehensive empirical studies to explore the impacts of various design elements of LTSF models on their temporal relation extraction capability. We hope this surprising finding opens up new research directions for the LTSF task. We also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future. Code is available at: \url{https://github.com/cure-lab/LTSF-Linear}.},
archivePrefix = {arXiv},
arxivId = {2205.13504},
author = {Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
doi = {10.1609/aaai.v37i9.26317},
eprint = {2205.13504},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Zeng et al. - 2022 - Are Transformers Effective for Time Series Forecasting.pdf:pdf},
isbn = {9781577358800},
issn = {2159-5399},
journal = {Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023},
month = {may},
pages = {11121--11128},
publisher = {AAAI Press},
title = {{Are Transformers Effective for Time Series Forecasting?}},
url = {https://arxiv.org/pdf/2205.13504},
volume = {37},
year = {2022}
}
@article{Zhao2006,
abstract = {Time series forecasting is an important area in data mining research. Feature preprocessing techniques have significant influence on forecasting accuracy, therefore are essential in a forecasting model. Although several feature preprocessing techniques have been applied in time series forecasting, there is so far no systematic research to study and compare their performance. How to select effective techniques of feature preprocessing in a forecasting model remains a problem. In this paper, the authors conduct a comprehensive study of existing feature preprocessing techniques to evaluate their empirical performance in time series forecasting. It is demonstrated in our experiment that, effective feature preprocessing can significantly enhance forecasting accuracy. This research can be a useful guidance for researchers on effectively selecting feature preprocessing techniques and integrating them with time series forecasting models. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
author = {Zhao, Jun Hua and Dong, Zhaoyang and Xu, Zhao},
doi = {10.1007/11811305_84},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Zhao, Dong, Xu - 2006 - Effective feature preprocessing for time series forecasting.pdf:pdf},
isbn = {3540370250},
issn = {16113349},
journal = {SpringerJH Zhao, ZY Dong, Z XuInternational conference on advanced data mining and applications, 2006•Springer},
pages = {769--781},
publisher = {Springer Verlag},
title = {{Effective feature preprocessing for time series forecasting}},
url = {https://link.springer.com/chapter/10.1007/11811305_84},
volume = {4093 LNAI},
year = {2006}
}
@article{Zhou2020,
abstract = {Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a $ProbSparse$ self-attention mechanism, which achieves $O(L \log L)$ in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem.},
archivePrefix = {arXiv},
arxivId = {2012.07436},
author = {Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
doi = {10.1609/aaai.v35i12.17325},
eprint = {2012.07436},
file = {:Users/jbejarano/Library/CloudStorage/Dropbox/my_mendeley/Zhou et al. - 2020 - Informer Beyond Efficient Transformer for Long Sequence Time-Series Forecasting.pdf:pdf},
isbn = {9781713835974},
issn = {2159-5399},
journal = {35th AAAI Conference on Artificial Intelligence, AAAI 2021},
month = {dec},
pages = {11106--11115},
publisher = {Association for the Advancement of Artificial Intelligence},
title = {{Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting}},
url = {https://arxiv.org/pdf/2012.07436},
volume = {12B},
year = {2020}
}
